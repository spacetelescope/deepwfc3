{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcR3do4SAr2_",
    "outputId": "c25bbf70-4814-463f-c934-480be9928728"
   },
   "source": [
    "<a id=\"title\"></a>\n",
    "# Convolutional Neural Network MNIST Classification Tutorial using PyTorch\n",
    "***\n",
    "## Learning Goals:\n",
    "By the end of this tutorial, you will:\n",
    "- learn the basics of using the PyTorch library\n",
    "- build a convolutional neural network (CNN) to classify MNIST images\n",
    "- train and evaluate a CNN\n",
    "- analyze performance on false positive/negative instances\n",
    "\n",
    "## Table of Contents\n",
    "[Introduction](#intro) <br>\n",
    "[0. Imports](#imports) <br>\n",
    "[1. MNIST Dataset and Scaling](#mnist) <br>\n",
    "[2. Build a CNN Classifier](#build) <br>\n",
    "[3. Test Model Functionality](#test) <br>\n",
    "[4. Set Training and Test Sets](#set) <br>\n",
    "[5. Hyperparameters and Loading](#hyper) <br>\n",
    "[6. Train Model](#train) <br>\n",
    "[7. Plot Loss Function and Accuracy](#plot) <br>\n",
    "[8. Analyze Samples](#analyze) <br>\n",
    "[9. Conclusions](#con) <br>\n",
    "[Additional Resources](#add) <br>\n",
    "[About this Notebook](#about) <br>\n",
    "[Citations](#cite) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a id=\"intro\"></a>\n",
    "\n",
    "The main purpose of this notebook is to teach basic functionality of [PyTorch](https://pytorch.org/), a deep learning Python library. This tutorial is not an exhaustive introduction to machine learning and assumes the user is familiar with vocabulary (supervised v unsupervised, neural networks, loss functions, backpropogation, etc) and methodology (model selection, feature selection, hyperparameter tuning, etc). Look at [Additional Resources](#add) for more complete machine learning guides. The paragraphs below serve as a brief refresher.\n",
    "\n",
    "Machine learning (ML) is a set of data driven algorithms in which a model can be trained to make predictions based off of past experiences. There are two main regimes of ML: supervised learning and unsupervised learning. Supervised learning techniques, such as regression and classification, require labels in order to be trained. Unsupervised learning techniques, such as clustering and dimensionality reduction, do not require labels to be trained.\n",
    "\n",
    "Neural networks are a subset of models that learn to extract features useful for training. Convolutional neural networks (CNNs) are neural networks that are excellent in computer vision, i.e. solving problems involving images. A CNN convolves patches of an image to learn filters that are useful for training. These networks became more accessible by [AlexNet](https://en.wikipedia.org/wiki/AlexNet) in 2012.\n",
    "\n",
    "[MNIST](http://yann.lecun.com/exdb/mnist/) is a dataset of 70,000 (60,000 train; 10,000 test) of handwritten digits from 0-9 (inclusive). Each image is 28 pixels by 28 pixels, each pixel being an 8-bit integer (0-255 integer gray scaled). The dataset is small compared to real world datasets used in industry, but is clean and large enough to use for efficient modeling. In addition, because of how well defined the features and labels are, it has become a staple in learning and testing different algorithms dealing with images.\n",
    "\n",
    "**In this notebook, we will build a CNN using PyTorch to classify MNIST images of handwritten digits.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports <a id=\"imports\"></a>\n",
    "\n",
    "If you are running this notebook on Google Colab, you shouldn't have to install anything. If you are running this notebook in Jupyter, this notebook assumes you created the virtual environment defined in `environment.yml`. If not, close this notebook and run the following lines in a terminal window:\n",
    "\n",
    "`conda env create -f environment.yml`\n",
    "\n",
    "`conda activate pytorch_mnist_cnn`\n",
    "\n",
    "We import the following libraries:\n",
    "- *numpy* for handling arrays\n",
    "- *matplotlib* for plotting\n",
    "- *tqdm* for keeping track of loop speed\n",
    "- *tensorflow* for accessing MNIST images \n",
    "- *torch* as our machine learning framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSb8DxN-5ioF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gQZlZgT5ioG"
   },
   "source": [
    "## 1. MNIST Dataset and Scaling<a id=\"mnist\"></a>\n",
    "\n",
    "The MNIST dataset is nicely packed in `tensorflow` as `np.arrays`, which is why we are grabbing our data from there instead of directly from `torch`. The data is unpacked as `x_train` for training features, `y_train` for training labels, `x_test` for testing features, and `y_test` for testing labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89l2--hv5ioG"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check to make sure the data is consistent with what was described in the [Introduction](#intro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IhPyD8XjQhp4",
    "outputId": "8e504109-f18c-4b27-81d4-4f560d5e9139"
   },
   "outputs": [],
   "source": [
    "print ('The shape of the training features is {} corresponding to '\n",
    "       '(# of samples, pixel length, pixel width). \\n'.format(x_train.shape))\n",
    "print ('The shape of the training labels is {} corresponding to '\n",
    "       '(# of samples). \\n'.format(y_train.shape))\n",
    "print ('The shape of the testing features is {} corresponding to '\n",
    "       '(# of samples, pixel length, pixel width). \\n'.format(x_test.shape))\n",
    "print ('The shape of the testing labels is {} corresponding to '\n",
    "       '(# of samples). \\n'.format(y_test.shape))\n",
    "print ('The min and max pixel values are {} and {}, respectively, as {}. \\n'.format(x_train.min(), \n",
    "                                                                                    x_train.max(), \n",
    "                                                                                    x_train.dtype))\n",
    "print ('The labels are type {}. \\n'.format(y_train.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at the contents of the dataset. Below is the first sample of the training set, the number 5. Feel free to change the index to view other images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "zTIgr3y2RmMA",
    "outputId": "ee286fc3-6d8d-483a-c719-e25af6a39e36"
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "plt.title('Training Image {} with a label of {}'.format(index, y_train[index]))\n",
    "plt.imshow(x_train[index])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also define some frequently used global variables. `x_train_size` is the number of images in the training set, `x_test_size` is the number of images in the test set, and `x_length` is the length/width of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRk5kpJF5ioG"
   },
   "outputs": [],
   "source": [
    "x_train_size = x_train.shape[0]\n",
    "x_test_size = x_test.shape[0]\n",
    "x_length = x_train.shape[1]\n",
    "\n",
    "print (x_train_size, x_test_size, x_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cGCuvON5ioG"
   },
   "source": [
    "In machine learning, it is common practice to normalize data into more reasonable values in order to optimize calculating gradients in our loss function and train more robustly. We min-max scale our images to have a minimum value of 0 and a maximum value of 1. The normalization factor is defined as the maximum pixel value (255) and used to define our scaled variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BmzeUkAQ5ioH"
   },
   "outputs": [],
   "source": [
    "norm = x_train.max()\n",
    "\n",
    "x_train_scale = x_train / norm\n",
    "x_test_scale = x_test / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at our previous example to confirm the image was properly scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "L_mRhBfp6b0C",
    "outputId": "498a919d-a9e9-4cf7-ca98-e407d0705e82"
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "plt.title('Training Scaled Image {} with a label of {}'.format(index, y_train[index]))\n",
    "plt.imshow(x_train_scale[index])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhl2MMkJ5ioH"
   },
   "source": [
    "## 2. Build a CNN Classifier <a id=\"build\"></a>\n",
    "\n",
    "PyTorch has its own unique data objects called `torch.utils.data.Dataset`. `Dataset` has functions to retrieve the data length and instances. The datasets built from the class are used as inputs for `torch.utils.data.Dataloader`, which prepares our data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ko78u4Eu5ioH"
   },
   "outputs": [],
   "source": [
    "class LoadDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhYRKvD15ioH"
   },
   "source": [
    "Here we define the functions and layers to build our two-layer CNN. The constructor has our model hyperparameters as inputs:\n",
    "\n",
    "- `filters`: the number of filters the CNN will learn\n",
    "- `neurons`: the numbers of output neurons in the fully connected layers\n",
    "- `sub_array_size`: the image length/width\n",
    "- `k`: the length/width of the filter being learned\n",
    "- `pool`: the length/width of max pooling\n",
    "- `pad`: the length/width of zero padding\n",
    "\n",
    "Using the constructor's parameters, we define the CNNs layers and functions. We use the [rectified linear unit (ReLU)](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) as our activation function to add nonlinearity to our model, use max pool to downsample (decrease model size and forces better feature extraction), use `filters`, `k`, and `pad` to build our CNN layers, and use `neurons` to build our fully connected layers.\n",
    "\n",
    "The `forward` fucntion organizes our CNN from the functions we defined in the constructor. The CNN is built as follows:\n",
    "- Convolutional Layer 1\n",
    "    - convole 1 28x28 image into 16 28x28 feature maps\n",
    "    - activate the feature maps using ReLU\n",
    "    - max pool the 16 28x28 feature maps to 16 14x14 feature maps\n",
    "- Convolutional Layer 2\n",
    "    - convole 16 14x14 feature maps into 32 14x14 feature maps\n",
    "    - activate the feature maps using ReLU\n",
    "    - max pool the 32 14x14 feature maps to 32 7x7 feature maps\n",
    "- Flatten the 32 7x7 feature maps to a 1D 32 * 7 * 7 array\n",
    "- Fully Connected Layer 1\n",
    "    - use the flatten 1D array as inputs for the 64 neuron hidden layer\n",
    "    - activate the neurons using ReLU\n",
    "- Fully Connected Layer 2\n",
    "    - use the 64 neuron hidden layer as inputs for the 10 output neurons\n",
    "    \n",
    "For deeper networks, we can regularize our network using [dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) and [batch normalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html). We choose to omit these techniques to have a simpler model.\n",
    "    \n",
    "**Note: the output is activated automatically in our loss function (see [Section 5](#hyper)).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMpoOh-B5ioI"
   },
   "outputs": [],
   "source": [
    "# flatten torch tensor\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "# define functions and build model\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 filters = [1, 16, 32],        # most people use powers of 2\n",
    "                 neurons = [64, 10],           # neurons of fully connected layer\n",
    "                 sub_array_size = x_length,    # image size (28x28)\n",
    "                 k = 3,                        # kernel size (3x3)\n",
    "                 pool = 2,                     # pooling size (2x2)\n",
    "                 pad = 1):                     # padding length (1 pixel border)\n",
    "\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        # The Rectified Linear Unit (ReLU)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Max Pool\n",
    "        self.mp = nn.MaxPool2d(pool, return_indices=False)\n",
    "        \n",
    "        # Flattens the feature map to a 1D array\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        # ---- CONVOLUTION  ----\n",
    "        self.conv1 = nn.Conv2d(in_channels=filters[0], out_channels=filters[1], kernel_size=k, padding=pad)\n",
    "        self.conv2 = nn.Conv2d(in_channels=filters[1], out_channels=filters[2], kernel_size=k, padding=pad)\n",
    "        \n",
    "        # Total number of pixels in the input of first connected layer:\n",
    "        # number of pooling layers = number of convolutional layers - 1\n",
    "        # total amount of pooling = pooling size ** number of pooling layers\n",
    "        # length/width of feature map after last pooling = sub array size / \n",
    "        #                                                  total amount of pooling\n",
    "        # total number of feature map pixels = feature map length/width ** 2\n",
    "        # total number of input neurons = number of filters in final layer * \n",
    "        #                                 total number of feature map pixels\n",
    "        \n",
    "        num_pool = len(filters) - 1\n",
    "        neurons_flat = filters[-1] * (sub_array_size // pool ** num_pool) ** 2 \n",
    "        \n",
    "        # ---- FULLY CONNECTED ----\n",
    "        self.fc1 = nn.Linear(neurons_flat, neurons[0])\n",
    "        self.fc2 = nn.Linear(neurons[0], neurons[1])\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        # Convolutional Layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.mp(x)\n",
    "\n",
    "        # Convolutional Layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.mp(x)\n",
    "\n",
    "        # Flatten Layer\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Fully Connected 1\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Fully Connected 2\n",
    "        x = self.fc2(x)      \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uw9Qoxeh5ioI"
   },
   "source": [
    "## 3. Test Model Functionality <a id=\"test\"></a>\n",
    "\n",
    "Before training, we need to make sure our model is properly built, i.e. the expected input (2D 28x28 array) will return the expected output (2D 1x10 array). An error indicates the architecture is inconsistent in some way, such as unexpected input and output filters, unexpected input and output neurons, etc. Some ways to \"break\" the model are listed below:\n",
    "- comment out a method in the constructor\n",
    "- manually change `neurons_flat` to a different value\n",
    "- manually change `out_channels` in `self.conv1` or `self.conv2` to a different value\n",
    "\n",
    "To start off, we will build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duve8WKo5ioJ"
   },
   "outputs": [],
   "source": [
    "model = Classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we change the shape of our image to be compatible with PyTorch. The input dimensions for images are (number of samples, number of input channels, y dimension, x dimension), which in our case is (1, 1, 28, 28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apN1_iWa5ioJ"
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "test_image = x_train_scale[index].reshape(1,1,x_length,x_length) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dimensions are changed, we convert the image from a `np.array` to a `torch.Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apN1_iWa5ioJ"
   },
   "outputs": [],
   "source": [
    "test_image_torch = torch.Tensor(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can \"predict\" the output neurons of the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wpQxovKl32dI",
    "outputId": "11bc3734-ea88-4d7e-8b41-3edecf4e8275"
   },
   "outputs": [],
   "source": [
    "testoutput_torch = model(test_image_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no error, we know our model is working. We also move the output from our model using the `detach()` method and convert the `torch.Tensor` to a `np.array` by using the `numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wpQxovKl32dI",
    "outputId": "11bc3734-ea88-4d7e-8b41-3edecf4e8275"
   },
   "outputs": [],
   "source": [
    "testoutput = testoutput_torch.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shape of the output neurons to make sure they are what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eMX9qm2K5ioJ",
    "outputId": "ccdec276-abe3-42f5-f1ca-ad717c491601"
   },
   "outputs": [],
   "source": [
    "print ('The shape of the output neurons are {}.'.format(testoutput.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return the output neurons. Note since we do not use an activation function at the end of our network, the domain of our output can be any real number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the [softmax activation function](https://en.wikipedia.org/wiki/Softmax_function) to convert the output neurons to probabilities for each classification with the index corresponding to the digit classification probability, e.g. the 0th index corresponds to the probability of a 0 classification. Since our model isn't trained, all the output probabilities are approximately 0.1, indicating our model is randomly \"classifying\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "softmax(testoutput_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, it's good practice to know how many trainable parameters are in our model. The number of trainable parameters can be used as a proxy for estimating total training time. We define [a counting function](https://stackoverflow.com/questions/49201236/check-the-total-number-of-parameters-in-a-pytorch-model) for us and determine how many trainable parameters there are in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlRZSLlA5ioJ"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        print([name, param])\n",
    "        total_params+=param\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bI_Ay6A35ioK",
    "outputId": "42ce0930-3c9d-4156-f5d4-0b6f5b6a53c3"
   },
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Br3lFwwt5ioK"
   },
   "source": [
    "## 4. Set Training and Test Sets <a id=\"set\"></a>\n",
    "\n",
    "PyTorch uses iterables to create its data objects. Here we show two ways to format the data to be PyTorch compatible.\n",
    "\n",
    "1. **Use lists:** Experienced Python users are more likely to be comfortable using and manipulating lists. The function `format_dataset` makes a 2D list with elements being [image, label]. The image is reshaped to (1, 28, 28) so the first dimension corresponds to number of input channels.\n",
    "\n",
    "2. **Use LoadDataset:** In [Section 2](#build), we defined the `LoadDataset` class to format the data to be PyTorch compatible. The `Dataset` class comes with additional functionality specifically for PyTorch, but is beyond the scope of this tutorial.\n",
    "\n",
    "We choose option 1 as default, but option 2 can be uncommented below. Using either does not affect training at all and is up to user comfortability/preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQU2kznG5ioK"
   },
   "outputs": [],
   "source": [
    "def format_dataset(image_set, labels, size=x_length):\n",
    "    data_set = []\n",
    "    for i in range(len(image_set)):\n",
    "        data_set.append([image_set[i].reshape(1,size,size), labels[i]])  \n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQU2kznG5ioK"
   },
   "outputs": [],
   "source": [
    "train_set = format_dataset(x_train_scale, y_train)\n",
    "val_set = format_dataset(x_test_scale, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQU2kznG5ioK"
   },
   "outputs": [],
   "source": [
    "# LoadDataset class\n",
    "#train_set = LoadDataset(x_train_scale.reshape(x_train_size,1,x_length,x_length), y_train)\n",
    "#val_set = LoadDataset(x_test_scale.reshape(x_test_size,1,x_length,x_length), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlocipjn5ioL"
   },
   "source": [
    "## 5. Hyperparameters and Loading <a id=\"hyper\"></a>\n",
    "\n",
    "First, we must set our hyperparameters for the model to use for training. The hyperparamters we are using are batch size, shuffle, and number of workers. Batch size can be tuned as needed to improve results. Shuffle should almost always be True since the data shouldn't be ordered in any specific way when training. In addition, the number of workers has a default of 0, which uses the main processor on the machine you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULKA-7wr5ioL"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "        'batch_size': 128,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we choose the number of epochs we wish to train for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULKA-7wr5ioL"
   },
   "outputs": [],
   "source": [
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful metric to know is how many updates our model will perform during training. We can calculate this by finding the number of batches in the training set (number of training samples / batch size) and multiplying it by the number of epochs. Knowing how many batches our model might need to be well trained can be a good place to start when tuning hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('The model will train using a total of {} batches'.format(num_epochs * \n",
    "                                                       int(x_train_scale.shape[0] / params['batch_size'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our hyperparameters set, we can load our training and test set using `DataLoader`. \n",
    "\n",
    "**Note the variable and function names in the notebook are directed for validation sets, but we will use them for the test set instead.** That being said, we use the definitions for validation set and test set interchangeably here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULKA-7wr5ioL"
   },
   "outputs": [],
   "source": [
    "# TRAINING SET\n",
    "train_loader = DataLoader(train_set, **params)\n",
    "\n",
    "# TEST SET\n",
    "valid_loader = DataLoader(val_set, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will initialize our model again to be sure we are starting from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s__fkF-_5ioL"
   },
   "outputs": [],
   "source": [
    "model = Classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our loss function to be [Cross Entropy Loss](https://en.wikipedia.org/wiki/Cross_entropy), which combines [softmax and the negative log likelihood loss](https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/). This function is standard in multiclass classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s__fkF-_5ioL"
   },
   "outputs": [],
   "source": [
    "distance = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we choose our optimizer to be [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam), since the learning rate updates automatically and trains relatvely fast compared to [Stochastic Gradient Descent (SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s__fkF-_5ioL"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),  weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have GPUs available, then those will be used for training. If not, then the model will train on CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s__fkF-_5ioL"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the device to make sure we know what's available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s__fkF-_5ioL"
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Model <a id=\"train\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train our model, we have to manually loop through our data for training. This is probably the biggest difference between PyTorch and [Tensorflow](https://www.tensorflow.org/), but this allows for more hands-on manipulation of how training is performed, which can be advantageous. We will train our model as follows:\n",
    "1. Change the model to trianing mode to activate backpropogation\n",
    "2. Initialize training loss to be 0\n",
    "3. Loop through each batch of features and labels by:\n",
    "    - Putting the data onto your device\n",
    "    - Calculating the output neurons and the loss\n",
    "    - Performing backgrpopogation and adding the batch training loss to total training loss\n",
    "4. Normalize the total training loss by number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrotiQpF5ioL"
   },
   "outputs": [],
   "source": [
    "# Define train loop\n",
    "\n",
    "def train_model(train_loader):\n",
    "\n",
    "    # Change model to training mode (activates backpropogation)\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize training loss\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Loop through batches of training data\n",
    "    for data, target in train_loader:\n",
    "        \n",
    "        # Put training batch on device\n",
    "        data = data.float().to(device)\n",
    "        target = target.type(torch.LongTensor).to(device)\n",
    "\n",
    "        # Calculate output and loss from training batch\n",
    "        output = model(data)\n",
    "        loss = distance(output, target)\n",
    "        \n",
    "        # Backpropogation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Normalize training loss from one epoch\n",
    "    train_loss_norm = train_loss / len(train_loader)\n",
    "    \n",
    "    return train_loss_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we define a similar loop for evaluating the test set at each epoch, which signals us if our model is generalizing. We will test our model as follows:\n",
    "1. Change model to evaluation mode to deactivate backpropogation\n",
    "2. Initialize test loss and number of correctly classified samples to 0\n",
    "3. Loop through each batch of features and labels by:\n",
    "    - Putting the data onto your device\n",
    "    - Calculating the output neurons and the loss\n",
    "    - Counting the number of correct predictions\n",
    "4. Calculate test set accuracy\n",
    "5. Normalize the total test loss by number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFCkW15B8a3F"
   },
   "outputs": [],
   "source": [
    "# Define validation loop\n",
    "\n",
    "def validate_model(valid_loader):\n",
    "\n",
    "    # Change model to evaluate mode (deactivates backpropogation)\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize validation loss and number of correct predictions\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # Do not calculate gradients for the loop\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Loop through batches of validation data\n",
    "        for data, target in valid_loader:\n",
    "            \n",
    "            # Put validation batch on device\n",
    "            data = data.float().to(device)\n",
    "            target = target.type(torch.LongTensor).to(device)\n",
    "            \n",
    "            # Calculate output and loss from validation batch\n",
    "            output = model(data)\n",
    "            val_loss += distance(output, target).item()\n",
    "            \n",
    "            # Count number of correct predictions\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = 100. * correct / len(valid_loader.dataset)\n",
    "    \n",
    "    # Normalize validation loss from one epoch\n",
    "    val_loss_norm = val_loss / len(valid_loader)\n",
    "    \n",
    "    return val_loss_norm, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train our model! We will print out the train and test loss/accuracy per epoch to keep track of performance. The loop below performs the training and validation loops defined above and records our metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-1mPpYr5ioM",
    "outputId": "375dc5f0-5c50-4e5e-f78d-b95b8a192c11",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keep track of metrics\n",
    "lst_train_loss = []\n",
    "lst_val_loss = []\n",
    "lst_accuracy = []\n",
    "\n",
    "# training loop\n",
    "for epoch in tqdm(range(num_epochs), total=num_epochs):\n",
    "\n",
    "    # Go through loops\n",
    "    train_loss = train_model(train_loader)\n",
    "    val_loss, accuracy = validate_model(valid_loader)\n",
    "\n",
    "    # Append metrics\n",
    "    lst_train_loss.append(train_loss)\n",
    "    lst_val_loss.append(val_loss)\n",
    "    lst_accuracy.append(accuracy)\n",
    "\n",
    "    # Log\n",
    "    print('Epoch {:.3f} - Train loss: {:.3f} - Val Loss: {:.3f} - Accuracy: ({:.0f}%)'.format(\n",
    "            epoch, train_loss, val_loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qip7uayN5ioM"
   },
   "source": [
    "## 7. Plot Loss Function and Accuracy <a id=\"plot\"></a>\n",
    "\n",
    "We plot the train/test loss and test accuracy to determine how well converged our model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "sCTCmI8u5ioM",
    "outputId": "0b15a3e8-5884-4d15-bccf-7bf6d38a1b92",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=[10,5])\n",
    "\n",
    "axs[0].set_title('Loss')\n",
    "axs[0].plot(np.arange(num_epochs), lst_train_loss, label='train')\n",
    "axs[0].plot(np.arange(num_epochs), lst_val_loss, label='val')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_title('Accuracy')\n",
    "axs[1].plot(np.arange(num_epochs), lst_accuracy, color='C1')\n",
    "axs[1].set_xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2jku9xZ5ioM"
   },
   "source": [
    "## 8. Analyze Samples <a id=\"analyze\">\n",
    "    \n",
    "Now that our model is trained, let's analyze some samples to see the classification probabilities of some images.\n",
    "    \n",
    "First, we predict the output neurons and labels of our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKUpueDb_aSP"
   },
   "outputs": [],
   "source": [
    "output = model(torch.Tensor(x_test_scale.reshape(x_test_size, 1, x_length, x_length)))\n",
    "\n",
    "val_pred = output.data.max(1, keepdim=True)[1].detach().numpy().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can look at random examples in our test set and plot classification probabilities using a bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEawafOrB4YZ"
   },
   "outputs": [],
   "source": [
    "# choose random image and corresponding output neurons from test set\n",
    "rand_index = np.random.randint(x_test_scale.shape[0])\n",
    "rand_image = x_test_scale[rand_index]\n",
    "rand_class_prob_tensor = softmax(val_pred[rand_index:rand_index+1])\n",
    "rand_class_prob = rand_class_prob_tensor.detach().numpy().flatten()\n",
    "\n",
    "# plot image and classification probabilities\n",
    "fig, axs = plt.subplots(1,2,figsize=[10,5])\n",
    "axs[0].set_title('Testing Scaled Image {} with a label of {}'.format(rand_index, y_test[rand_index]))\n",
    "axs[0].imshow(rand_image)\n",
    "axs[1].set_title('Classification Probabilities for Testing Scaled Image {}'.format(rand_index))\n",
    "axs[1].bar(np.arange(10), rand_class_prob)\n",
    "axs[1].set_xlabel('Label')\n",
    "axs[1].set_ylabel('Probability (Log Scale)')\n",
    "axs[1].set_yscale('log')\n",
    "print ('Prediction: {}, Label: {}'.format(val_pred[rand_index], y_test[rand_index]))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition we can look at false positive/negative samples to investigate harder samples in the test set.\n",
    "\n",
    "We create a mask that will return incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = val_pred != y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose random incorrect image and corresponding output neurons from test set\n",
    "false_index = np.random.randint(x_test_scale[mask].shape[0])\n",
    "false_image = x_test_scale[mask][false_index]\n",
    "false_class_prob = (nn.Softmax(dim=1)(output[mask][false_index:false_index+1])).detach().numpy().flatten()\n",
    "\n",
    "# plot image and classification probabilities\n",
    "fig, axs = plt.subplots(1,2,figsize=[10,5])\n",
    "axs[0].set_title('Testing Scaled Image {} with a label of {}'.format(false_index, y_test[mask][false_index]))\n",
    "axs[0].imshow(false_image)\n",
    "axs[1].set_title('Classification Probabilities (Log Scale)')\n",
    "axs[1].bar(np.arange(10), false_class_prob)\n",
    "axs[1].set_yscale('log')\n",
    "print ('Prediction: {}, Label: {}'.format(val_pred[mask][false_index], y_test[mask][false_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions <a id=\"con\"></a>\n",
    "\n",
    "Thank you for walking through this notebook. Now you should be more familiar with:\n",
    "- the basics of using the PyTorch library\n",
    "- building a convolutional neural network (CNN)\n",
    "- training and evaluating a CNN\n",
    "- analyzing performance on samples, such as false positive/negative instances\n",
    "\n",
    "**Congratulations, you have completed the notebook!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources <a id=\"add\"></a>\n",
    "\n",
    "Machine learning is a dense and rapidly evolving field of study. Becoming an expert takes years of practice and patience, but hopefully this notebook brought you closer in that direction. Here are some of the author's favorite resources for learning about machine learning and data science:\n",
    "\n",
    "- [Google Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/ml-intro)\n",
    "    - [CNN Practica](https://developers.google.com/machine-learning/practica/image-classification)\n",
    "- [scikit-learn Python Library](https://scikit-learn.org/stable/index.html) (go-to for most ML algorithms besides neural networks)\n",
    "- [StatQuest Youtube Channel](https://www.youtube.com/c/joshstarmer)\n",
    "- [DeepLearningAI YouTube Channel](https://www.youtube.com/c/Deeplearningai/videos)\n",
    "- [Towards Data Science](https://towardsdatascience.com/) (articles about data science and machine learning, some invlovling example blocks of code)\n",
    "- Advance searching [arxiv](https://arxiv.org/search/advanced) (e.g. search term \"machine learning\" in Abstract for Subject astro-ph) to see what others are doing currently\n",
    "- Google, YouTube, and Wikipedia in general\n",
    "\n",
    "## About this Notebook <a id=\"about\"></a>\n",
    "\n",
    "**Author:** Fred Dauphin, DeepWFC3\n",
    "\n",
    "**Updated on:** 2021-12-03\n",
    "\n",
    "## Citations <a id=\"cite\"></a>\n",
    "\n",
    "If you use `numpy`, `matplotlib`, or `torch` for published research, please cite the authors. Follow these links for more information about citing `numpy`, `matplotlib`, and `torch`:\n",
    "\n",
    "* [Citing `numpy`](https://numpy.org/doc/stable/license.html)\n",
    "* [Citing `matplotlib`](https://matplotlib.org/stable/users/project/license.html#:~:text=Matplotlib%20only%20uses%20BSD%20compatible,are%20acceptable%20in%20matplotlib%20toolkits.)\n",
    "* [Citing `torch`](https://github.com/pytorch/pytorch/blob/master/LICENSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "[Top of Page](#title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "classifier_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

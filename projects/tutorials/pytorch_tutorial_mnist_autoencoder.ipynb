{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"title\"></a>\n",
    "# Autoencoder MNIST Tutorial using PyTorch\n",
    "***\n",
    "## Learning Goals:\n",
    "By the end of this tutorial, you will:\n",
    "- build an autoencoder\n",
    "- train and evaluate an autoencoder\n",
    "- visualize the latent space of an autoencoder\n",
    "- detect anomalies using an autoencoder\n",
    "\n",
    "## Table of Contents\n",
    "[Introduction](#intro) <br>\n",
    "[0. Imports](#imports) <br>\n",
    "[1. MNIST Dataset and Scaling](#mnist) <br>\n",
    "[2. Build an Autoencoder](#build) <br>\n",
    "[3. Test Model Functionality](#test) <br>\n",
    "[4. Set Training and Test Sets](#set) <br>\n",
    "[5. Hyperparameters and Loading](#hyper) <br>\n",
    "[6. Train Model](#train) <br>\n",
    "[7. Plot Loss Function and R2](#plot) <br>\n",
    "[8. Analyze Samples](#analyze) <br>\n",
    "[9. Visualize the Latent Space](#latent) <br>\n",
    "[10. Detect Anomalies](#detect) <br>\n",
    "[11.. Conclusions](#con) <br>\n",
    "[Additional Resources](#add) <br>\n",
    "[About this Notebook](#about) <br>\n",
    "[Citations](#cite) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a id=\"intro\"></a>\n",
    "\n",
    "The main purpose of this notebook is to build an autoencoder in [PyTorch](https://pytorch.org/), a deep learning Python library. This tutorial is not an exhaustive introduction to machine learning and assumes the user is familiar with vocabulary (supervised v unsupervised, neural networks, loss functions, backpropogation, etc) and methodology (model selection, feature selection, hyperparameter tuning, etc). This notebook also assumes the user is familiar with convolutional neural networks (CNNs) and the [MNIST handwritten dataset](http://yann.lecun.com/exdb/mnist/). Look at [Additional Resources](#add) for more complete machine learning guides. The paragraphs below serve as a brief introduction to autoencoders.\n",
    "\n",
    "An [autoencoder](https://en.wikipedia.org/wiki/Autoencoder) is an unsupervised learning algorithm that learns how to reconstruct the input as an output (i.e. using a neural network to learn a complex version of the identify function). It is comprised of an encoder, which compresses the input into a low dimensional latent space, and a decoder, which decompresses the representation from the latent space into the output. Autoencoders are versatile models with some of the main uses including dimensionality reduction, anomaly detection, and image denoising. Below explains why each purpose may be useful:\n",
    "\n",
    "1. Dimensionality reduction: If an autoencoder is able to reconstruct the input to a high degree, then the latent space (low dimensionality) must be a well constructed representation of the input space (high dimensionality). That is to say the model has learned an efficient, compressed representation of the data. You could vastly decrease the training time or increase the complexity of other machine learning models by using the latent space representation as features instead of using the original inputs.\n",
    "\n",
    "    - Example: on a training data of cats and dogs, the autoencoder could differentiate between these two classes in the latent space (a cluster of cat images separate from a cluster of dog images). \n",
    "\n",
    "\n",
    "2. Anomaly detection: If an autoencoder is able to reconstruct the input to a high degree, then it has learned the features and distribution of the training data well. Therefore features outside of the distribution of the training data will reconstruct poorly and can be automatically detected by having a high loss compared to inputs within the distribution. **Note: a deep model trained on an extremely diverse dataset could learn features general enough to reconstruct inputs outside of its learned domain. Since the model is \"too good\", the model may perform poorly on anomaly detection.** \n",
    "\n",
    "    - Example: on training data of cats and dogs, the autoencoder has learned the features of these two animals. If an image of a car was an input, the autoencoder would not understand this image and would try to reconstruct a cat or dog, leading to a high loss between the input and reconstruction.\n",
    "    \n",
    "    \n",
    "3. Image denoising: An autoencoder in theory can be mapped between any two input and output spaces. If we use training data with features as noisy input and labels as clean inputs, then the autoencoder will learn to remove the noise from those inputs. Now any noisy inputs from the future can be denoised and used for further analysis.\n",
    "\n",
    "    - Example: on training data of noisy cat images as features and clean cat images as labels, the autoencoder could denoise any new cat images.\n",
    "    \n",
    "Other types of autoencoders exists, such as the [variational autoencoder (VAE)](https://en.wikipedia.org/wiki/Variational_autoencoder), but implementing one is beyond the scope of this tutorial.\n",
    "    \n",
    "**In this notebook, we will build an autoencoder using PyTorch to learn the representation of MNIST handwritten digit dataset. We will also go over the first two use cases of an autoencoder as a dimensionality reducer and an anomaly detector.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YcR3do4SAr2_",
    "outputId": "c25bbf70-4814-463f-c934-480be9928728"
   },
   "source": [
    "## 0. Imports <a id=\"imports\"></a>\n",
    "\n",
    "If you are running this notebook on Google Colab, you shouldn't have to install anything. If you are running this notebook in Jupyter, this notebook assumes you created the virtual environment defined in `environment.yml`. If not, close this notebook and run the following lines in a terminal window:\n",
    "\n",
    "`conda env create -f environment.yml`\n",
    "\n",
    "`conda activate pytorch_mnist`\n",
    "\n",
    "We import the following libraries:\n",
    "- *numpy* for handling arrays\n",
    "- *matplotlib* for plotting\n",
    "- *tqdm* for keeping track of loop speed\n",
    "- *tensorflow* for accessing MNIST images \n",
    "- *torch* as our machine learning framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YcR3do4SAr2_",
    "outputId": "c25bbf70-4814-463f-c934-480be9928728"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MNIST Dataset and Scaling<a id=\"mnist\"></a>\n",
    "\n",
    "The MNIST dataset is nicely packed in `tensorflow` as `np.arrays`, which is why we are grabbing our data from there instead of directly from `torch`. The data is unpacked as `x_train` for training features, `y_train` for training labels, `x_test` for testing features, and `y_test` for testing labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also define some frequently used global variables. `x_train_size` is the number of images in the training set, `x_test_size` is the number of images in the test set, and `x_length` is the length/width of an image. In addition, we min-max scale our images to have a minimum value of 0 and a maximum value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_size = x_train.shape[0]\n",
    "x_test_size = x_test.shape[0]\n",
    "x_length = x_train.shape[1]\n",
    "\n",
    "norm = x_train.max()\n",
    "x_train_scale = x_train / norm\n",
    "x_test_scale = x_test / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build an Autoencoder <a id=\"build\"></a>\n",
    "\n",
    "PyTorch has its own unique data objects called `torch.utils.data.Dataset`. `Dataset` has methods to retrieve the data length and instances. The datasets built from the class are used as inputs for `torch.utils.data.Dataloader`, which prepares our data for training. Since an autoencoder isn't trained using labels, the \"labels\" are not defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, images):\n",
    "        self.images = images\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also define some helper functions to flatten our last encoded feature maps to neurons and unflatten our last decoded neurons to feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, shape_before_flatten):\n",
    "        return input.view(input.size(0), *shape_before_flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the functions and layers to build our autoencoder. The constructor has our model hyperparameters as inputs:\n",
    "\n",
    "- `filters`: the number of filters the convolutional layers will learn\n",
    "- `latent_dimensions`: the dimensionality of the latent space\n",
    "- `sub_array_size`: the image length/width\n",
    "- `k`: the length/width of the filter being learned\n",
    "- `pool`: the length/width of max pooling\n",
    "- `pad`: the length/width of zero padding\n",
    "\n",
    "Using the constructor's parameters, we define the CNNs layers and functions. We use the [rectified linear unit (ReLU)](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) as our activation function to add nonlinearity to our model, use max pool to downsample (decrease model size and force feature extraction), use `filters`, `k`, and `pad` to build our convolutional layers, and use `latent_dimensions` to build our latent space. In addition, we use max unpooling and transpose convolution to upsample in our decoder.\n",
    "\n",
    "The `forward` fucntion builds our autoencoder from the functions we defined in the constructor. The autoencoder is built as follows:\n",
    "- Encoder Layer 1\n",
    "    - convole 1 28x28 image into 16 28x28 feature maps\n",
    "    - activate the feature maps using ReLU\n",
    "    - max pool the 16 28x28 feature maps to 16 14x14 feature maps\n",
    "- Encoder Layer 2\n",
    "    - convole 16 14x14 feature maps into 32 14x14 feature maps\n",
    "    - activate the feature maps using ReLU\n",
    "    - max pool the 32 14x14 feature maps to 32 7x7 feature maps\n",
    "- Flatten the 32 7x7 feature maps to a 1D 32 * 7 * 7 array\n",
    "- Latent Layer\n",
    "    - use the flatten 1D array as inputs for the 2 dimensional latent space\n",
    "    - use the 2 dimensional latent space as inputs for the flatten 1D array\n",
    "- Unflatten a 1D 32 * 7 * 7 array to the 32 7x7 feature maps\n",
    "- Decoder Layer 1\n",
    "    - unmax pool the 32 7x7 feature maps to 32 14x14 feature maps\n",
    "    - activate the feature maps using ReLU\n",
    "    - transpose convole 32 14x14 feature maps into 16 14x14 feature maps\n",
    "- Decoder Layer 2\n",
    "    - unmax pool the 16 14x14 feature maps to 16 28x28 feature maps\n",
    "    - activate the feature maps using ReLU\n",
    "    - transpose convole 16 28x28 feature maps into 1 28x28 output\n",
    "- Final activation using ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions and build model\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 filters = [1, 16, 32],\n",
    "                 latent_dimensions = 2,\n",
    "                 sub_array_size = x_length,\n",
    "                 k = 3,\n",
    "                 pool = 2,\n",
    "                 pad = 1):\n",
    "\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # The Rectified Linear Unit (ReLU)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Max Pool and Un-Max Pool\n",
    "        self.mp = nn.MaxPool2d(pool, return_indices=True)\n",
    "        self.up = nn.MaxUnpool2d(pool)\n",
    "        \n",
    "        # Flattens the feature map to a 1D array\n",
    "        self.flatten = Flatten()\n",
    "        # Unflattens 1D array to feature map\n",
    "        self.unflatten = UnFlatten()\n",
    "\n",
    "        # ---- ENCODER ----\n",
    "        self.conv1 = nn.Conv2d(in_channels=filters[0], out_channels=filters[1], kernel_size=k, padding=pad)\n",
    "        self.conv2 = nn.Conv2d(in_channels=filters[1], out_channels=filters[2], kernel_size=k, padding=pad)\n",
    "        \n",
    "        # ---- LATENT ----\n",
    "        size_before_latent = filters[-1] * (sub_array_size // pool ** (len(filters) - 1)) ** 2\n",
    "        self.latent = nn.Linear(size_before_latent, latent_dimensions)\n",
    "        self.out_of_latent = nn.Linear(latent_dimensions, size_before_latent)\n",
    "\n",
    "        # ---- DECODER ----\n",
    "        self.trans1 = nn.ConvTranspose2d(in_channels=filters[2], out_channels=filters[1], kernel_size=k, padding=pad)\n",
    "        self.trans2 = nn.ConvTranspose2d(in_channels=filters[1], out_channels=filters[0], kernel_size=k, padding=pad)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        # ENCODER\n",
    "        # Layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x, ind1 = self.mp(x)\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x, ind2 = self.mp(x)\n",
    "\n",
    "        # LATENT\n",
    "        shape_before_flatten = x.size()[1:]\n",
    "        x = self.flatten(x)\n",
    "        x = self.latent(x)\n",
    "        x = self.out_of_latent(x)\n",
    "        x = self.unflatten(x, shape_before_flatten)        \n",
    "\n",
    "        # DECODER\n",
    "        # Layer 1\n",
    "        x = self.up(x, ind2)\n",
    "        x = self.relu(x)\n",
    "        x = self.trans1(x)\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.up(x, ind1)\n",
    "        x = self.relu(x)\n",
    "        x = self.trans2(x)\n",
    "\n",
    "        # Final activation\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Model Functionality <a id=\"test\"></a>\n",
    "\n",
    "Before training, we need to make sure our model is properly built, i.e. the expected input (2D 28x28 array) will return the expected output (2D 28x28 array). An error indicates the architecture is inconsistent in some way, such as unexpected input and output filters, unexpected input and output neurons, etc. Some ways to \"break\" the model are listed below:\n",
    "- comment out a method in the constructor or forward\n",
    "- manually change arguments in the methods to a different value\n",
    "\n",
    "To start off, we will build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we change the shape of our image to be compatible with PyTorch. The input dimensions for images are (number of samples, number of input channels, y dimension, x dimension), which in our case is (1, 1, 28, 28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "test_image = x_train_scale[index].reshape(1,1,x_length,x_length) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dimensions are changed, we convert the image from a `np.array` to a `torch.Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_torch = torch.Tensor(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can \"reconstruct\" our input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testoutput_torch = model(test_image_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no error, we know our model is working. We also move the output from our model using the `detach()` method and convert the `torch.Tensor` to a `np.array` by using the `numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testoutput = testoutput_torch.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shape of the output to make sure they are what we expect. If it's not, then we have to fix our parameters where we defined the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('The shape of the output is {}.'.format(testoutput.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the input and output. Since the model hasn't been trained, the ouput should look like random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=[10,5])\n",
    "axs[0].set_title('Training Scaled Image {}'.format(index))\n",
    "axs[0].imshow(test_image[0,0].reshape(x_length, x_length))\n",
    "axs[1].set_title('Reconstructed Output')\n",
    "axs[1].imshow(testoutput[0,0].reshape(x_length, x_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, it's good practice to know how many trainable parameters are in our model. The number of trainable parameters can be used as a proxy for estimating total training time. We define [a counting function](https://stackoverflow.com/questions/49201236/check-the-total-number-of-parameters-in-a-pytorch-model) for us and determine how many trainable parameters there are in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        print([name, param])\n",
    "        total_params+=param\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Training and Test Sets <a id=\"set\"></a>\n",
    "\n",
    "PyTorch uses iterables to create its data objects. Here we show two ways to format the data to be PyTorch compatible.\n",
    "\n",
    "1. **Use arrays:** Experienced Python users are more likely to be comfortable using and manipulating arrays. We will just reshape our images to have an input channel of 1, i.e. (1, 28, 28).\n",
    "\n",
    "2. **Use LoadDataset:** In [Section 2](#build), we defined the `LoadDataset` class to format the data to be PyTorch compatible. The `Dataset` class comes with additional functionality specifically for PyTorch, but is beyond the scope of this tutorial.\n",
    "\n",
    "We choose option 1 as default, but option 2 can be uncommented below. Using either does not affect training at all and is up to user comfortability/preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = x_train_scale.reshape(x_train_size, 1, x_length, x_length)\n",
    "val_set = x_test_scale.reshape(x_test_size, 1, x_length, x_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoadDataset class\n",
    "#train_set = LoadDataset(x_train_scale.reshape(x_train_size, 1, x_length, x_length))\n",
    "#val_set = LoadDataset(x_test_scale.reshape(x_test_size, 1, x_length, x_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define a baseline for our model to perform better than. The baseline helps us understand if our model is learning anything at all. We choose the mean pixel of the inputs to be our baseline, i.e. a poor model would learn the reconstructed image as an image of the mean pixel. By calculating the [Mean Squared Error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error) of our training set and mean pixels, we have an established baseline to outperform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find mean pixel values of each image\n",
    "mean = np.mean(x_train_scale, axis=(1,2)).reshape(x_train_size,1,1)\n",
    "\n",
    "# create mean pixel value images\n",
    "ones = np.ones((x_train_size, x_length, x_length))\n",
    "mean_ones = mean * ones\n",
    "\n",
    "# calculate baseline\n",
    "baseline = np.sum(np.square(x_train_scale - mean_ones)) / (x_train_size * x_length ** 2)\n",
    "\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameters and Loading <a id=\"hyper\"></a>\n",
    "\n",
    "We must set our hyperparameters for the model to use for training. The hyperparamters we are using are batch size, shuffle, and number of workers. Batch size can be tuned as needed to improve results. Shuffle should almost always be True since the data shouldn't be ordered in any specific way when training. In addition, the number of workers has a default of 0, which uses the main processor on the machine you are using. We also choose the number of epochs we wish to train for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepping arguments we have to feed to `DataLoader`\n",
    "params = {\n",
    "        'batch_size': 128,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0\n",
    "    }\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful metric to calculate is how many updates our model will perform during training. We can calculate this by finding the number of batches in the training set (number of training samples / batch size) and multiplying it by the number of epochs. Knowing how many batches our model might need to be well trained can be a good place to start when tuning hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('The model will train using a total of {} batches'.format(num_epochs * \n",
    "                                                       int(x_train_size / params['batch_size'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our hyperparameters set, we can load our training and test set using `DataLoader`. \n",
    "\n",
    "**Note the variable and function names in the notebook are directed for validation sets, but we will use them for the test set instead.** That being said, we use the definitions for validation set and test set interchangeably here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING SET\n",
    "train_loader = DataLoader(train_set, **params)\n",
    "\n",
    "# VALIDATION SET\n",
    "valid_loader = DataLoader(val_set, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will initialize our model again to be sure we are starting from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our loss function to be Mean Square Error. This function is standard in regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we choose our optimizer to be [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam), since the learning rate updates automatically and trains relatvely fast compared to [Stochastic Gradient Descent (SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),  weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have GPUs available, then those will be used for training. If not, then the model will train on CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the device to make sure we know what's available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Model <a id=\"train\"></a>\n",
    "\n",
    "In order to train our model, we have to manually loop through our data for training. This is probably the biggest difference between PyTorch and [Tensorflow](https://www.tensorflow.org/), but this allows for more hands-on manipulation of how training is performed, which can be advantageous. We will train our model as follows:\n",
    "1. Change the model to trianing mode to activate backpropogation\n",
    "2. Initialize training loss to be 0\n",
    "3. Loop through each batch of features by:\n",
    "    - Putting the data onto your device\n",
    "    - Calculating the outputs and the loss\n",
    "    - Performing backgrpopogation and adding the batch training loss to total training loss\n",
    "4. Normalize the total training loss by number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train loop\n",
    "\n",
    "def train_model(train_loader):\n",
    "\n",
    "    # Change model to training mode (activates backpropogation)\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize training loss\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Loop through batches of training data\n",
    "    for data in train_loader:\n",
    "        \n",
    "        # Put training batch on device\n",
    "        data = data.float().to(device)\n",
    "\n",
    "        # Calculate output and loss from training batch\n",
    "        output = model(data)\n",
    "        loss = distance(output, data)\n",
    "        \n",
    "        # Backpropogation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Normalize training loss from one epoch\n",
    "    train_loss_norm = train_loss / len(train_loader)\n",
    "    \n",
    "    return train_loss_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we define a similar loop for evaluating the test set at each epoch, which signals us if our model is generalizing. We will test our model as follows:\n",
    "1. Change model to evaluation mode to deactivate backpropogation\n",
    "2. Initialize test loss and variances to 0\n",
    "3. Loop through each batch of features by:\n",
    "    - Putting the data onto your device\n",
    "    - Calculating the outputs and the loss\n",
    "4. Calculate test set loss and normalize it by number of samples\n",
    "5. Calculate R2 score to determine how correlated the inputs are with the outputs (no correlation tends to 0, high correlation tends to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define validation loop\n",
    "\n",
    "def validate_model(valid_loader):\n",
    "\n",
    "    # Change model to evaluate mode (deactivates backpropogation)\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize validation loss and variances\n",
    "    val_loss = 0\n",
    "    data_variance = 0\n",
    "    res_variance = 0\n",
    "    \n",
    "    # Do not calculate gradients for the loop\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Loop through batches of validation data\n",
    "        for data in valid_loader:\n",
    "            \n",
    "            # Put validation batch on device\n",
    "            data = data.float().to(device)\n",
    "            \n",
    "            # Calculate output and loss from validation batch\n",
    "            output = model(data)\n",
    "            val_loss += distance(output, data).item()\n",
    "            \n",
    "            # Calculate variances\n",
    "            data = data.detach().numpy()\n",
    "            output = output.detach().numpy()\n",
    "            \n",
    "            data_mean = np.mean(data, axis=(1,2,3)).reshape(data.shape[0], 1, 1, 1)\n",
    "            data_var = np.nansum((data - data_mean)**2)\n",
    "            res_var = np.nansum((data - output)**2)\n",
    "            \n",
    "            data_variance += data_var\n",
    "            res_variance += res_var\n",
    "    \n",
    "    # Normalize validation loss from one epoch\n",
    "    val_loss_norm = val_loss / len(valid_loader)\n",
    "    \n",
    "    # Calculate r2 score\n",
    "    r2_score = 1 - res_variance / data_variance\n",
    "    \n",
    "    return val_loss_norm, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train our model! We will print out the train and test loss/R2 score per epoch to keep track of performance. The loop below performs the training and validation loops defined above and records our metrics. The last print statement will tell us how many times better our model is than our baseline with respect to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keep track of metrics\n",
    "lst_train_loss = []\n",
    "lst_val_loss = []\n",
    "lst_r2_score = []\n",
    "\n",
    "# training loop\n",
    "for epoch in tqdm(range(num_epochs), total=num_epochs):\n",
    "\n",
    "    # Go through loops\n",
    "    train_loss = train_model(train_loader)\n",
    "    val_loss, r2_score = validate_model(valid_loader)\n",
    "\n",
    "    # Append metrics\n",
    "    lst_train_loss.append(train_loss)\n",
    "    lst_val_loss.append(val_loss)\n",
    "    lst_r2_score.append(r2_score)\n",
    "\n",
    "    # Log\n",
    "    print('Epoch {:.3f} - Train loss: {:.4f} - Val Loss: {:.4f} - R2 Score: ({:.4f})'.format(\n",
    "            epoch, train_loss, val_loss, r2_score))\n",
    "print ('The model is performing {:.4f} times better than the baseline'.format(baseline / lst_train_loss[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plot Loss Function and R2 <a id=\"plot\"></a>\n",
    "\n",
    "We plot the train/test loss and R2 scores to determine how well converged our model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=[10,5])\n",
    "\n",
    "axs[0].set_title('Loss')\n",
    "axs[0].plot(np.arange(num_epochs), lst_train_loss, label='train')\n",
    "axs[0].plot(np.arange(num_epochs), lst_val_loss, label='val')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_title('R2 Score')\n",
    "axs[1].plot(np.arange(num_epochs), lst_r2_score, color='C1')\n",
    "axs[1].set_xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Samples <a id=\"analyze\">\n",
    "    \n",
    "Now that our model is trained, let's analyze some samples to see how well our images are being reconstructed.\n",
    "    \n",
    "First, we predict the outputs of our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(torch.Tensor(val_set).to(device))\n",
    "recon = output.detach().numpy()\n",
    "mse = np.sum((val_set - recon) ** 2, axis=(1,2,3)) / x_length ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can plot random samples, their reconstructions, and their squared residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose random image and corresponding output from test set\n",
    "rand_index = np.random.randint(x_test_size)\n",
    "rand_image = x_test_scale[rand_index]\n",
    "rand_recon = recon[rand_index][0]\n",
    "rand_sq_res = (rand_image - rand_recon) ** 2\n",
    "rand_mse = mse[rand_index]\n",
    "\n",
    "# plot input, output, and squared residuals\n",
    "fig, axs = plt.subplots(2,2,figsize=[10,10])\n",
    "axs[0,0].set_title('Testing Scaled Image {}'.format(rand_index))\n",
    "axs[0,0].imshow(rand_image)\n",
    "axs[0,1].set_title('Reconstructed Output'.format(rand_index))\n",
    "axs[0,1].imshow(rand_recon)\n",
    "axs[1,0].set_title('Squared Residual Image')\n",
    "axs[1,0].imshow(rand_sq_res)\n",
    "axs[1,1].set_title('Squared Residual Image (0-1 min-max)')\n",
    "axs[1,1].imshow(rand_sq_res, vmin=0, vmax=1)\n",
    "plt.tight_layout()\n",
    "\n",
    "print ('MSE: {:.4f}'.format(rand_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the distribution of MSEs to get a better understanding of how well our model reconstructs each test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,5])\n",
    "plt.title('Test Set MSE Distribution')\n",
    "plt.hist(mse, bins=50)\n",
    "plt.xlabel('mse')\n",
    "plt.ylabel('frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, let's see if we can distinguish the loss by class. If most of the distributions are within reason in relation to each other, then the model generalizes to all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,5])\n",
    "plt.title('Test Set MSE Distribution (by class)')\n",
    "for digit in range(10):\n",
    "    plt.hist(mse[y_test == digit], bins=50, label=digit, alpha=0.25)\n",
    "plt.xlabel('mse')\n",
    "plt.ylabel('frequency')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the model is performing a lot better than the baseline, there are still samples it struggles with. Let's see how many samples have a MSE more than 3 sigma above the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = mse.mean() + 3 * mse.std()\n",
    "mask = mse > threshold\n",
    "\n",
    "print ('There are {} MSEs above {:.4f}.'.format(mask.sum(), threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with our mask, we can look through \"poorly\" reconstructed samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose random incorrect image and corresponding output from test set\n",
    "rand_index = np.random.randint(mask.sum())\n",
    "rand_image = x_test_scale[mask][rand_index]\n",
    "rand_recon = recon[mask][rand_index][0]\n",
    "rand_sq_res = (rand_image - rand_recon) ** 2\n",
    "rand_mse = mse[mask][rand_index]\n",
    "\n",
    "# plot input, output, and squared residuals\n",
    "fig, axs = plt.subplots(2,2,figsize=[10,10])\n",
    "axs[0,0].set_title('Testing Masked Scaled Image {}'.format(rand_index))\n",
    "axs[0,0].imshow(rand_image)\n",
    "axs[0,1].set_title('Reconstructed Output'.format(rand_index))\n",
    "axs[0,1].imshow(rand_recon)\n",
    "axs[1,0].set_title('Squared Residual Image')\n",
    "axs[1,0].imshow(rand_sq_res)\n",
    "axs[1,1].set_title('Squared Residual Image (0-1 min-max)')\n",
    "axs[1,1].imshow(rand_sq_res, vmin=0, vmax=1)\n",
    "plt.tight_layout()\n",
    "\n",
    "print ('MSE: {:.4f}'.format(rand_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize the Latent Space <a id=\"latent\"></a>\n",
    "\n",
    "As mentioned in the [Introduction](#intro), one of the use cases of an autoencoder is to reduce the dimensionality of the dataset. Since our decoder is able to reconstruct our inputs to a high degree, that means our data is efficiently stored in the latent space. By using the encoder as a feature extractor, we can visualize the data in the latent space.\n",
    "\n",
    "First, we'll define [a function](https://discuss.pytorch.org/t/how-can-i-extract-intermediate-layer-output-from-loaded-cnn-model/77301/3) that extracts the features at an intermediate step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model.latent.register_forward_hook(get_activation('latent'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll transform the test set to the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = torch.Tensor(val_set)\n",
    "output = model(input_batch)\n",
    "features = activation['latent'].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,5])\n",
    "plt.title('Latent Space')\n",
    "for digit in range(10):\n",
    "    plt.scatter(features[:, 0][y_test==digit], features[:, 1][y_test==digit], label=digit, alpha=0.25)\n",
    "plt.xlabel('AE 1')\n",
    "plt.ylabel('AE 2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if the classes do not separate out distinctly, it's still impressive that the decoder is able to decipher what a digit will look like in a two dimensional space! There are a few things you can try to see if we can get better representation in the latent space.\n",
    "- Increase the number of dimensions of the latent space: currently we are using 2 dimensions because it's easy to plot and visualize. However, as the number of dimensions increases so does the amount of information the latent space can store. The classes could further be distinguished in these slightly higher dimensions (5-10), but still far lower than the original input space (784).\n",
    "- Increase the depth of the autoencoder: currently we are not activating the neurons from our final flattened feature maps to the latent space because that would constrain our reduced space. By increasing the depth of these fully connected layers, we can activate the neurons, adding nonlinearity and extracting even higher level features that could separate out the digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Detect Anomalies <a id=\"detect\"></a>\n",
    "\n",
    "As mentioned in the [Introduction](#intro), another use case of an autoencoder is to detect anomalies. Since it understands the features (straight lines, curves, etc.) that make up digits really well, it should not be able to reconstruct images outside of this domain. To demonstrate this, we will generate 10000 anomalies and compare the MSEs of their reconstructions to that of the test set.\n",
    "\n",
    "First, let's generate our \"anomalies\" and visualize what they look like. Our anomalies will be normally distributed noise with the same mean and variance as the scaled test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "anom = np.random.normal(x_test_scale.mean(), x_test_scale.std(), (n, 1, x_length, x_length))\n",
    "plt.imshow(anom[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can reconstruct the noise images using the autoencoder and plot an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_anom = model(torch.Tensor(anom)).detach().numpy()\n",
    "plt.imshow(output_anom[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate the loss of each anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_anom = np.sum((anom - output_anom) ** 2, axis=(1,2,3)) / x_length ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the losses and compare distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,10])\n",
    "plt.title('Test Set and Anomaly MSE Distributions')\n",
    "plt.hist(mse, bins=50, label='test set', alpha=0.5)\n",
    "plt.hist(mse_anom, bins=50, label='anom', alpha=0.5)\n",
    "plt.xlabel('mse')\n",
    "plt.ylabel('frequency')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The anomalies separate from the real data. Now if we were getting images in real time, we'd be able to distinguish between digits and noise by using a loss threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusions <a id=\"con\"></a>\n",
    "\n",
    "Thank you for walking through this notebook. Now you should be more familiar with:\n",
    "- building an autoencoder\n",
    "- training and evaluating an autoencoder\n",
    "- visualizing the latent space of an autoencoder\n",
    "- detecting anomalies using an autoencoder\n",
    "\n",
    "**Congratulations, you have completed the notebook!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources <a id=\"add\"></a>\n",
    "\n",
    "Machine learning is a dense and rapidly evolving field of study. Becoming an expert takes years of practice and patience, but hopefully this notebook brought you closer in that direction. Here are some of the author's favorite resources for learning about machine learning and data science:\n",
    "\n",
    "- [Google Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/ml-intro)\n",
    "- [scikit-learn Python Library](https://scikit-learn.org/stable/index.html) (go-to for most ML algorithms besides neural networks)\n",
    "- [StatQuest YouTube Channel](https://www.youtube.com/c/joshstarmer)\n",
    "- [DeepLearningAI YouTube Channel](https://www.youtube.com/c/Deeplearningai/videos)\n",
    "- [Towards Data Science](https://towardsdatascience.com/) (articles about data science and machine learning, some invlovling example blocks of code)\n",
    "- Advance searching [arxiv](https://arxiv.org/search/advanced) (e.g. search term \"machine learning\" in Abstract for Subject astro-ph) to see what others are doing currently\n",
    "- Google, YouTube, and Wikipedia in general\n",
    "\n",
    "## About this Notebook <a id=\"about\"></a>\n",
    "\n",
    "**Author:** Fred Dauphin, DeepWFC3\n",
    "\n",
    "**Updated on:** 2021-12-03\n",
    "\n",
    "## Citations <a id=\"cite\"></a>\n",
    "\n",
    "If you use `numpy`, `matplotlib`, or `torch` for published research, please cite the authors. Follow these links for more information about citing `numpy`, `matplotlib`, and `torch`:\n",
    "\n",
    "* [Citing `numpy`](https://numpy.org/doc/stable/license.html)\n",
    "* [Citing `matplotlib`](https://matplotlib.org/stable/users/project/license.html#:~:text=Matplotlib%20only%20uses%20BSD%20compatible,are%20acceptable%20in%20matplotlib%20toolkits.)\n",
    "* [Citing `torch`](https://github.com/pytorch/pytorch/blob/master/LICENSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "[Top of Page](#title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "wfc3-ir-blob-finder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

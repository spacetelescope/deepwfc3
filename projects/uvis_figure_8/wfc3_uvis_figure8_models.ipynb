{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WFC3 Figure-8 Ghost Classification using Convolutional Neural Networks (CNNs)\n",
    "---\n",
    "\n",
    "The purpose of the notebook is to demonstrate how to use a DeepWFC3 model to predict if a WFC3 image contains a figure-8 ghost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports <a id=\"imports\"></a>\n",
    "\n",
    "If you are running this notebook in Jupyter, this notebook assumes you created the virtual environment defined in `environment.yml`. If not, close this notebook and run the following lines in a terminal window:\n",
    "\n",
    "`conda env create -f environment.yml`\n",
    "\n",
    "`conda activate deepwfc3_env`\n",
    "\n",
    "We import the following libraries:\n",
    "- *numpy* for handling arrays\n",
    "- *matplotlib* for plotting\n",
    "- *torch* as our machine learning framework\n",
    "\n",
    "We also import functions from various model `utils.py` to load the models. In addition, `model_a_utils.py` has functions to further process images and produce saliency maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from model_a.model_a_utils import process_image, load_wfc3_fig8_model_a, saliency_map\n",
    "from model_b.model_b_utils import load_wfc3_fig8_model_b\n",
    "from model_c.model_c_utils import load_wfc3_fig8_model_c\n",
    "from model_d.model_d_utils import load_wfc3_fig8_model_d\n",
    "from model_syn.model_syn_utils import load_wfc3_fig8_model_syn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "`examples.npz` is a compressed numpy file containing two WFC3 images post processing pipeline (see `process_uvis_image.py`). The first image is a null image of the galaxy N5643 (idgg69pmq) and the second is the globular cluster NGC-6752 (ibhf01sjq), which contains a figure-8 ghost.\n",
    "\n",
    "We load the images using `np.load()` and transform them to `torch.Tensor` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_0, example_1 = np.load('examples.npz')['examples']\n",
    "\n",
    "example_0_torch = torch.Tensor(example_0.reshape(1,1,256,256))\n",
    "example_1_torch = torch.Tensor(example_1.reshape(1,1,256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Data to ImageNet \n",
    "\n",
    "Since Model A was pretrained using [ImageNet](https://www.image-net.org/), we need to scale our examples to match the statistics of the dataset. We do this by:\n",
    "- min-max scaling the images to have a minimum/maximum pixel value of 0/1\n",
    "- making three copies of our examples to use as \"RGB channels\"\n",
    "- center cropping to a 224x224\n",
    "- normalizing the channels to $N(\\mu=(0.485, 0.456, 0.406), \\sigma=(0.229, 0.224, 0.225))$\n",
    "\n",
    "See some [documentation](https://pytorch.org/hub/pytorch_vision_googlenet/) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_0_process = process_image(example_0)\n",
    "example_1_process = process_image(example_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models\n",
    "\n",
    "We load the five models described in the [insert ISR here]. Here are their brief descriptions:\n",
    "\n",
    "- Model A uses the [GoogLeNet](https://arxiv.org/pdf/1409.4842.pdf) architecture\n",
    "- Model B uses 3 convolutional layers and 4 fully connected layers\n",
    "- Model C uses 3 convolutional layers and 3 fully connected layers\n",
    "- Model D uses 3 convolutional layers and 4 fully connected layers\n",
    "- Model Syn uses 2 convolutional layers and 3 fully connected layers (LeNet architecture)\n",
    "\n",
    "The model is saved as `wfc3_fig8_model_{name}.torch` and can be loaded using `load_wfc3_fig8_model_{name}()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = load_wfc3_fig8_model_a('model_a/wfc3_fig8_model_a.torch')\n",
    "model_b = load_wfc3_fig8_model_b('model_b/wfc3_fig8_model_b.torch')\n",
    "model_c = load_wfc3_fig8_model_c('model_c/wfc3_fig8_model_c.torch')\n",
    "model_d = load_wfc3_fig8_model_d('model_d/wfc3_fig8_model_d.torch')\n",
    "model_syn = load_wfc3_fig8_model_syn('model_syn/wfc3_fig8_model_syn.torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Examples\n",
    "\n",
    "To predict the example classifications, we use them as arguments for the models, which returns the last two output neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A\n",
    "pred_0_a = model_a(example_0_process)\n",
    "pred_1_a = model_a(example_1_process)\n",
    "\n",
    "# Model B\n",
    "pred_0_b = model_b(example_0_torch)\n",
    "pred_1_b = model_b(example_1_torch)\n",
    "\n",
    "# Model C\n",
    "pred_0_c = model_c(example_0_torch)\n",
    "pred_1_c = model_c(example_1_torch)\n",
    "\n",
    "# Model D\n",
    "pred_0_d = model_d(example_0_torch)\n",
    "pred_1_d = model_d(example_1_torch)\n",
    "\n",
    "# Model Syn\n",
    "pred_0_syn = model_syn(example_0_torch)\n",
    "pred_1_syn = model_syn(example_1_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index of the greatest neuron output is the prediction, e.g. [-3, 2] would be classified as having a figure-8 ghost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Model A Results:')\n",
    "print ('Example 0 Model A Output Neurons: {}'.format(pred_0_a))\n",
    "print ('Example 1 Model A Output Neurons: {}'.format(pred_1_a))\n",
    "print ('')\n",
    "\n",
    "print ('Model B Results:')\n",
    "print ('Example 0 Model B Output Neurons: {}'.format(pred_0_b))\n",
    "print ('Example 1 Model B Output Neurons: {}'.format(pred_1_b))\n",
    "print ('')\n",
    "\n",
    "print ('Model C Results:')\n",
    "print ('Example 0 Model C Output Neurons: {}'.format(pred_0_c))\n",
    "print ('Example 1 Model C Output Neurons: {}'.format(pred_1_c))\n",
    "print ('')\n",
    "\n",
    "print ('Model D Results:')\n",
    "print ('Example 0 Model D Output Neurons: {}'.format(pred_0_d))\n",
    "print ('Example 1 Model D Output Neurons: {}'.format(pred_1_d))\n",
    "print ('')\n",
    "\n",
    "print ('Model Syn Results:')\n",
    "print ('Example 0 Model Syn Output Neurons: {}'.format(pred_0_syn))\n",
    "print ('Example 1 Model Syn Output Neurons: {}'.format(pred_1_syn))\n",
    "print ('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Saliency Maps\n",
    "\n",
    "We can view the [saliency maps](https://arxiv.org/pdf/1312.6034.pdf) our models produce for the examples by using `saliency_map()`, which prints the prediction probabilities, and plots the original image and the saliency map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model A\n",
    "print ('Model A')\n",
    "sm_0_a = saliency_map(model_a, example_0_process)\n",
    "sm_1_a = saliency_map(model_a, example_1_process)\n",
    "print ('#######################################')\n",
    "\n",
    "# Model B\n",
    "print ('Model B')\n",
    "sm_0_b = saliency_map(model_b, example_0_torch)\n",
    "sm_1_b = saliency_map(model_b, example_1_torch)\n",
    "print ('#######################################')\n",
    "\n",
    "# Model C\n",
    "print ('Model C')\n",
    "sm_0_c = saliency_map(model_c, example_0_torch)\n",
    "sm_1_c = saliency_map(model_c, example_1_torch)\n",
    "print ('#######################################')\n",
    "\n",
    "# Model D\n",
    "print ('Model D')\n",
    "sm_0_d = saliency_map(model_d, example_0_torch)\n",
    "sm_1_d = saliency_map(model_d, example_1_torch)\n",
    "print ('#######################################')\n",
    "\n",
    "# Model Syn\n",
    "print ('Model Syn')\n",
    "sm_0_syn = saliency_map(model_syn, example_0_torch)\n",
    "sm_1_syn = saliency_map(model_syn, example_1_torch)\n",
    "print ('#######################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions <a id=\"con\"></a>\n",
    "\n",
    "Thank you for walking through this notebook. Now you should be more familiar with using our models to predict if figure-8 ghosts are on WFC3 images.\n",
    "\n",
    "## About this Notebook <a id=\"about\"></a>\n",
    "\n",
    "**Author:** DeepWFC3 team: Fred Dauphin, Mireia Montes, Nilufar Easmin, Varun Bajaj, Peter McCullough\n",
    "\n",
    "**Updated on:** 2022-03-07\n",
    "\n",
    "## Citations <a id=\"cite\"></a>\n",
    "\n",
    "If you use `numpy`, `matplotlib`, or `torch` for published research, please cite the authors. Follow these links for more information about citing `numpy`, `matplotlib`, and `torch`:\n",
    "\n",
    "* [Citing `numpy`](https://numpy.org/doc/stable/license.html)\n",
    "* [Citing `matplotlib`](https://matplotlib.org/stable/users/project/license.html#:~:text=Matplotlib%20only%20uses%20BSD%20compatible,are%20acceptable%20in%20matplotlib%20toolkits.)\n",
    "* [Citing `torch`](https://github.com/pytorch/pytorch/blob/master/LICENSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

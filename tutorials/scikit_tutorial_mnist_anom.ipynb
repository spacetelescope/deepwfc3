{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"title\"></a>\n",
    "# Anomaly Detection using LOF, iForest, and OC-SVM\n",
    "\n",
    "This notebook assumes you are familiar with basic machine learning vocabulary.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "[Introduction](#intro) <br>\n",
    "[0. Imports](#imports) <br>\n",
    "[1. MNIST dataset and scaling](#mnist) <br>\n",
    "[2. Reduce MNIST using UMAP](#umap) <br>\n",
    "[3. Local Outlier Factor (LOF)](#lof) <br>\n",
    "- [3a. Fit, predict and visualize using training set](#lof_train) <br>\n",
    "- [3b. Predict test set and visualize boundaries](#lof_test) <br>\n",
    "- [3c. LOF Distributions](#loc_adv) <br>\n",
    "\n",
    "[4. Isolation Forest (iForest)](#if) <br>\n",
    "- [4a. Fit, predict, and visualize using training set](#if_train) <br>\n",
    "- [4b. Predict test set and visualize boundaries](#if_test) <br>\n",
    "- [4c. iForest Distributions and Trees](#if_adv) <br>\n",
    "\n",
    "[5. One Class Support Vector Machine (OC-SVM)](#svm) <br>\n",
    "- [5a. Fit, predict, and visualize using training set](#svm_train) <br>\n",
    "- [5b. Predict test set and visualize boundaries](#svm_test) <br>\n",
    "- [5c. OC-SVM Distributions](#svm_adv) <br>\n",
    "\n",
    "[6. Conclusions](#con) <br>\n",
    "[Additional Resources](#add) <br>\n",
    "[About this Notebook](#about) <br>\n",
    "[Citations](#cite) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "## Introduction\n",
    "\n",
    "Finding anomalies, or outliers, in data is a difficult, yet crucial task for analysis, modeling, and science. Anomaly detection is a subset of machine learning techniques that determines outliers within a data set. We can detect anomalies to better understand the data set's complexity, and the extrema of its distribution. After detection, anomalies can be followed up for further investigation, which may provide insight into the data collection/processing procedures, or cleaned from the dataset to enhance modeling and prediction.\n",
    "\n",
    "**The purpose of this notebook is to demonstrate Local Outlier Factor (LOF), Isolation Forest (iForest), and One Class Support Vector Machine (OC-SVM) as anomaly detection techniques on the MNIST dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"imports\"></a>\n",
    "## 0. Imports\n",
    "\n",
    "We use `numpy` for arrays, `matplotlib` for plotting, `tensorflow` for loading MNIST, and `umap` for reducing MNIST from a high dimensional space to a low dimensional space. In addition, we `sklearn` for our anomaly detection algorithms.\n",
    "\n",
    "If you do not have some of the packages, please follow the installation guides:\n",
    "- [Numpy](https://numpy.org/install/)\n",
    "- [Matplotlib](https://matplotlib.org/stable/users/installing/index.html)\n",
    "- Tensorflow ([conda](https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/) and [pip](https://www.tensorflow.org/install))\n",
    "- [UMAP](https://umap-learn.readthedocs.io/en/latest/)\n",
    "- [Scikit-learn](https://scikit-learn.org/stable/install.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.ensemble._iforest import _average_path_length\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.svm import OneClassSVM\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mnist\"></a>\n",
    "## 1. MNIST dataset and scaling\n",
    "\n",
    "MNIST is a popular image dataset of handwritten digits from 0 to 9. We use it to showcase the different clustering techniques. Here are some qualities of the dataset:\n",
    "- 60k training samples\n",
    "- 10k testing samples\n",
    "- 10 classifications (digits 0-9)\n",
    "- 28x28 images\n",
    "- 8-bit gray scaled (0-255 pixel values)\n",
    "\n",
    "Why is MNIST such a good dataset to use for learning ML? \n",
    "- Relatively small images (784 features)\n",
    "- Relatively large dataset (70k samples)\n",
    "- 10 unique well defined labels (all the digits are clearly different from each other)\n",
    "- Very clean dataset (little noise)\n",
    "    - Backgorund pixels are 0 and signal pixels are nearly 255 so it approximates a binomial distibution\n",
    "    - The digits are well centered, meaning pixels for similar parts of a digit should consistently be in the same vicinity\n",
    "    \n",
    "We retrieve our data using `tensorflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some global variables and min-max normalize the images so pixels range between 0-1 (normalizing data is a common practice in machine learning). We also flatten our 28x28 images into a 784 feature arrays, in which each pixel is a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "x_train_size = x_train.shape[0]\n",
    "x_test_size = x_test.shape[0]\n",
    "x_length = x_train.shape[1]\n",
    "norm = x_train.max()\n",
    "rs = 42\n",
    "\n",
    "# Scale images\n",
    "x_train_scale = x_train / norm\n",
    "x_test_scale = x_test / norm\n",
    "\n",
    "# Flatten arrays\n",
    "x_train_scale_flat = x_train_scale.reshape(x_train_size, x_length ** 2)\n",
    "x_test_scale_flat = x_test_scale.reshape(x_test_size, x_length ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 16 samples in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,4,figsize=[10,10])\n",
    "for i in range (4):\n",
    "    for j in range (4):\n",
    "        axs[i,j].imshow(x_train_scale[i*4+j])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"umap\"></a>\n",
    "## 2. Reduce MNIST using UMAP\n",
    "\n",
    "Before detecting anomalies in MNIST, we use dimensionality reduction to reduce our data from a 784D space to a 2D space. In a high dimensional space, the distance between most samples are too similar (i.e. curse of dimensionality). Since most anomaly detection techniques use a distance metric for determining anomalies, this quality of our data is a clear weakness. However, dimensionality reduction embeds similar samples near one another, and disimilar samples away from one another, making the distance metric more viable. Here, we use Uniform Manifold Approximation and Projection (UMAP) as our dimensionality reduction technique for its quality in embedding data, and its fast performance speed. For more information on this technique and dimensionality reduction in general, see `scikit_tutorial_mnist_dimred.ipynb`.\n",
    "\n",
    "The two main hyperparameters for UMAP are `n_neighbors` and `min_dist`. The former determines the number of neighbors used for the local approximation, and larger values conserve more global structure. The latter determines how tight the data is in the reduced space, and smaller values conserve more local structure. Tuning these hyperparameters smoothly changes the overall embedding. We reduce to two components and choose the default hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit and transform the scaled MNIST training data using UMAP.\n",
    "\n",
    "**Note: This should take no longer than a few minutes on a standard Mac laptop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_mnist_train = umap.fit_transform(x_train_scale_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We confirm the size of the reduced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_mnist_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the data with labels, where each data point represents one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=[20,10])\n",
    "axs[0].grid()\n",
    "axs[0].set_title('UMAP MNIST (Train; Unlabeled)')\n",
    "axs[0].scatter(umap_mnist_train[:, 0], umap_mnist_train[:, 1], s=1, alpha=0.5)\n",
    "axs[0].set_xlabel('UMAP1')\n",
    "axs[0].set_ylabel('UMAP2')\n",
    "\n",
    "axs[1].grid()\n",
    "axs[1].set_title('UMAP MNIST (Train; Labeled)')\n",
    "for i in range (10):\n",
    "    mask = y_train == i\n",
    "    axs[1].scatter(umap_mnist_train[:, 0][mask], umap_mnist_train[:, 1][mask], \n",
    "                   s=1, alpha=0.5, label=i, color='C{}'.format(i))\n",
    "axs[1].set_xlabel('UMAP1')\n",
    "axs[1].set_ylabel('UMAP2')\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP does an excellent job reducing MNIST to separate digits with little noise. \n",
    "\n",
    "We also tranform the test set to UMAP space, check its size, and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_mnist_test = umap.transform(x_test_scale_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_mnist_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=[20,10])\n",
    "axs[0].grid()\n",
    "axs[0].set_title('UMAP MNIST (Test; Unlabeled)')\n",
    "axs[0].scatter(umap_mnist_test[:, 0], umap_mnist_test[:, 1], s=1, alpha=0.5)\n",
    "axs[0].set_xlabel('UMAP1')\n",
    "axs[0].set_ylabel('UMAP2')\n",
    "\n",
    "axs[1].grid()\n",
    "axs[1].set_title('UMAP MNIST (Test; Labeled)')\n",
    "for i in range (10):\n",
    "    mask = y_test == i\n",
    "    axs[1].scatter(umap_mnist_test[:, 0][mask], umap_mnist_test[:, 1][mask], \n",
    "                   s=1, alpha=0.5, label=i, color='C{}'.format(i))\n",
    "axs[1].set_xlabel('UMAP1')\n",
    "axs[1].set_ylabel('UMAP2')\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the test set samples fall directly on the training set samples, we confirm UMAP has generalized to new data. Now with the UMAP MNIST data, we can detect anomalies as if they were unlabeled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lof\"></a>\n",
    "## 3. Local Outlier Factor (LOF)\n",
    "\n",
    "Local outlier factor [(LOF; detailed explanation with equations can be found under \"Formal\")](https://en.wikipedia.org/wiki/Local_outlier_factor) is a density-based anomaly detection algorithm that identifies outliers with respect to local data instead of global data. The objective of LOF is to find samples in low density regions with respect to the samples' k-nearest neighbors. First, we find a sample's reachability distance (similar to DBSCAN). Then, we find a sample's local reachability density, which is the harmonic mean of the sample's k-nearest neighbors' reachability distances. Lastly, we find a sample's local outlier factor by taking the ratio of the arithmetic mean of the samples' k-nearest neighbors' local reachability densities to the sample's local reachability density. After finding each samples' local outlier factors, a threshold is applied to determine which samples are outliers. Inliers have small values, while outliers have large values.\n",
    "\n",
    "LOF is relatively fast, and effective at modeling imbalanced data and data of varying densities. However, LOF scales quadratically with respect to the data's dimensionality. In addition, the same threshold may not work on different data sets. Domain knowledge and analyzing how different thresholds detect outliers are necessary for optimal results. Many improvements to LOF have been developed, but are beyond the scope of this tutorial.\n",
    "\n",
    "Here are some complementary resources:\n",
    "\n",
    "- [LOF Overview Video (5 minues, simple language)](https://www.youtube.com/watch?v=Ymvq6JHjoBY)\n",
    "- [LOF Step by Step Video (6 minutes, simple language)](https://www.youtube.com/watch?v=7L23sCOZjns)\n",
    "- [LOF Detailed Video (16 minutes, intermediate lanuage)](https://www.youtube.com/watch?v=9-PHBzI_rDk)\n",
    "- [LOF Original Paper](https://dl.acm.org/doi/pdf/10.1145/335191.335388)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lof_train\"></a>\n",
    "### 3a. Fit, predict, and visualize using training set\n",
    "\n",
    "We use [scikit-learn for LOF](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html). The main parameter for LOF is `n_neighbors`, which defines the number of nearest neighbors to consider for outlier detection. We use `LocalOutlierFactor` default values, which are 20 nearest neighbors, and an offset (i.e. outlier score threshold) of 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof = LocalOutlierFactor(n_neighbors=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a baseline, we fit and predict outliers using the pixels as features. \n",
    "\n",
    "**Note: this may take a couple of minutes to execute.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_mnist_train = lof.fit_predict(x_train_scale_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shape to make sure we have a vector of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_mnist_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we plot the UMAP training set with the training labels and detected outliers from pixel features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "plt.grid()\n",
    "plt.title(f'UMAP MNIST (Train; LOF (Pixels); {lof.n_neighbors} Neighbors)')\n",
    "mask = lof_mnist_train == -1\n",
    "plt.scatter(\n",
    "    umap_mnist_train[:, 0][~mask], umap_mnist_train[:, 1][~mask], \n",
    "    s=1, alpha=0.5, c=y_train[~mask], cmap='tab10'\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.scatter(\n",
    "    umap_mnist_train[:, 0][mask], umap_mnist_train[:, 1][mask], \n",
    "    s=10, alpha=1, label=f'Anomalies', color='k'\n",
    ")\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the scaled pixels, LOF biased a lot of outliers towards the 1's (orange cluster). However, a good portion of outliers were on the borders of the other clusters.\n",
    "\n",
    "We plot the 10 most anomalous train samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.argsort(lof.negative_outlier_factor_)\n",
    "fig, axs = plt.subplots(2,5,figsize=[20,5])\n",
    "for i in range (2):\n",
    "    for j in range (5):\n",
    "        ind = inds[i*5+j]\n",
    "        lof_nof = lof.negative_outlier_factor_[ind]\n",
    "        axs[i, j].set_title(f'Label: {y_train[ind]}\\nNegative Outlier Factor: {lof_nof:.3f}')\n",
    "        axs[i, j].imshow(x_train_scale[ind])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few slanted 8s and 2s were detected along with a 1 and 7.\n",
    "\n",
    "Using a more efficient data representation will yield better results. Thus, we fit and predict anomalies using the UMAP MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof = LocalOutlierFactor(n_neighbors=20)\n",
    "\n",
    "t0_lof = time()\n",
    "lof_mnist_train = lof.fit_predict(umap_mnist_train)\n",
    "t1_lof = time()\n",
    "\n",
    "t_lof = t1_lof - t0_lof\n",
    "print (f'Time spent fitting model: {t_lof:.4f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOF fit exceptionally fast. Now, we plot the UMAP training set with the training labels and detected outliers from the UMAP embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "plt.grid()\n",
    "plt.title(f'UMAP MNIST (Train; LOF; {lof.n_neighbors} Neighbors)')\n",
    "mask = lof_mnist_train == -1\n",
    "plt.scatter(\n",
    "    umap_mnist_train[:, 0][~mask], umap_mnist_train[:, 1][~mask], \n",
    "    s=1, alpha=0.5, c=y_train[~mask], cmap='tab10'\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.scatter(\n",
    "    umap_mnist_train[:, 0][mask], umap_mnist_train[:, 1][mask], \n",
    "    s=10, alpha=1, label=f'Anomalies', color='k'\n",
    ")\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly all of the anomalies are on the borders of the clusters, which matches our intuition. \n",
    "\n",
    "We plot the 10 most anomalous train samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.argsort(lof.negative_outlier_factor_)\n",
    "fig, axs = plt.subplots(2,5,figsize=[20,5])\n",
    "for i in range (2):\n",
    "    for j in range (5):\n",
    "        ind = inds[i*5+j]\n",
    "        lof_nof = lof.negative_outlier_factor_[ind]\n",
    "        axs[i, j].set_title(f'Label: {y_train[ind]}\\nNegative Outlier Factor: {lof_nof:.3f}')\n",
    "        axs[i, j].imshow(x_train_scale[ind])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few 2s with a slanted base were detected along with two 8s with missing pixels.\n",
    "\n",
    "Next, we fit using 100 nearest neighbors, and produce similar plots as above. We also set the parameter `novelty=True` to predict new samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit using 100 nearest neighbors (predict is not available when novelty=True)\n",
    "lof = LocalOutlierFactor(n_neighbors=100, novelty=True)\n",
    "lof.fit(umap_mnist_train)\n",
    "lof_mnist_train_nof = lof.negative_outlier_factor_\n",
    "lof_mnist_train = (lof_mnist_train_nof > lof.offset_).astype(int)\n",
    "lof_mnist_train[lof_mnist_train==0] = -1\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.grid()\n",
    "plt.title(f'UMAP MNIST (Train; LOF; {lof.n_neighbors} Neighbors)')\n",
    "mask = lof_mnist_train == -1\n",
    "plt.scatter(\n",
    "    umap_mnist_train[:, 0][~mask], umap_mnist_train[:, 1][~mask], \n",
    "    s=1, alpha=0.5, c=y_train[~mask], cmap='tab10'\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.scatter(\n",
    "    umap_mnist_train[:, 0][mask], umap_mnist_train[:, 1][mask], \n",
    "    s=10, alpha=1, label=f'Anomalies', color='k'\n",
    ")\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all of the anomalies are on the borders of the clusters. \n",
    "\n",
    "Again, we plot the 10 most anomalous train samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.argsort(lof_mnist_train_nof)\n",
    "fig, axs = plt.subplots(2,5,figsize=[20,5])\n",
    "for i in range (2):\n",
    "    for j in range (5):\n",
    "        ind = inds[i*5+j]\n",
    "        lof_nof = lof_mnist_train_nof[ind]\n",
    "        axs[i, j].set_title(f'Label: {y_train[ind]}\\nNegative Outlier Factor: {lof_nof:.3f}')\n",
    "        axs[i, j].imshow(x_train_scale[ind])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several uncharacteristic 7s were detected along with a 0 and an 8.\n",
    "\n",
    "Finally, we print the number of anomalies detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'Number of Anomalies Detected: {(lof_mnist_train == -1).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around less than half of a percent of samples were found to be anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lof_test\"></a>\n",
    "### 3b. Predict test set and visualize boundaries\n",
    "\n",
    "Now that LOF is trained, we predict outliers in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_mnist_test_nof = lof.score_samples(umap_mnist_test)\n",
    "lof_mnist_test = (lof_mnist_test_nof > lof.offset_).astype(int)\n",
    "lof_mnist_test[lof_mnist_test==0] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the test set with the test labels and the detected outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "plt.grid()\n",
    "plt.title(f'UMAP MNIST (Test; LOF; {lof.n_neighbors} Neighbors)')\n",
    "mask = lof_mnist_test == -1\n",
    "plt.scatter(\n",
    "    umap_mnist_test[:, 0][~mask], umap_mnist_test[:, 1][~mask], \n",
    "    s=1, alpha=0.5, c=y_test[~mask], cmap='tab10'\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.scatter(\n",
    "    umap_mnist_test[:, 0][mask], umap_mnist_test[:, 1][mask], \n",
    "    s=10, alpha=1, label=f'Anomalies', color='k'\n",
    ")\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOF generalizes to the test set as well.\n",
    "\n",
    "We also produce a similar plot of the detected outliers with rings proportional to the respective outlier factors (i.e. larger rings are more anomalous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "plt.grid()\n",
    "plt.title(f'UMAP MNIST (Test; LOF; {lof.n_neighbors} Neighbors)')\n",
    "mask = lof_mnist_test == -1\n",
    "plt.scatter(\n",
    "    umap_mnist_test[:, 0][~mask], umap_mnist_test[:, 1][~mask], \n",
    "    s=1, alpha=0.5, c=y_test[~mask], cmap='tab10'\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.scatter(\n",
    "    umap_mnist_test[:, 0][mask], umap_mnist_test[:, 1][mask], \n",
    "    s=1, alpha=1, label=f'Anomalies', color='k'\n",
    ")\n",
    "plt.scatter(\n",
    "    umap_mnist_test[:, 0][mask], umap_mnist_test[:, 1][mask], \n",
    "    s=-lof_mnist_test_nof[mask]*100, facecolor='none', edgecolor='k'\n",
    ")\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers near clusters have smaller rings, and outliers far from clusters have larger rings.\n",
    "\n",
    "Similarly, we plot the 10 most anomalous test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.argsort(lof_mnist_test_nof)\n",
    "fig, axs = plt.subplots(2,5,figsize=[20,5])\n",
    "for i in range (2):\n",
    "    for j in range (5):\n",
    "        ind = inds[i*5+j]\n",
    "        lof_nof = lof_mnist_test_nof[ind]\n",
    "        axs[i, j].set_title(f'Label: {y_test[ind]}\\nNegative Outlier Factor: {lof_nof:.3f}')\n",
    "        axs[i, j].imshow(x_test_scale[ind])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We detect several digits that are out of distribution.\n",
    "\n",
    "To visualize the outlier boundaries, we can predict the outlier scores on a grid of points in our manifold. We define a function that creates a grid of points based on our features, plots the labeled and anomalous data, and plots the outlier detection grid with high transparency to act as a background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outlier_boundaries(X, Y, Y_pred, model, name, num=200):\n",
    "    \"\"\"Plot outlier boundaries.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        Features as a 2D array.\n",
    "    Y : np.array\n",
    "        Labels as a 1D array.\n",
    "    Y_pred : np.array\n",
    "        Predictions as a 1D array.\n",
    "    model : sklearn.model\n",
    "        Anomaly detection model to predict outliers.\n",
    "    name : str\n",
    "        Name for the plot. Allowed values are 'LOF', 'iForest', and 'OC-SVM'.\n",
    "    num : int, default=200\n",
    "        Number of x/y coordinates. The number of grid points is num**2.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Z_grid : np.array\n",
    "        Outlier labels for grid of points as a 2D array (num, num).\n",
    "    \"\"\"\n",
    "    # Make grid points\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xs = np.linspace(x_min, x_max, num)\n",
    "    ys = np.linspace(y_min, y_max, num)\n",
    "    x_grid, y_grid = np.meshgrid(xs, ys)\n",
    "    grid_coords = np.c_[x_grid.ravel(), y_grid.ravel()].astype(np.float32)\n",
    "\n",
    "    # Predict outlier labels of grid points\n",
    "    if name in ['LOF', 'iForest', 'OC-SVM']:\n",
    "        Z = model.predict(grid_coords)\n",
    "    else:\n",
    "        raise ValueError(f'`name` is {name}. Use \"LOF\", \"iForest\", or \"OC-SVM\".')\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=[10,10])\n",
    "    plt.title(f'{name} Outlier Boundaries')\n",
    "    for i in range (Y.max() + 1):\n",
    "        # Data points\n",
    "        mask_xy = Y == i\n",
    "        plt.scatter(\n",
    "            X[:, 0][mask_xy], X[:, 1][mask_xy], \n",
    "            color=f'C{i}', s=1, alpha=0.5, label=i\n",
    "        )\n",
    "    # Outliers\n",
    "    mask_z = Z == -1\n",
    "    plt.scatter(\n",
    "        grid_coords[:, 0][mask_z], grid_coords[:, 1][mask_z], \n",
    "        color='k', alpha=.025, marker='s'\n",
    "    )\n",
    "    mask_xy = Y_pred == -1\n",
    "    plt.scatter(\n",
    "        X[:, 0][mask_xy], X[:, 1][mask_xy], \n",
    "        color='k', s=10, alpha=1, label=f'Outliers'\n",
    "    )\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel('UMAP1')\n",
    "    plt.ylabel('UMAP2')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    Z_grid = Z.reshape(x_grid.shape)\n",
    "    return Z_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_grid_lof = plot_outlier_boundaries(umap_mnist_train, y_train, lof_mnist_train, lof, 'LOF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LOF outlier boundaries are relatively smooth around each cluster. Any point outside of a cluster is detected as an anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lof_adv\"></a>\n",
    "### 3c. LOF Distributions\n",
    "\n",
    "As previously mentioned, LOF assigns an outlier score to each sample, and detects an anomaly as having a score greater than 1.5 (or a negative outlier factor less than -1.5). We plot the distributions of the outlier scores for the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid()\n",
    "plt.title('LOF Outlier Score Histogram')\n",
    "plt.hist(lof_mnist_train_nof, bins=200, alpha=0.5, label='train')\n",
    "plt.hist(lof_mnist_test_nof, bins=200, alpha=0.5, label='test')\n",
    "plt.vlines(lof.offset_, 0, 10**4, color='k', label='offset')\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outlier scores are exponentially distributed with rarer samples having extreme negative scores.\n",
    "\n",
    "In addition, we can calculate the outlier score of the manifold using LOF. Samples that are more likely to be observed have an outlier score closer to 0. By visualizing the outlier score distribution on the manifold, we can determine where scores are similar. We define a function to plot contour lines showing the outlier score of samples across the UMAP manifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outlier_contour(X, Y, Y_pred, model, name, num=200, levels=[], tree=None):\n",
    "    \"\"\"Plot outlier score contour lines.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        Features as a 2D array.\n",
    "    Y : np.array\n",
    "        Labels as a 1D array.\n",
    "    Y_pred : np.array\n",
    "        Predictions as a 1D array.\n",
    "    model : sklearn.model\n",
    "        Anomaly detection model to predict outliers.\n",
    "    name : str\n",
    "        Name for the plot. Allowed values are 'LOF', 'IF', and 'OC-SVM'.\n",
    "    num : int, default=200\n",
    "        Number of x/y coordinates. The number of grid points is num**2.\n",
    "    levels : array-like, default=[]\n",
    "        The levels for the countour lines.\n",
    "    tree : int, default=None\n",
    "        If the model is an isolation forest, choose a tree to visualize.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Z_grid_scores : np.array\n",
    "        Outlier scores for grid of points as a 2D array (num, num).\n",
    "    \"\"\"\n",
    "    # Make grid points\n",
    "    length = Y.max() + 1\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xs = np.linspace(x_min, x_max, num)\n",
    "    ys = np.linspace(y_min, y_max, num)\n",
    "    x_grid, y_grid = np.meshgrid(xs, ys)\n",
    "    grid_coords = np.c_[x_grid.ravel(), y_grid.ravel()].astype(np.float32)\n",
    "\n",
    "    # Raise error if incorrect name\n",
    "    if name not in ['LOF', 'iForest', 'OC-SVM']:\n",
    "        raise ValueError(f'`name` is {name}. Use \"LOF\", \"iForest\", or \"OC-SVM\".')\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=[12,10])\n",
    "    plt.grid()\n",
    "    tree_str = ''\n",
    "    if name == 'iForest' and isinstance(tree, int):\n",
    "        tree_str = f'(Tree {tree})'\n",
    "    plt.title(f'{name} Contours {tree_str}')\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel('UMAP1')\n",
    "    plt.ylabel('UMAP2')\n",
    "\n",
    "    # Plot data points\n",
    "    for i in range (length):\n",
    "        mask_xy = Y == i\n",
    "        plt.scatter(\n",
    "            X[:, 0][mask_xy], X[:, 1][mask_xy], \n",
    "            s=1, alpha=0.5, label=i\n",
    "        )\n",
    "    if name == 'iForest' and isinstance(tree, int):\n",
    "        Y_pred = Y_pred[tree]\n",
    "    mask_xy = Y_pred == -1\n",
    "    plt.scatter(\n",
    "        X[:, 0][mask_xy], X[:, 1][mask_xy], \n",
    "        color='k', s=10, alpha=1, label=f'Outliers'\n",
    "    )\n",
    "    \n",
    "    # Calculate scores and plot contours\n",
    "    if name == 'iForest' and isinstance(tree, int):\n",
    "        Z_grid_scores = if_score_samples_tree(grid_coords, model)[tree].reshape(x_grid.shape)\n",
    "    else:\n",
    "        Z_grid_scores = model.score_samples(grid_coords).reshape(x_grid.shape)\n",
    "    plt.contour(x_grid, y_grid, Z_grid_scores, levels=levels)\n",
    "        \n",
    "    plt.colorbar()\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    return Z_grid_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_grid_lof_scores = plot_outlier_contour(umap_mnist_test, y_test, lof_mnist_test, lof, 'LOF', levels=np.linspace(-10,0,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The darker contour lines represent less likely regions, and brighter contour lines represent more likely regions. The outlier score contour map produced from LOF is smooth with intuitive local maxima, such as in the centers of the clusters. As expected, the farther a sample is from the distribution, the higher the outlier score.\n",
    "\n",
    "Generally, LOF performed well for anomaly detection. The next algorithm uses an ensemble of decision trees to detect anomalies, which has some advantages over this density-based algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"if\"></a>\n",
    "## 4. Isolation Forest (iForest)\n",
    "\n",
    "An isolation forest [(iForest)](https://en.wikipedia.org/wiki/Isolation_forest) is an ensemble tree-based anomaly detection algorithm that identifies outliers as shallow leaves on decision trees. iForest is an ensemble of decision trees, each trained on a random subset of data and a random subset of features. The objective of iForest is to randomly partition the data, and generate various binary search trees. Trees are split until all samples are isolated, or until a max tree depth is reached. Samples that travel more into a tree (i.e. require more partitions) are more normal, where as samples that travel less into a tree (i.e. require less partitions) are more anomalous. With a single tree, we calculate how shallow (i.e. early in tree depth) a sample is predicted to be. With the forest, we calculate the average depth of the sample, and normalize by the average depth of the tree. We then raise that quotient to the power of 2, which defines the anomaly score. Similar to LOF, a threshold is applied to determine which samples are outliers. Inliers have values close to 0, while outliers have values far from 0.\n",
    "\n",
    "iForests are extremely fast, and can efficiently span large training sets and feature spaces. However, a priori knowledge of the percentage of outliers present in the data set is necessary for choosing an optimal threshold. In addition, since iForest is an ensemble of many weak decision trees, the learned decision boundaries tends to be rougher and more rigid. An improved algorithm called [extended isolation forest](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8888179) has been developed with [code](https://github.com/sahandha/eif) from the first author available, but we leave exploring that algorithm as an exercise for the reader.\n",
    " \n",
    "Here are some complementary resources:\n",
    "\n",
    "- [iForest Overview Video (5 minutes, simple language)](https://www.youtube.com/watch?v=Y1x51i1936M)\n",
    "- [Six Sigma Pro Smart Video (11 minutes, simple language)](https://www.youtube.com/watch?v=kN--TRv1UDY)\n",
    "- [PyData iForest Walkthrough Video (24 minutes, intermediate language)](https://www.youtube.com/watch?v=RyFQXQf4w4w)\n",
    "- [iForest Original Paper](https://www.outspokenmarket.com/uploads/8/8/2/3/88233040/isolation_forest.pdf)\n",
    "- [Extended iForest Code from Maintained Repository](https://github.com/h2oai/h2o-3/blob/master/h2o-py/h2o/estimators/extended_isolation_forest.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"if_train\"></a>\n",
    "### 4a. Fit, predict, and visualize using training set\n",
    "\n",
    "We use [scikit-learn for iForest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html). The main parameters for iForest are `n_estimators`, which is the number of trees to fit, `max_samples`, which is the number of samples to train each tree, and `max_features`, which is the number of features to train each tree. We start with all the default values: 100, 256, and every feature (indicated by 1.0), respectively. We also use the default offset (i.e. outlier score threshold) of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_forest = IsolationForest(n_estimators=100, max_samples=256, max_features=1.0, random_state=rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again as a baseline, we fit and predict outliers using the pixels as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_mnist_train = iso_forest.fit_predict(x_train_scale_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iForest is significantly faster at fitting the full set of features in MNIST than LOF. Let's check the shape to make sure we have a vector of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_mnist_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we plot the UMAP training set with the training labels and detected outliers from pixel features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "plt.grid()\n",
    "plt.title(f'UMAP MNIST (Train; iForest (Pixels))\\n{iso_forest.n_estimators} Estimators; {iso_forest.max_samples} Samples')\n",
    "mask = if_mnist_train == -1\n",
    "plt.scatter(\n",
    "    umap_mnist_train[:, 0][~mask], umap_mnist_train[:, 1][~mask], \n",
    "    s=1, alpha=0.5, c=y_train[~mask], cmap='tab10'\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.scatter(\n",
    "    umap_mnist_train[:, 0][mask], umap_mnist_train[:, 1][mask], \n",
    "    s=10, alpha=1, label=f'Anomalies', color='k'\n",
    ")\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using pixels as features, iForest performs poorly on the manifold, labeling most 0s (blue) and 2s (green) as outliers. However, the outliers for 4s (purple), 7s (gray), and 9s (cyan) are mostly on the borders of the clusters. Lastly, only a handful of 1s (orange) were detected as outliers.\n",
    "\n",
    "We plot the 10 most anomalous train samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_negative_outlier_scores = iso_forest.score_samples(x_train_scale_flat)\n",
    "inds = np.argsort(if_negative_outlier_scores)\n",
    "fig, axs = plt.subplots(2,5,figsize=[20,5])\n",
    "for i in range (2):\n",
    "    for j in range (5):\n",
    "        ind = inds[i*5+j]\n",
    "        if_nos = if_negative_outlier_scores[ind]\n",
    "        axs[i, j].set_title(f'Label: {y_train[ind]}\\nNegative Outlier Score: {if_nos:.3f}')\n",
    "        axs[i, j].imshow(x_train_scale[ind])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the pixels as features, the IF detects thick 0s and 8s as outliers, and one 4 where the pixels are a Bernoulli distribution (i.e. only pixel values 0s and 1s).\n",
    "\n",
    "Although iForest detected a particular subset of outliers, we fit and predict anomalies using the UMAP MNIST data for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_forest = IsolationForest(n_estimators=100, max_samples=256, max_features=1.0, random_state=rs)\n",
    "\n",
    "t0_if = time()\n",
    "if_mnist_train = iso_forest.fit_predict(umap_mnist_train)\n",
    "t1_if = time()\n",
    "\n",
    "t_if = t1_if - t0_if\n",
    "print (f'Time spent fitting model: {t_if:.4f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we plot the UMAP training set with the training labels and detected outliers from the UMAP embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "plt.grid()\n",
    "plt.title(f'UMAP MNIST (Train; iForest)\\n{iso_forest.n_estimators} Estimators; {iso_forest.max_samples} Samples')\n",
    "mask = if_mnist_train == -1\n",
    "plt.scatter(\n",
    "    umap_mnist_train[:, 0][~mask], umap_mnist_train[:, 1][~mask], \n",
    "    s=1, alpha=0.5, c=y_train[~mask], cmap='tab10'\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.scatter(\n",
    "    umap_mnist_train[:, 0][mask], umap_mnist_train[:, 1][mask], \n",
    "    s=10, alpha=1, label=f'Anomalies', color='k'\n",
    ")\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using UMAP embedding as features, iForest performs even worse on the manifold. The iForest labeled the 3s (red), 5s (brown) and 8s (yellow) as the central cluster, while most 0s (blue), 1s (orange), and 6s (pink) were labeled as outliers. Similarly, the outliers for 4s (purple), 7s (gray), and 9s (cyan) are mostly on the borders of the clusters. \n",
    "\n",
    "We plot the 10 most anomalous train samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_negative_outlier_scores = iso_forest.score_samples(umap_mnist_train)\n",
    "inds = np.argsort(if_negative_outlier_scores)\n",
    "fig, axs = plt.subplots(2,5,figsize=[20,5])\n",
    "for i in range (2):\n",
    "    for j in range (5):\n",
    "        ind = inds[i*5+j]\n",
    "        if_nos = if_negative_outlier_scores[ind]\n",
    "        axs[i, j].set_title(f'Label: {y_train[ind]}\\nNegative Outlier Score: {if_nos:.3f}')\n",
    "        axs[i, j].imshow(x_train_scale[ind])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the pixels as features, the iForest trivially detects slanted 1s as outliers.\n",
    "\n",
    "The poor fit is primarily caused by the number of samples used to fit each tree. By using 256 random samples to fit 100 random trees, even if each tree used a unique set of samples, we would only use less than half of our data. Here, we fit the iForest using 20000 samples per tree so there is near 100% certainty every sample is used. In addition, we set `contamination=0.05` to adjust the offset such that only 5% of samples are detected as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit using optimal parameters\n",
    "iso_forest = IsolationForest(n_estimators=100, max_samples=20000, max_features=1.0, contamination=0.05, random_state=rs)\n",
    "if_mnist_train = iso_forest.fit_predict(umap_mnist_train)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.grid()\n",
    "plt.title(f'UMAP MNIST (Train; iForest)\\n{iso_forest.n_estimators} Estimators; {iso_forest.max_samples} Samples')\n",
    "mask = if_mnist_train == -1\n",
    "plt.scatter(\n",
    "    umap_mnist_train[:, 0][~mask], umap_mnist_train[:, 1][~mask], \n",
    "    s=1, alpha=0.5, c=y_train[~mask], cmap='tab10'\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.scatter(\n",
    "    umap_mnist_train[:, 0][mask], umap_mnist_train[:, 1][mask], \n",
    "    s=10, alpha=1, label=f'Anomalies', color='k'\n",
    ")\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the outliers are on the borders of each cluster, indicating a good fit. \n",
    "\n",
    "Again, we plot the 10 most anomalous train samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_mnist_train_nos = iso_forest.score_samples(umap_mnist_train)\n",
    "inds = np.argsort(if_mnist_train_nos)\n",
    "fig, axs = plt.subplots(2,5,figsize=[20,5])\n",
    "for i in range (2):\n",
    "    for j in range (5):\n",
    "        ind = inds[i*5+j]\n",
    "        if_nos = if_mnist_train_nos[ind]\n",
    "        axs[i, j].set_title(f'Label: {y_train[ind]}\\nNegative Outlier Score: {if_nos:.3f}')\n",
    "        axs[i, j].imshow(x_train_scale[ind])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several uncharacteristic 2s were detected in addition to some other out of distribution digits. Contrasting with LOF, this subset of outliers is slightly more diverse by class.\n",
    "\n",
    "Finally, we print the number of anomalies detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'Number of Anomalies Detected: {(if_mnist_train == -1).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, 5% of samples were found to be anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"if_test\"></a>\n",
    "### 4b. Predict test set and visualize boundaries\n",
    "\n",
    "Now that the iForest is trained, we predict outliers in the test set and their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_mnist_test = iso_forest.predict(umap_mnist_test)\n",
    "if_mnist_test_nos = iso_forest.score_samples(umap_mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the test set with the test labels and the detected outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "plt.grid()\n",
    "plt.title(f'UMAP MNIST (Test; iForest)\\n{iso_forest.n_estimators} Estimators; {iso_forest.max_samples} Samples')\n",
    "mask = if_mnist_test == -1\n",
    "plt.scatter(\n",
    "    umap_mnist_test[:, 0][~mask], umap_mnist_test[:, 1][~mask], \n",
    "    s=1, alpha=0.5, c=y_test[~mask], cmap='tab10'\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.scatter(\n",
    "    umap_mnist_test[:, 0][mask], umap_mnist_test[:, 1][mask], \n",
    "    s=10, alpha=1, label=f'Anomalies', color='k'\n",
    ")\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iForest generalizes to the test set as well.\n",
    "\n",
    "We also produce a similar plot of the detected outliers with rings proportional to the respective outlier factors (i.e. larger rings are more anomalous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "plt.grid()\n",
    "plt.title(f'UMAP MNIST (Test; iForest)\\n{iso_forest.n_estimators} Estimators; {iso_forest.max_samples} Samples')\n",
    "mask = if_mnist_test == -1\n",
    "plt.scatter(\n",
    "    umap_mnist_test[:, 0][~mask], umap_mnist_test[:, 1][~mask], \n",
    "    s=1, alpha=0.5, c=y_test[~mask], cmap='tab10'\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.scatter(\n",
    "    umap_mnist_test[:, 0][mask], umap_mnist_test[:, 1][mask], \n",
    "    s=1, alpha=1, label=f'Anomalies', color='k'\n",
    ")\n",
    "plt.scatter(\n",
    "    umap_mnist_test[:, 0][mask], umap_mnist_test[:, 1][mask], \n",
    "    s=-if_mnist_test_nos[mask]*100, facecolor='none', edgecolor='k'\n",
    ")\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the outliers have a similar score. There also appears to be a slight bias for anomalies on the borders of the 0s (blue), 1s (orange), and 2s (green) clusters.\n",
    "\n",
    "Similarly, we plot the 10 most anomalous test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_mnist_test_nos = iso_forest.score_samples(umap_mnist_test)\n",
    "inds = np.argsort(if_mnist_test_nos)\n",
    "fig, axs = plt.subplots(2,5,figsize=[20,5])\n",
    "for i in range (2):\n",
    "    for j in range (5):\n",
    "        ind = inds[i*5+j]\n",
    "        if_nos = if_mnist_test_nos[ind]\n",
    "        axs[i, j].set_title(f'Label: {y_test[ind]}\\nNegative Outlier Score: {if_nos:.3f}')\n",
    "        axs[i, j].imshow(x_test_scale[ind])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We detect a few 0s, 1s, and 2s with out of distribution properties.\n",
    "\n",
    "To visualize the outlier boundaries, we predict the outlier scores on a grid of points in our manifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_grid_if = plot_outlier_boundaries(umap_mnist_train, y_train, if_mnist_train, iso_forest, 'iForest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to LOF, the iForest outlier boundaries surround the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"if_adv\"></a>\n",
    "### 4c. iForest Distributions and Trees\n",
    "\n",
    "Similar to LOF, iForest assigns an outlier score to each sample, and detects an anomaly as having a score greater than the offset. We plot the distributions of the outlier scores for the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid()\n",
    "plt.title('iforest Outlier Score Histogram')\n",
    "plt.hist(if_mnist_train_nos, bins=200, alpha=0.5, label='train')\n",
    "plt.hist(if_mnist_test_nos, bins=200, alpha=0.5, label='test')\n",
    "plt.vlines(iso_forest.offset_, 0, 10**3, color='k', label='offset')\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outlier scores are skewed towards the left with rarer samples having extreme negative scores.\n",
    "\n",
    "In addition, we can calculate the outlier score of the manifold using the iForest. Samples that are more likely to be observed have an outlier score closer to 0. By visualizing the outlier score distribution on the manifold, we can determine where scores are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_grid_if_scores = plot_outlier_contour(umap_mnist_test, y_test, if_mnist_test, iso_forest, 'iForest', levels=np.linspace(-0.7,-0.4,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The darker contour lines represent less likely regions, and brighter contour lines represent more likely regions. The outlier score contour map produced from the iForest is much rougher than LOF. However, they both share similar properties (e.g. local maxima in the clusters' centers, higher outlier scores for more isolated samples, etc.).\n",
    "\n",
    "Since the iForest is an ensemble of decision trees, we can analyze each tree to gain further insight on the trained model. The attribute `estimators_features_` returns the features that were used to train each tree. Since `max_features` was set to use all available features by default, each tree used both UMAP embeddings. If the following execution is `True`, then all trees use both UMAP dimensions as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.array(iso_forest.estimators_features_) == np.arange(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute `estimators_samples_` returns the unique set of sample indices that were used to train each tree. If the following execution is `True`, then all samples were used at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(iso_forest.estimators_samples_).shape[0] == x_train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can mean stack each set of samples used by the tree to determine if each set has similar image properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(10,10,figsize=[20,20])\n",
    "for i in range (10):\n",
    "    for j in range (10):\n",
    "        ind = i*10+j\n",
    "        if_ind = iso_forest.estimators_samples_[ind]\n",
    "        mean_tree = x_train_scale[if_ind].mean(0)\n",
    "        axs[i,j].set_title(ind)\n",
    "        axs[i,j].imshow(mean_tree)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All mean stacks look nearly identical, indicating each set of samples has similar image properties.\n",
    "\n",
    "We can also access each tree individually using the attribute `estimators_` to further understand how they detect outliers. First, we investigate each trees' feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_feat_imp = np.array([i.feature_importances_ for i in iso_forest.estimators_])\n",
    "\n",
    "plt.figure(figsize=[5,5])\n",
    "plt.grid()\n",
    "plt.title(f'iForest Feature Importance (UMAP1)')\n",
    "plt.plot(if_feat_imp[:, 0])\n",
    "plt.xlabel('Tree Number')\n",
    "plt.ylabel('Feature Importance')\n",
    "print (f'Feature importances of UMAP1 and UMAP2: {if_feat_imp.mean(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importances of UMAP1 and UMAP2 are essentially equal. However, they do oscillate about 0.5, meaning the trees are capturing different qualities of MNIST.\n",
    "\n",
    "We can also compute the negative outlier score from each tree for a sample. This metric gives us more insight on how each tree contributes to the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def if_score_samples_tree(X, iso_forest, subsample_features=False):\n",
    "    \"\"\"Compute the negative outlier score from each tree.\n",
    "\n",
    "    This code was modified from the following function:\n",
    "    https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_iforest.py#L574\n",
    "\n",
    "    which computes the negative outlier score for the entire isolation forest.\n",
    "    Instead of using the summed depths of the forest scaled by the number of trees in the forest\n",
    "    to calculate the score, we calculate each depth individually and scale by unity (i.e. one tree).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        Features as a 2D array.\n",
    "    iso_forest : sklearn.model\n",
    "        Isolation forest model.\n",
    "    subsample_features : bool, default=False\n",
    "        If True, use a subsample of the features.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    scores : np.array\n",
    "        Negative outlier scores as a 2D array (trees, samples).\n",
    "    \"\"\"\n",
    "    # Prepare variables before loop\n",
    "    n_samples = X.shape[0]\n",
    "    depths = np.zeros((iso_forest.n_estimators, n_samples))\n",
    "    average_path_length_max_samples = _average_path_length([iso_forest._max_samples])\n",
    "    \n",
    "    # Find the depth of each sample for each tree\n",
    "    for tree_idx, (tree, features) in enumerate(\n",
    "        zip(iso_forest.estimators_, iso_forest.estimators_features_)\n",
    "    ):\n",
    "        # Use subset of feature if necessary\n",
    "        X_subset = X[:, features] if subsample_features else X\n",
    "        # Find the index of the leaf each sample is predicted as\n",
    "        leaves_index = tree.apply(X_subset, check_input=False)\n",
    "        # Calculate the depth\n",
    "        depths[tree_idx] = (\n",
    "            iso_forest._decision_path_lengths[tree_idx][leaves_index]\n",
    "            + iso_forest._average_path_length_per_tree[tree_idx][leaves_index]\n",
    "            - 1.0\n",
    "        )\n",
    "    # Scale the depth by the average path length of the maximum samples\n",
    "    denominator = average_path_length_max_samples\n",
    "    # Calculate the negative outlier scores\n",
    "    scores = -2 ** (\n",
    "        -np.divide(\n",
    "            depths, denominator, out=np.ones_like(depths), where=denominator != 0\n",
    "        )\n",
    "    )\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_mnist_test_nos_tree = if_score_samples_tree(umap_mnist_test, iso_forest)\n",
    "if_mnist_test_tree = (if_mnist_test_nos_tree > iso_forest.offset_).astype(int)\n",
    "if_mnist_test_tree[if_mnist_test_tree==0] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the scores of each tree, we plot some histograms to understand their distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ensemble metrics\n",
    "if_mnist_test_nos_mean = if_mnist_test_nos_tree.mean(0)\n",
    "if_mnist_test_nos_per = (if_mnist_test_nos_tree>iso_forest.offset_).sum(0)\n",
    "\n",
    "# Plot\n",
    "fig, axs = plt.subplots(1,2,figsize=[10,5])\n",
    "axs[0].grid()\n",
    "axs[0].set_title('Mean Negative Outlier Score')\n",
    "axs[0].hist(if_mnist_test_nos_mean[~mask].flatten(), alpha=0.5, bins=100, label='Nominal')\n",
    "axs[0].hist(if_mnist_test_nos_mean[mask].flatten(), alpha=0.5, bins=100, label='Anomaly', color='k')\n",
    "axs[0].vlines(iso_forest.offset_, 0, 160, color='k', label=f'offset={iso_forest.offset_:.3f}')\n",
    "axs[0].set_xlabel('Mean NOS')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].legend()\n",
    "axs[1].grid()\n",
    "axs[1].set_title('Percentage of NOS Greater than Offset')\n",
    "axs[1].hist(if_mnist_test_nos_per[~mask].flatten(), alpha=0.5, bins=100, label='Nominal', range=[0,100])\n",
    "axs[1].hist(if_mnist_test_nos_per[mask].flatten(), alpha=0.5, bins=100, label='Anomaly', color='k', range=[0,100])\n",
    "axs[1].vlines(50, 0, 300, color='k', label='50%')\n",
    "axs[1].set_xlabel('% NOS > Offset')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left histogram illustrates for each sample in the MNIST test set the mean score from all the trees, separated by anomaly class. The right histogram illustrates for each sample in the MNIST test set the percentage of scores greater than the offset. The nominal distribution is separate from the anomalous distribution in each histogram with minimal overlap near the offset and 50%. The overlap shows that some nominal samples near the threshold may have exceptionally larger scores in a few trees, which is enough to make them nominal even if less than 50% of the trees predict the sample to be nominal.\n",
    "\n",
    "Similarly to the whole iForest, we can plot contour maps for individual trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_grid_tree_scores = plot_outlier_contour(umap_mnist_test, y_test, if_mnist_test_tree, iso_forest, 'iForest', levels=np.linspace(-0.7,-0.4,5), tree=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single tree's decision boundaries are very rigid, and perform weakly against MNIST. This plot demonstrates how powerful ensemble methods are as the complement for the iForest was smoother, and performed well.\n",
    "\n",
    "Lastly, we can plot the decision tree itself. However, we restrict the max depth to 3 for faster rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20,10])\n",
    "plot_tree(iso_forest.estimators_[0], max_depth=3, feature_names=['UMAP1', 'UMAP2'], proportion=True, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each node contains four quantities:\n",
    "- The decision condition for splitting\n",
    "- The squared error computed of the training set from each split\n",
    "- The percentage of samples of the training set from each split\n",
    "- A spurious \"mean value\" of the training set from each split\n",
    "\n",
    "We can gain even deeper insight on the iForest by analyzing the decision conditions for each node, and why those set of decision conditions may be useful for outlier detection.\n",
    "\n",
    "Generally, iForest also performed well for anomaly detection with some various pros and cons in comparison to LOF. The last algorithm uses a kernel to detect anomalies, which also has prefered use cases over the previous two algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"svm\"></a>\n",
    "## 5. One Class Support Vector Machine (OC-SVM)\n",
    "\n",
    "A one-class support vector machine [(OC-SVM; detailed explanation with equations can be found under \"Introduction\")](https://en.wikipedia.org/wiki/One-class_classification) is a kernel-based anomaly detection algorithm that identifies outliers as samples outside of a decision boundary. A vanilla support vector machine (SVM) is a supervised learning technique that predicts a regression or classification value. The objective of SVM is to find a decision boundary (i.e. margin) that separates dissimilar predictions the most with respect to support vectors (i.e. training samples near the margin). SVMs use kernels, which are functions used to transform low dimensional data into a higher dimension. In the higher dimension, a margin is found to separate the data. Then, the data and learned margin are inverse transformed back to the lower dimension. In the case of OC-SVMs, the task is \"binary classification\" with the training data being one class, and the origin being a pseudo-class. In addition, the parameter nu is introduced, which determines the probability of a sample being outside the margin. This parameter is similar to iForest contamination parameter, and controls the proportion of outliers allowed in the training set. By minimizing the enclosing space of the inliers and maximizing nu proportion of outliers, we learn a margin that optimally separates the two classes. \n",
    "\n",
    "Some advantages of OC-SVM are few hyperparameters and a smooth margin over the learned manifold. OC-SVM is also useful when only normal samples are in the training set. However, it is relatively slow to train/predict compared to other algorithms, and weakly scales with sample size. A [linear approximation using stochastic gradient descent (SGD)](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDOneClassSVM.html#sklearn.linear_model.SGDOneClassSVM) exists, but we leave exploring that option as an exercise for the reader.\n",
    " \n",
    "Here are some complementary resources:\n",
    "\n",
    "- [SVM Overview Video (2 minutes, simple language)](https://www.youtube.com/watch?v=_YPScrckx28)\n",
    "- [StatQuest SVM Main Ideas Video (21 minutes, simple language)](https://www.youtube.com/watch?v=efR1C6CvhmE)\n",
    "- [OC-SVM Overview Video (6 minutes, simple language)](https://www.youtube.com/watch?v=5YlhdWzltM4)\n",
    "- [OC-SVM Detailed Video (12 minutes, advanced language)](https://www.youtube.com/watch?v=7HroYexkvXs)\n",
    "- [OC-SVM Paper 1](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-99-87.pdf)\n",
    "- [OC-SVM Paper 2](https://proceedings.neurips.cc/paper_files/paper/1999/file/8725fb777f25776ffa9076e44fcfd776-Paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"svm_train\"></a>\n",
    "### 5a. Fit, transform, and visualize using training set\n",
    "\n",
    "We use [scikit-learn for OC-SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html). The main parameters for OC-SVM are `kernel`, which is the kernel used for the SVM, `gamma`, which is the scale for the kernel, and `nu`, which is the portion of outliers allowed. We use the default parameters `kernel='rbf'`, which is the [radial basis function kernel](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html) (or the squared exponential kernel), `gamma='scale'`, which scales the kernel by the inverse of the product of the number of features and the data's variance, and `nu=0.5`, which fits 50% of samples as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_svm = OneClassSVM(kernel='rbf', gamma='scale', nu=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the pixel dimensions are too high to sufficiently fit (i.e. would take around half an hour), we directly fit and detect outliers using the UMAP MNIST data.\n",
    "\n",
    "**Note: this may take a few minutes to execute.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0_oc_svm = time()\n",
    "oc_svm_mnist_train = oc_svm.fit_predict(umap_mnist_train)\n",
    "t1_oc_svm = time()\n",
    "\n",
    "t_oc_svm = t1_oc_svm - t0_oc_svm\n",
    "print (f'Time spent fitting model: {t_oc_svm:.4f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this 2D data, fitting OC-SVM is significantly slower than the other models. Let's check the shape to make sure we have a vector of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oc_svm_mnist_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we plot the UMAP training set with the training labels and detected outliers from the UMAP embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "plt.grid()\n",
    "plt.title(f'UMAP MNIST (Train; OC-SVM)\\nKernel={oc_svm.kernel}; Gamma={oc_svm.gamma}; Nu={oc_svm.nu}')\n",
    "mask = oc_svm_mnist_train == -1\n",
    "plt.scatter(\n",
    "    umap_mnist_train[:, 0][~mask], umap_mnist_train[:, 1][~mask], \n",
    "    s=1, alpha=0.5, c=y_train[~mask], cmap='tab10'\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.scatter(\n",
    "    umap_mnist_train[:, 0][mask], umap_mnist_train[:, 1][mask], \n",
    "    s=10, alpha=1, label=f'Anomalies', color='k'\n",
    ")\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OC-SVM overfits the data, detecting a majority of clusters as outliers besides 3 (green), 5 (brown), 8 (yellow), and 9 (cyan). \n",
    "\n",
    "We plot the 10 most anomalous train samples. **Note: this may also take a few minutes to execute.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_svm_mnist_train_os = oc_svm.score_samples(umap_mnist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.argsort(oc_svm_mnist_train_os)\n",
    "fig, axs = plt.subplots(2,5,figsize=[20,5])\n",
    "for i in range (2):\n",
    "    for j in range (5):\n",
    "        ind = inds[i*5+j]\n",
    "        oc_svm_os = oc_svm_mnist_train_os[ind]\n",
    "        axs[i, j].set_title(f'Label: {y_train[ind]}\\nOutlier Score: {oc_svm_os:.3f}')\n",
    "        axs[i, j].imshow(x_train_scale[ind])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly 0s were detected along with one 5. \n",
    "\n",
    "Now, we fit using optimal parameters found a priori. We use `gamma='auto'`, which scales the kernel by the inverse of the number of features, and `nu=0.05`, which significantly decreases the portion of outliers to 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_svm = OneClassSVM(kernel='rbf', gamma='auto', nu=0.05)\n",
    "\n",
    "t0_oc_svm = time()\n",
    "oc_svm_mnist_train = oc_svm.fit_predict(umap_mnist_train)\n",
    "t1_oc_svm = time()\n",
    "\n",
    "t_oc_svm = t1_oc_svm - t0_oc_svm\n",
    "print (f'Time spent fitting model: {t_oc_svm:.4f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although these parameters fit ten times faster than the default parameters and fits within one minute, OC-SVM is still significantly slower than the other models.\n",
    "\n",
    "We plot the UMAP training set with the training labels and detected outliers from the UMAP embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "plt.grid()\n",
    "plt.title(f'UMAP MNIST (Train; OC-SVM)\\nKernel={oc_svm.kernel}; Gamma={oc_svm.gamma}; Nu={oc_svm.nu}')\n",
    "mask = oc_svm_mnist_train == -1\n",
    "plt.scatter(\n",
    "    umap_mnist_train[:, 0][~mask], umap_mnist_train[:, 1][~mask], \n",
    "    s=1, alpha=0.5, c=y_train[~mask], cmap='tab10'\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.scatter(\n",
    "    umap_mnist_train[:, 0][mask], umap_mnist_train[:, 1][mask], \n",
    "    s=10, alpha=1, label=f'Anomalies', color='k'\n",
    ")\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OC-SVM performs well with most outliers on the clusters' borders. However, some outliers were found within the clusters, such as for 4 (purple), 5 (brown), and 7 (gray).\n",
    "\n",
    "Again, we plot the 10 most anomalous train samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_svm_mnist_train_os = oc_svm.score_samples(umap_mnist_train)\n",
    "inds = np.argsort(oc_svm_mnist_train_os)\n",
    "fig, axs = plt.subplots(2,5,figsize=[20,5])\n",
    "for i in range (2):\n",
    "    for j in range (5):\n",
    "        ind = inds[i*5+j]\n",
    "        oc_svm_os = oc_svm_mnist_train_os[ind]\n",
    "        axs[i, j].set_title(f'Label: {y_train[ind]}\\nOutlier Score: {oc_svm_os:.3f}')\n",
    "        axs[i, j].imshow(x_train_scale[ind])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to LOF, several uncharacteristic 7s were detected along with a 0.\n",
    "\n",
    "Finally, we print the number of anomalies detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'Number of Anomalies Detected: {(oc_svm_mnist_train == -1).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, around 5% of samples were outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"svm_test\"></a>\n",
    "### 5b. Predict test set and visualize boundaries\n",
    "\n",
    "Now that the OC-SVM is trained, we predict outliers in the test set and their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_svm_mnist_test = oc_svm.predict(umap_mnist_test)\n",
    "oc_svm_mnist_test_os = oc_svm.score_samples(umap_mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the test set with the test labels and the detected outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "plt.grid()\n",
    "plt.title(f'UMAP MNIST (Test; OC-SVM)\\nKernel={oc_svm.kernel}; Gamma={oc_svm.gamma}; Nu={oc_svm.nu}')\n",
    "mask = oc_svm_mnist_test == -1\n",
    "plt.scatter(\n",
    "    umap_mnist_test[:, 0][~mask], umap_mnist_test[:, 1][~mask], \n",
    "    s=1, alpha=0.5, c=y_test[~mask], cmap='tab10'\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.scatter(\n",
    "    umap_mnist_test[:, 0][mask], umap_mnist_test[:, 1][mask], \n",
    "    s=10, alpha=1, label=f'Anomalies', color='k'\n",
    ")\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OC-SVM generalizes to the test set as well.\n",
    "\n",
    "We also produce a similar plot with the detected outliers with rings *inversely* proportional to the respective outlier factors (i.e. smaller rings are more anomalous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "plt.grid()\n",
    "plt.title(f'UMAP MNIST (Test; OC-SVM)\\nKernel={oc_svm.kernel}; Gamma={oc_svm.gamma}; Nu={oc_svm.nu}')\n",
    "mask = oc_svm_mnist_test == -1\n",
    "plt.scatter(\n",
    "    umap_mnist_test[:, 0][~mask], umap_mnist_test[:, 1][~mask], \n",
    "    s=1, alpha=0.5, c=y_test[~mask], cmap='tab10'\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.scatter(\n",
    "    umap_mnist_test[:, 0][mask], umap_mnist_test[:, 1][mask], \n",
    "    s=1, alpha=1, label=f'Anomalies', color='k'\n",
    ")\n",
    "plt.scatter(\n",
    "    umap_mnist_test[:, 0][mask], umap_mnist_test[:, 1][mask], \n",
    "    s=oc_svm_mnist_test_os[mask], facecolor='none', edgecolor='k'\n",
    ")\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the outliers have a similar score.\n",
    "\n",
    "Similarly, we plot the 10 most anomalous test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_svm_mnist_test_os = oc_svm.score_samples(umap_mnist_test)\n",
    "inds = np.argsort(oc_svm_mnist_test_os)\n",
    "fig, axs = plt.subplots(2,5,figsize=[20,5])\n",
    "for i in range (2):\n",
    "    for j in range (5):\n",
    "        ind = inds[i*5+j]\n",
    "        oc_svm_os = oc_svm_mnist_test_os[ind]\n",
    "        axs[i, j].set_title(f'Label: {y_test[ind]}\\nOutlier Score: {oc_svm_os:.3f}')\n",
    "        axs[i, j].imshow(x_test_scale[ind])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We detect a handful of digits that are out of distribution.\n",
    "\n",
    "To visualize the outlier boundaries, we predict the outlier scores on a grid of points in our manifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_grid_oc_svm = plot_outlier_boundaries(umap_mnist_train, y_train, oc_svm_mnist_train, oc_svm, 'OC-SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OC-SVM outlier boundaries are similar to LOF and iForest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"svm_adv\"></a>\n",
    "### 5c. OC-SVM Distributions\n",
    "\n",
    "Similar to LOF and iForest, OC-SVM assigns an outlier score to each sample, but detects an anomaly as having a score *less* than the offset. We plot the distributions of the outlier scores for the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid()\n",
    "plt.title('OC-SVM Outlier Score Histogram')\n",
    "plt.hist(oc_svm_mnist_train_os, bins=200, alpha=0.5, label='train')\n",
    "plt.hist(oc_svm_mnist_test_os, bins=200, alpha=0.5, label='test')\n",
    "plt.vlines(oc_svm.offset_, 0, 10**4, color='k', label='offset')\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outlier scores are skewed towards the left with rarer samples having low scores.\n",
    "\n",
    "Similarly, we can calculate the outlier score of the manifold using the OC-SVM. Samples that are *less* likely to be observed have an outlier score closer to 0. By visualizing the outlier score distribution on the manifold, we can determine where scores are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_grid_oc_svm_scores = plot_outlier_contour(umap_mnist_test, y_test, oc_svm_mnist_test, oc_svm, 'OC-SVM', levels=np.linspace(30, 100, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The darker contour lines represent less likely regions, and brighter contour lines represent more likely regions. The outlier score contour map produced from the OC-SVM is also similar to LOF with slightly smoother regions.\n",
    "\n",
    "Generally, OC-SVM also performed well for anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"con\"></a>\n",
    "## 6. Conclusions\n",
    "\n",
    "Anomaly detection is a cruicial task for exploring data and understanding its complexity. Local Outlier Factor (LOF) is a density-based algorithm that scores low density regions as anomalous. Isolation Forest (iForest) is an ensemble tree-based algorithm that trains multiple decisions trees simultaneously and scores short tree paths (i.e. isolated samples) as anomalous. One Class Support Vector Machine (OC-SVM) is a kernel-based algorithm that maximizes the inlier/outlier margin, and scores rare and deviant samples as anomalous. There are many anomaly detection algorithms, but using these three in unison is a substantial start to any machine learning based exploratory data analysis.\n",
    "\n",
    "**Thank you and congratulations for completing the notebook!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time spent for each model\n",
    "print (f'Time spent fitting LOF: {t_lof:.4f} seconds')\n",
    "print (f'Time spent fitting iForest: {t_if:.4f} seconds')\n",
    "print (f'Time spent fitting OC-SVM: {t_oc_svm:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Boundaries\n",
    "fig, axs = plt.subplots(1,3,figsize=[15,5])\n",
    "axs[0].set_title('LOF Outlier Boundary')\n",
    "axs[0].imshow(Z_grid_lof, origin='lower')\n",
    "axs[1].set_title('iForest Outlier Boundary')\n",
    "axs[1].imshow(Z_grid_if, origin='lower')\n",
    "axs[2].set_title('OC-SVM Outlier Boundary')\n",
    "axs[2].imshow(Z_grid_oc_svm, origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Contours\n",
    "fig, axs = plt.subplots(1,3,figsize=[15,5])\n",
    "axs[0].set_title('LOF Outlier Score Contour')\n",
    "axs[0].imshow(Z_grid_lof_scores, origin='lower', vmin=-4, vmax=-1)\n",
    "axs[1].set_title('iForest Outlier Score Contour')\n",
    "axs[1].imshow(Z_grid_if_scores, origin='lower', vmin=-0.7, vmax=-0.5)\n",
    "axs[2].set_title('OC-SVM Outlier Score Contour')\n",
    "axs[2].imshow(Z_grid_oc_svm_scores, origin='lower', vmin=50, vmax=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Scores\n",
    "fig, axs = plt.subplots(1,3,figsize=[15,5])\n",
    "axs[0].grid()\n",
    "axs[0].set_title('LOF Outlier Score (Colored)')\n",
    "axs0 = axs[0].scatter(umap_mnist_train[:, 0], umap_mnist_train[:, 1], s=1, alpha=1, c=lof_mnist_train_nof)\n",
    "cbar0 = fig.colorbar(axs0, ax=axs[0])\n",
    "axs[1].grid()\n",
    "axs[1].set_title('iForest Outlier Score (Colored)')\n",
    "axs1 = axs[1].scatter(umap_mnist_train[:, 0], umap_mnist_train[:, 1], s=1, alpha=1, c=if_mnist_train_nos)\n",
    "cbar1 = fig.colorbar(axs1, ax=axs[1])\n",
    "axs[2].grid()\n",
    "axs[2].set_title('OC-SVM Outlier Score (Colored)')\n",
    "axs2 = axs[2].scatter(umap_mnist_train[:, 0], umap_mnist_train[:, 1], s=1, alpha=1, c=oc_svm_mnist_train_os)\n",
    "cbar2 = fig.colorbar(axs2, ax=axs[2])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"add\"></a>\n",
    "## Additional Resources\n",
    "\n",
    "Machine learning is a dense and rapidly evolving field of study. Becoming an expert takes years of practice and patience, but hopefully this notebook brought you closer in that direction. Here are some of the author's favorite resources for learning about machine learning and data science:\n",
    "\n",
    "- [Google Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course)\n",
    "- [scikit-learn Python Library](https://scikit-learn.org/stable/index.html) (go-to for most ML algorithms besides neural networks)\n",
    "  - [Novelty and Outlier Detection](https://scikit-learn.org/stable/modules/outlier_detection.html)\n",
    "- [StatQuest YouTube Channel](https://www.youtube.com/c/joshstarmer)\n",
    "- [DeepLearningAI YouTube Channel](https://www.youtube.com/c/Deeplearningai/videos)\n",
    "- [Towards Data Science](https://towardsdatascience.com/) (articles about data science and machine learning, some involving example blocks of code)\n",
    "- Advance searching [arxiv](https://arxiv.org/search/advanced) (e.g. search term \"machine learning\" in Abstract for Subject astro-ph) to see what others are doing currently\n",
    "- Google, YouTube, Wikipedia and ChatGPT (confirm results with external sources) in general\n",
    "  - [SVM Kernel Trick Video (3 minutes, simple language)](https://www.youtube.com/watch?v=Q7vT0--5VII)\n",
    "- [PyOD: Python library for outlier detection](https://pyod.readthedocs.io/en/latest/)\n",
    "\n",
    "<a id=\"about\"></a>\n",
    "## About this Notebook\n",
    "\n",
    "**Author:** Fred Dauphin, DeepWFC3\n",
    "\n",
    "**Updated on:** 2025-01-16\n",
    "\n",
    "<a id=\"cite\"></a>\n",
    "## Citations\n",
    "\n",
    "If you use `numpy`, `matplotlib`, `sklearn`, or `umap` for published research, please cite the authors. Follow these links for more information about citing `numpy`, `matplotlib`, `sklearn`, and `umap`:\n",
    "\n",
    "* [Citing `numpy`](https://numpy.org/doc/stable/license.html)\n",
    "* [Citing `matplotlib`](https://matplotlib.org/stable/users/project/license.html#:~:text=Matplotlib%20only%20uses%20BSD%20compatible,are%20acceptable%20in%20matplotlib%20toolkits.)\n",
    "* [Citing `sklearn`](https://scikit-learn.org/stable/about.html#citing-scikit-learn)\n",
    "* [Citing `umap`](https://github.com/lmcinnes/umap/blob/master/LICENSE.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "[Top of Page](#title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcR3do4SAr2_",
    "outputId": "c25bbf70-4814-463f-c934-480be9928728"
   },
   "source": [
    "<a id=\"title\"></a>\n",
    "# Transfer Learning MNIST Classification Tutorial using PyTorch\n",
    "***\n",
    "## Learning Goals:\n",
    "By the end of this tutorial, you will:\n",
    "- load and transform the MNIST dataset\n",
    "- load and modify a pretrained model\n",
    "- train and evaluate a pretrained model\n",
    "\n",
    "## Table of Contents\n",
    "[Introduction](#intro) <br>\n",
    "[0. Imports](#imports) <br>\n",
    "[1. MNIST Dataset and Scaling](#mnist) <br>\n",
    "[2. Load and Modify a Pretrained Model](#load) <br>\n",
    "[3. Test Model Functionality](#test) <br>\n",
    "[4. Set Training and Test Sets](#set) <br>\n",
    "[5. Hyperparameters and Loading](#hyper) <br>\n",
    "[6. Train Model](#train) <br>\n",
    "[7. Plot Metrics](#plot) <br>\n",
    "[8. Analyze Samples](#analyze) <br>\n",
    "[9. Conclusions](#con) <br>\n",
    "[Additional Resources](#add) <br>\n",
    "[About this Notebook](#about) <br>\n",
    "[Citations](#cite) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a id=\"intro\"></a>\n",
    "\n",
    "The main purpose of this notebook is to demonstrate transfer learning in [PyTorch](https://pytorch.org/), a deep learning Python library. This tutorial is not an exhaustive introduction to machine learning and assumes the user is familiar with vocabulary (supervised v unsupervised, neural networks, loss functions, backpropogation, etc) and methodology (model selection, feature selection, hyperparameter tuning, etc). This notebook also assumes the user is familiar with convolutional neural networks (CNNs) and the [MNIST handwritten dataset](http://yann.lecun.com/exdb/mnist/). Look at [Additional Resources](#add) for more complete machine learning guides. The paragraphs below serve as a brief introduction to transfer learning.\n",
    "\n",
    "[Transfer learning](https://en.wikipedia.org/wiki/Transfer_learning) is a method of machine learning that uses a well trained model to solve a similar problem instead of the problem it was orginally designed for. Training and tuning a deep model takes a lot of time, data, and computation, so it's not in the best interest for all models to be trained from scratch when some really high performing models already exist. \n",
    "\n",
    "We can use a pretrained model as a \"feature extractor\" by freezing most of the layers in the model. Then, we can retrain the unforzen layers after changing the number of output neurons specific to our problem (e.g. changing the number of final classifications from 1000 to 10) and/or adding more hidden layers to the end of the pretrained model (e.g. changing the number of hidden fully connected layers from 1 to 3). At the end, we'll have a model that was initially designed for one task customized for our task. \n",
    "\n",
    "For example, let's take a CNN trained to classify house cats and house dogs. The model will learn features that are useful for distinguishing between the two animals, such as ears, noses, etc. That model can transfer it's knowledge to classify lynxes and wolves, animals that are similar to cats and dogs. The model can use it previous knowledge of ears, noses, etc. to fine tune on and immediately produce good results. In addition, you don't have to go through the trouble of training an entire model from scratch, which would dramatically increase training time and computation for a possible drop in performance.\n",
    "\n",
    "To extrapolate from that example, extremely deep CNNs trained on an extensive diverse dataset will learn features general enough for a large number of computer vision problems, such as multiclass classification and object detection.\n",
    "\n",
    "The two main uses for transfer learning are when you have data limits or computation limits, which are explained below:\n",
    "- Data Limitations: most datasets to solve real world problems are actually pretty small, making them proned to overfitting if being trained on too deep of a network. The most adventageous solution is to obtain more trainning data, but that could be heavily taxing and time consuming. However, utilizing a model previously trained on millions of examples could be a nice starting point since it already has well defined features useful for solving most problems. \n",
    "\n",
    "- Computation Limitations: training a deep network from scratch can take anywhere between hours to weeks depending on the scope of the problem at hand. Not everyone has the time or the resources to train deep models. However, since a pretrained model acts as a feature extractor, backpropagation is only performed on the last few layers, which is computationally inexpensive and faster than training from scratch.\n",
    "\n",
    "**In this notebook, we will perform transfer learning on a pretrained network to classify MNIST handwritten digits using PyTorch.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports <a id=\"imports\"></a>\n",
    "\n",
    "If you are running this notebook on Google Colab, you shouldn't have to install anything. If you are running this notebook in Jupyter, this notebook assumes you created the virtual environment defined in `environment.yml`. If not, close this notebook and run the following lines in a terminal window:\n",
    "\n",
    "`conda env create -f environment.yml`\n",
    "\n",
    "`conda activate deepwfc3_env`\n",
    "\n",
    "We import the following libraries:\n",
    "- *numpy* for handling arrays\n",
    "- *matplotlib* for plotting\n",
    "- *tqdm* for keeping track of loop speed\n",
    "- *tensorflow* for accessing MNIST images \n",
    "- *torch* as our machine learning framework\n",
    "- *torchvision* for loading models and data transforms\n",
    "- *sklearn.metrics* for model evaluation\n",
    "- *seaborn* for plotting confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSb8DxN-5ioF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn import metrics\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gQZlZgT5ioG"
   },
   "source": [
    "## 1. MNIST Dataset and Scaling<a id=\"mnist\"></a>\n",
    "\n",
    "The MNIST dataset is nicely packed in `tensorflow` as `np.arrays`, which is why we are grabbing our data from there instead of directly from `torch`. The data is unpacked as `x_train` for training features, `y_train` for training labels, `x_test` for testing features, and `y_test` for testing labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89l2--hv5ioG"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform our dataset into the desired input space. The pretrained model was trained on 3 channeled (RGB) 224x224 images normalized to the means and standard deviations listed in `preprocess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our images are 8-bit gray scaled, we will append two copies of each sample as \"additional channels\" in `process` and perform the transformations on the \"RGB\" image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(image):\n",
    "    image = image.reshape(1,image.shape[0],image.shape[1])\n",
    "    image_3 = np.concatenate((image, image, image))\n",
    "    image_rgb = np.transpose(image_3, axes=(1,2,0))\n",
    "    image_rgb_tensor = preprocess(image_rgb)\n",
    "    return image_rgb_tensor.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time this notebook was written, `torchvision.transform` does not support transforming batches of several images so we must manually loop through our data and transform them. **Note: we will be training on the first 1000 training images to showcase transfer learning on a small dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_1000_process = []\n",
    "for image in x_train[:1000]:\n",
    "    x_train_1000_process.append(process(image))\n",
    "x_train_1000_process = np.array(x_train_1000_process)\n",
    "\n",
    "y_train_1000 = y_train[:1000]\n",
    "\n",
    "x_test_process = []\n",
    "for image in x_test:\n",
    "    x_test_process.append(process(image))\n",
    "x_test_process = np.array(x_test_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shapes of `x_train_1000_process` and `x_test_process` to make sure they are what we expect (number of images, 3, 224, 224)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Train data shape is {}'.format(x_train_1000_process.shape))\n",
    "print ('Test data shape is {}'.format(x_test_process.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a final check, let's look at the separate channels of an image to confirm they are the same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "fig, axs = plt.subplots(1,3,figsize=[15,5])\n",
    "axs[0].set_title('Training Image {} with label {} (R channel)'.format(index, y_train_1000[index]))\n",
    "axs[0].imshow(x_train_1000_process[index,0])\n",
    "axs[1].set_title('Training Image {} with label {} (G channel)'.format(index, y_train_1000[index]))\n",
    "axs[1].imshow(x_train_1000_process[index,1])\n",
    "axs[2].set_title('Training Image {} with label {} (B channel)'.format(index, y_train_1000[index]))\n",
    "axs[2].imshow(x_train_1000_process[index,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is processed, we can move into loading a pretrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhl2MMkJ5ioH"
   },
   "source": [
    "## 2. Load and Modify a Pretrained Model <a id=\"load\"></a>\n",
    "\n",
    "PyTorch has dozens of pretrained models in `torchvision` under [torchvision.models](https://pytorch.org/vision/stable/models.html). Some famous models include [VGG](https://pytorch.org/hub/pytorch_vision_vgg/), [ResNet](https://pytorch.org/hub/pytorch_vision_resnet/), [Inception v3](https://pytorch.org/hub/pytorch_vision_inception_v3/), and [GoogLeNet](https://pytorch.org/hub/pytorch_vision_googlenet/). The published papers for these models are in [Additional Resources](#add). These models were trained using [the ImageNet dataset](https://image-net.org/index.php), which contains over 1M images with 1000 unique classifications. \n",
    "\n",
    "In this tutorial, we will use GoogLeNet because it is a relatively small network (7M trainable parameters) with a high accuracy (89%) for ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.googlenet(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also switch to evaluation mode to ensure we are not training the nextwork. The architecture will be printed, which starts with a convolutional layer, goes through several [inception modules](https://paperswithcode.com/method/inception-module#:~:text=An%20Inception%20Module%20is%20an,pass%20onto%20the%20next%20layer.), and ends with a fully connected layer. Notice in the final layer (fc) that `out_features=1000`, which is the number of classifications for ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a default, all of the parameters in the model have the attribute `requires_grad` set to `True`, which determines if the gradients should be calculated for that layer. Since we only want to train the last layer, we first set all the `requires_grad` to `False`. Later when we modify our model, we will reactivate the last layer to require gradient calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will change the final layer to have 10 output classifications instead of 1000. Commented is an example of adding another layer going from 1024 neurons to 128 and 128 to 10 with ReLU activation and dropout. Uncommenting the additional lines will train a deeper model, but training will take longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_input_features, 10)\n",
    "\n",
    "# adding another layer\n",
    "#dropout = 0.2\n",
    "#intermediate = 128\n",
    "#model.fc = nn.Sequential(\n",
    " #   nn.Linear(num_input_features, intermediate),\n",
    " #   nn.Relu(),\n",
    " #   nn.Dropout(0.2),\n",
    " #   nn.Linear(intermediate, 10)\n",
    "#)\n",
    "\n",
    "model.fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip: all of the layers in the model are organized as attributes, making it easy to call and modify individual layers, i.e. `model.layer.sublayer`.** For example, if you wanted to see the 0th layer of branch 3 in Inception 3a, you would return `model.inception3a.branch3[0]`. \n",
    "\n",
    "**Tip: In order to modify a layer, you must redefine the layer and can't simply change the attribute values.** For example, trying to change the number of output classes by returning `model.fc.out_features = 10` wouldn't actually change the model output neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, we'll require gradients for only the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that all of our gradients are as we expect: False for all layers expect the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print (name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uw9Qoxeh5ioI"
   },
   "source": [
    "## 3. Test Model Functionality <a id=\"test\"></a>\n",
    "\n",
    "Before training, we need to make sure our model is properly built, i.e. the expected input (3D 3x224x224 array) will return the expected output (2D 1x10 array). An error indicates that the architecture is inconsistent in some way, such as unexpected input and output neurons, incorrectly changing the number of output neurons, etc.\n",
    "\n",
    "We'll use the first example in our training set to test our model's functionality. Slicing will guarantee the image shape will be (1, 3, 224, 224)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apN1_iWa5ioJ"
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "test_image = x_train_1000_process[index:index+1]\n",
    "test_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dimensions are changed, we convert the image from a `np.array` to a `torch.Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apN1_iWa5ioJ"
   },
   "outputs": [],
   "source": [
    "test_image_torch = torch.Tensor(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can \"predict\" the output neurons of the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wpQxovKl32dI",
    "outputId": "11bc3734-ea88-4d7e-8b41-3edecf4e8275"
   },
   "outputs": [],
   "source": [
    "testoutput_torch = model(test_image_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there isn't an error, we know our model is working. If there is, go back and check to make sure the input size is (1, 3, 224, 224), the input and outputs are consistent, etc.\n",
    "\n",
    "We also move the output from our model using the `detach()` method and convert the `torch.Tensor` to a `np.array` by using the `numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wpQxovKl32dI",
    "outputId": "11bc3734-ea88-4d7e-8b41-3edecf4e8275"
   },
   "outputs": [],
   "source": [
    "testoutput = testoutput_torch.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shape of the output neurons to make sure they are what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eMX9qm2K5ioJ",
    "outputId": "ccdec276-abe3-42f5-f1ca-ad717c491601"
   },
   "outputs": [],
   "source": [
    "print ('The shape of the output neurons are {}.'.format(testoutput.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return the output neurons. Note since we do not use an activation function at the end of our network, the domain of our output can be any real number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the [softmax activation function](https://en.wikipedia.org/wiki/Softmax_function) to convert the output neurons to probabilities for each classification with the index corresponding to the digit classification probability, e.g. the 0th index corresponds to the probability of a 0 classification. Since our model isn't trained, all the output probabilities are approximately 0.1, indicating our model is randomly \"classifying\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "softmax(testoutput_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, it's good practice to know how many trainable parameters are in our model. The number of trainable parameters can be used as a proxy for estimating total training time. We define [a counting function](https://stackoverflow.com/questions/49201236/check-the-total-number-of-parameters-in-a-pytorch-model) for us and determine how many trainable parameters there are in our model. **Note this function only counts the layers that require gradient descent so it should only show the fully connected layer's parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlRZSLlA5ioJ"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        print([name, param])\n",
    "        total_params+=param\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bI_Ay6A35ioK",
    "outputId": "42ce0930-3c9d-4156-f5d4-0b6f5b6a53c3"
   },
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10K training parameters is vastly smaller than the original 7M so training will take so much shorter than training GoogLeNet from scratch! Now that our model is ready, let's further prepare our data and set our hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Br3lFwwt5ioK"
   },
   "source": [
    "## 4. Set Training and Test Sets <a id=\"set\"></a>\n",
    "\n",
    "PyTorch uses iterables to create its data objects. Here we use lists to format the data to be PyTorch compatible. Experienced Python users are more likely to be comfortable using and manipulating lists. The function `format_dataset` makes a 2D list with each element being [image, label]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQU2kznG5ioK"
   },
   "outputs": [],
   "source": [
    "def format_dataset(image_set, labels):\n",
    "    data_set = []\n",
    "    for i in range(len(image_set)):\n",
    "        data_set.append([image_set[i], labels[i]])  \n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQU2kznG5ioK"
   },
   "outputs": [],
   "source": [
    "train_set = format_dataset(x_train_1000_process, y_train_1000)\n",
    "val_set = format_dataset(x_test_process, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define a baseline for our model to perform better than. The baseline helps us understand if our model is learning anything at all. We choose the model classifying at random to be our baseline since a poor model would perform as such. That being said, if our model's accuracy is well above 10%, then we know the model is learning something useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlocipjn5ioL"
   },
   "source": [
    "## 5. Hyperparameters and Loading <a id=\"hyper\"></a>\n",
    "\n",
    "First, we must set our hyperparameters for the model to use for training. The hyperparamters we are using are batch size, shuffle, and number of workers. Batch size can be tuned as needed to improve results. Shuffle should almost always be True since the data shouldn't be ordered in any specific way when training. In addition, the number of workers has a default of 0, which uses the main processor on the machine you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULKA-7wr5ioL"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "params = {\n",
    "        'batch_size': 32,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we choose the number of epochs we wish to train for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULKA-7wr5ioL"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful metric to know is how many updates our model will perform during training. We can calculate this by finding the number of batches in the training set (number of training samples / batch size) and multiplying it by the number of epochs. Knowing how many batches our model might need to be well trained can be a good place to start when tuning hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('The model will train using a total of {} batches'.format(num_epochs * \n",
    "                                                       int(x_train_1000_process.shape[0] / params['batch_size'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip: the author's rule of thumb is to have at least 100 batches trained per epoch, but this can be difficult with a small dataset. In addition, a minimum batch of 32 hints at enough samples for central limit theorem to hold true, although that may not be the case for each batch.**\n",
    "\n",
    "With our hyperparameters set, we can load our training and test set using `DataLoader`. \n",
    "\n",
    "**Note the variable and function names in the notebook are directed for validation sets, but we will use them for the test set instead.** That being said, we use the definitions for validation set and test set interchangeably here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULKA-7wr5ioL"
   },
   "outputs": [],
   "source": [
    "# TRAINING SET\n",
    "train_loader = DataLoader(train_set, **params)\n",
    "\n",
    "# TEST SET\n",
    "valid_loader = DataLoader(val_set, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our loss function to be [Cross Entropy Loss](https://en.wikipedia.org/wiki/Cross_entropy), which combines [softmax and the negative log likelihood loss](https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/). This function is standard in multiclass classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s__fkF-_5ioL"
   },
   "outputs": [],
   "source": [
    "distance = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we choose our optimizer to be [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam), since the learning rate updates automatically and trains relatvely fast compared to [Stochastic Gradient Descent (SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s__fkF-_5ioL"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),  weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have GPUs available, then those will be used for training. If not, then the model will train on CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s__fkF-_5ioL"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the device to make sure we know what's available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s__fkF-_5ioL"
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Model <a id=\"train\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train our model, we have to manually loop through our data for training. This is probably the biggest difference between PyTorch and [Tensorflow](https://www.tensorflow.org/), but this allows for more hands-on manipulation of how training is performed, which can be advantageous. We will train our model as follows:\n",
    "1. Change the model to trianing mode to activate backpropogation\n",
    "2. Initialize training loss to be 0\n",
    "3. Loop through each batch of features and labels by:\n",
    "    - Putting the data onto your device\n",
    "    - Calculating the output neurons and the loss\n",
    "    - Performing backgrpopogation and adding the batch training loss to total training loss\n",
    "4. Normalize the total training loss by number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrotiQpF5ioL"
   },
   "outputs": [],
   "source": [
    "# Define train loop\n",
    "\n",
    "def train_model(train_loader):\n",
    "\n",
    "    # Change model to training mode (activates backpropogation)\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize training loss\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Loop through batches of training data\n",
    "    for data, target in train_loader:\n",
    "        \n",
    "        # Put training batch on device\n",
    "        data = data.float().to(device)\n",
    "        target = target.type(torch.LongTensor).to(device)\n",
    "\n",
    "        # Calculate output and loss from training batch\n",
    "        output = model(data)\n",
    "        loss = distance(output, target)\n",
    "        \n",
    "        # Backpropogation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Normalize training loss from one epoch\n",
    "    train_loss_norm = train_loss / len(train_loader)\n",
    "    \n",
    "    return train_loss_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we define a similar loop for evaluating the test set at each epoch, which signals us if our model is generalizing. We will test our model as follows:\n",
    "1. Change model to evaluation mode to deactivate backpropogation\n",
    "2. Initialize test loss and number of correctly classified samples to 0\n",
    "3. Loop through each batch of features and labels by:\n",
    "    - Putting the data onto your device\n",
    "    - Calculating the output neurons and the loss\n",
    "    - Counting the number of correct predictions\n",
    "4. Calculate test set accuracy\n",
    "5. Normalize the total test loss by number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFCkW15B8a3F"
   },
   "outputs": [],
   "source": [
    "# Define validation loop\n",
    "\n",
    "def validate_model(valid_loader):\n",
    "\n",
    "    # Change model to evaluate mode (deactivates backpropogation)\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize validation loss and number of correct predictions\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # Do not calculate gradients for the loop\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Loop through batches of validation data\n",
    "        for data, target in valid_loader:\n",
    "            \n",
    "            # Put validation batch on device\n",
    "            data = data.float().to(device)\n",
    "            target = target.type(torch.LongTensor).to(device)\n",
    "            \n",
    "            # Calculate output and loss from validation batch\n",
    "            output = model(data)\n",
    "            val_loss += distance(output, target).item()\n",
    "            \n",
    "            # Count number of correct predictions\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = 100. * correct / len(valid_loader.dataset)\n",
    "    \n",
    "    # Normalize validation loss from one epoch\n",
    "    val_loss_norm = val_loss / len(valid_loader)\n",
    "    \n",
    "    return val_loss_norm, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train our model! We will print out the train and test loss/accuracy per epoch to keep track of performance. The loop below performs the training and validation loops defined above and records our metrics. Most of the time taken in the loop will actually be for pushing the batches through the network (performing millions of computations to go from a 3x224x224 to 10 neurons) and not calculating gradients, which is usually the workhorse of training. To get around this for more enhanced speed, [manually extract the features from the data](https://discuss.pytorch.org/t/how-can-i-extract-intermediate-layer-output-from-loaded-cnn-model/77301/3), and build a custom neural network to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-1mPpYr5ioM",
    "outputId": "375dc5f0-5c50-4e5e-f78d-b95b8a192c11",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keep track of metrics\n",
    "lst_train_loss = []\n",
    "lst_val_loss = []\n",
    "lst_accuracy = []\n",
    "\n",
    "# training loop\n",
    "for epoch in tqdm(range(num_epochs), total=num_epochs):\n",
    "\n",
    "    # Go through loops\n",
    "    train_loss = train_model(train_loader)\n",
    "    val_loss, accuracy = validate_model(valid_loader)\n",
    "\n",
    "    # Append metrics\n",
    "    lst_train_loss.append(train_loss)\n",
    "    lst_val_loss.append(val_loss)\n",
    "    lst_accuracy.append(accuracy)\n",
    "\n",
    "    # Log\n",
    "    print('Epoch {:.3f} - Train loss: {:.3f} - Val Loss: {:.3f} - Accuracy: ({:.0f}%)'.format(\n",
    "            epoch, train_loss, val_loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model trained on ImageNet was able to use it's previous knowledge to classify digits! In addition, we only used 1000 training images (~100/class) and it was able to generalize pretty well, which is really impressive. Now we'll look at some metrics to get more details on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qip7uayN5ioM"
   },
   "source": [
    "## 7. Plot Metrics <a id=\"plot\"></a>\n",
    "\n",
    "We plot the train/test loss and test accuracy to determine how well converged our model is. Any small overfitting is most likely caused by how small our training set is and how much larger our test set is than our training set. This could be resolved by changing the size of our datasets or using stronger regularization. The model may also not be fully converged yet so training for more epochs may increase performance. However, since our model is performing a lot better than our baseline, we are satisfied with the model we trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "sCTCmI8u5ioM",
    "outputId": "0b15a3e8-5884-4d15-bccf-7bf6d38a1b92",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=[10,5])\n",
    "\n",
    "axs[0].set_title('Loss')\n",
    "axs[0].plot(np.arange(num_epochs), lst_train_loss, label='train')\n",
    "axs[0].plot(np.arange(num_epochs), lst_val_loss, label='val')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_title('Accuracy')\n",
    "axs[1].plot(np.arange(num_epochs), lst_accuracy, color='C1')\n",
    "axs[1].set_xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also plot a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) to see how our model is performing on an individual class basis and see what digits are most difficult to classify. We will only be evaluating the first 1000 samples of the test set to save on computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate test subset\n",
    "x_test_1000_process = x_test_process[:1000]\n",
    "y_test_1000 = y_test[:1000]\n",
    "\n",
    "val_pred0 = model(torch.Tensor(x_test_1000_process))\n",
    "val_pred = val_pred0.data.max(1, keepdim=True)[1].detach().numpy().flatten()\n",
    "confusion_matrix = metrics.confusion_matrix(y_test_1000, val_pred, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.heatmap(confusion_matrix, annot=True, cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model seems to perform well (> 70%) on most digits, but falls short for some. Let's analyze some individual samples to see the classification probabilities per sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Samples <a id=\"analyze\">\n",
    "    \n",
    "Now that our model is trained, let's analyze some samples to see the classification probabilities of some images. We can look at random examples in our test set and plot classification probabilities using a bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEawafOrB4YZ"
   },
   "outputs": [],
   "source": [
    "# choose random image and corresponding output neurons from test subset\n",
    "rand_index = np.random.randint(x_test_1000_process.shape[0])\n",
    "rand_image = x_test_1000_process[rand_index][0]\n",
    "rand_class_prob_tensor = softmax(val_pred0[rand_index:rand_index+1])\n",
    "rand_class_prob = rand_class_prob_tensor.detach().numpy().flatten()\n",
    "\n",
    "# plot image and classification probabilities\n",
    "fig, axs = plt.subplots(1,2,figsize=[10,5])\n",
    "axs[0].set_title('Testing Image {} with a label of {}'.format(rand_index, y_test_1000[rand_index]))\n",
    "axs[0].imshow(rand_image)\n",
    "axs[1].set_title('Classification Probabilities for Testing Image {}'.format(rand_index))\n",
    "axs[1].bar(np.arange(10), rand_class_prob)\n",
    "axs[1].set_xlabel('Label')\n",
    "axs[1].set_ylabel('Probability')\n",
    "print ('Prediction: {}, Label: {}'.format(val_pred[rand_index], y_test_1000[rand_index]))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition we can look at false positive/negative samples to investigate harder samples in the test set. Let's create a mask that will return incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = val_pred != y_test_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose random incorrect sample\n",
    "false_index = np.random.randint(mask.sum())\n",
    "false_image = x_test_1000_process[mask][false_index][0]\n",
    "false_class_prob_tensor = softmax(val_pred0[mask][false_index:false_index+1])\n",
    "false_class_prob = false_class_prob_tensor.detach().numpy().flatten()\n",
    "\n",
    "# plot image and classification probabilities\n",
    "fig, axs = plt.subplots(1,2,figsize=[10,5])\n",
    "axs[0].set_title('Testing Image (mask) {} with a label of {}'.format(false_index, y_test_1000[mask][false_index]))\n",
    "axs[0].imshow(false_image)\n",
    "axs[1].set_title('Classification Probabilities')\n",
    "axs[1].bar(np.arange(10), false_class_prob)\n",
    "print ('Prediction: {}, Label: {}'.format(val_pred[mask][false_index], y_test_1000[mask][false_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions <a id=\"con\"></a>\n",
    "\n",
    "Transfer learning can be a powerful tool in the absence of data, time, and/or computational resources. In most cases, it's often better to train an entire model from scratch if those restrictions previously mentioned do not apply. However in the real world, those restrictions are frequent and challenging to deal with as a data scientist. It's nice to have a technique that can compensate for small training data and \"slow\" computers, which in the end makes machine learning and deep learning more accessible to all.\n",
    "\n",
    "Thank you for walking through this notebook. Now you should be more familiar with:\n",
    "- loading and transforming the MNIST dataset\n",
    "- loading and modifying a pretrained model\n",
    "- training and evaluating a pretrained model\n",
    "\n",
    "**Congratulations, you have completed the notebook!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources <a id=\"add\"></a>\n",
    "\n",
    "Machine learning is a dense and rapidly evolving field of study. Becoming an expert takes years of practice and patience, but hopefully this notebook brought you closer in that direction. Here are some of the author's favorite resources for learning about machine learning and data science:\n",
    "\n",
    "- [Google Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/ml-intro)\n",
    "- [scikit-learn Python Library](https://scikit-learn.org/stable/index.html) (go-to for most ML algorithms besides neural networks)\n",
    "- [StatQuest YouTube Channel](https://www.youtube.com/c/joshstarmer)\n",
    "- [DeepLearningAI YouTube Channel](https://www.youtube.com/c/Deeplearningai/videos)\n",
    "- [Towards Data Science](https://towardsdatascience.com/) (articles about data science and machine learning, some involving example blocks of code)\n",
    "- Advance searching [arxiv](https://arxiv.org/search/advanced) (e.g. search term \"machine learning\" in Abstract for Subject astro-ph) to see what others are doing currently\n",
    "- Google, YouTube, and Wikipedia in general\n",
    "- Pretrained model papers:\n",
    "    - [VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION (VGG)](https://arxiv.org/pdf/1409.1556.pdf)\n",
    "    - [Deep Residual Learning for Image Recognition (ResNet)](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "    - [Going deeper with convolutions (GoogLeNet)](https://arxiv.org/pdf/1409.4842.pdf)\n",
    "    - [Rethinking the Inception Architecture for Computer Vision (Inception v3)](https://arxiv.org/pdf/1512.00567.pdf)\n",
    "- Supplementary articles used for learning transfer learning in PyTorch:\n",
    "    - [Pytorch: Transfer Learning and Image Classification](https://pyimagesearch.com/2021/10/11/pytorch-transfer-learning-and-image-classification/)\n",
    "    - [Deep Learning for Everyone](https://www.analyticsvidhya.com/blog/2019/10/how-to-master-transfer-learning-using-pytorch/)\n",
    "    - [PyTorch freeze part of the layers](https://jimmy-shen.medium.com/pytorch-freeze-part-of-the-layers-4554105e03a6)\n",
    "\n",
    "## About this Notebook <a id=\"about\"></a>\n",
    "\n",
    "**Author:** Fred Dauphin, DeepWFC3\n",
    "\n",
    "**Updated on:** 2021-12-03\n",
    "\n",
    "## Citations <a id=\"cite\"></a>\n",
    "\n",
    "If you use `numpy`, `matplotlib`, or `torch` for published research, please cite the authors. Follow these links for more information about citing `numpy`, `matplotlib`, and `torch`:\n",
    "\n",
    "* [Citing `numpy`](https://numpy.org/doc/stable/license.html)\n",
    "* [Citing `matplotlib`](https://matplotlib.org/stable/users/project/license.html#:~:text=Matplotlib%20only%20uses%20BSD%20compatible,are%20acceptable%20in%20matplotlib%20toolkits.)\n",
    "* [Citing `torch`](https://github.com/pytorch/pytorch/blob/master/LICENSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "[Top of Page](#title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "classifier_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"title\"></a>\n",
    "# Variational Autoencoder MNIST Tutorial using PyTorch\n",
    "***\n",
    "## Learning Goals:\n",
    "By the end of this tutorial, you will:\n",
    "- build a variational autoencoder (VAE)\n",
    "- train and evaluate a VAE\n",
    "- visualize the latent space of a VAE\n",
    "- generate samples from the latent space\n",
    "\n",
    "## Table of Contents\n",
    "[Introduction](#intro) <br>\n",
    "[0. Imports](#imports) <br>\n",
    "[1. MNIST Dataset and Scaling](#mnist) <br>\n",
    "[2. Build a VAE](#build) <br>\n",
    "[3. Test Model Functionality](#test) <br>\n",
    "[4. Set Training and Test Sets](#set) <br>\n",
    "[5. Hyperparameters and Loading](#hyper) <br>\n",
    "[6. Train Model](#train) <br>\n",
    "[7. Plot Loss Function and R2](#plot) <br>\n",
    "[8. Analyze Samples](#analyze) <br>\n",
    "[9. Visualize the Latent Space](#latent) <br>\n",
    "[10. Generate Samples](#detect) <br>\n",
    "[11.. Conclusions](#con) <br>\n",
    "[Additional Resources](#add) <br>\n",
    "[About this Notebook](#about) <br>\n",
    "[Citations](#cite) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a id=\"intro\"></a>\n",
    "\n",
    "The main purpose of this notebook is to build an autoencoder in [PyTorch](https://pytorch.org/), a deep learning Python library. This tutorial is not an exhaustive introduction to machine learning and assumes the user is familiar with vocabulary (supervised v unsupervised, neural networks, loss functions, backpropogation, etc) and methodology (model selection, feature selection, hyperparameter tuning, etc). This notebook also assumes the user is familiar with autoencoders and the [MNIST handwritten dataset](http://yann.lecun.com/exdb/mnist/). Look at [Additional Resources](#add) for more complete machine learning guides. The paragraphs below serve as a brief introduction to variational autoencoders.\n",
    "\n",
    "A [variational autoencoder (VAE)](https://en.wikipedia.org/wiki/Variational_autoencoder) is an autoencoder that forces the latent space to approximate a given probability distribution. Most VAEs approximate the latent space to be a standard normal distribution. If the latent space approximates a normal distribution, the latent space becomes smooth and the decoding is more robust across the latent space, unlike in traditional autoencoders. The same use cases from traditional autoencoders still apply with the addition of the VAE being a generative model. Since VAEs learned the probability distribution of the input data through the latent space, it can generate new fake samples that well represent the training data.\n",
    "\n",
    "[Reparametrization trick](https://stats.stackexchange.com/questions/199605/how-does-the-reparameterization-trick-for-vaes-work-and-why-is-it-important)\n",
    "    \n",
    "**In this notebook, we will build a variational autoencoder using PyTorch to learn the representation of MNIST handwritten digit dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YcR3do4SAr2_",
    "outputId": "c25bbf70-4814-463f-c934-480be9928728"
   },
   "source": [
    "## 0. Imports <a id=\"imports\"></a>\n",
    "\n",
    "If you are running this notebook on Google Colab, you shouldn't have to install anything. If you are running this notebook in Jupyter, this notebook assumes you created the virtual environment defined in `environment.yml`. If not, close this notebook and run the following lines in a terminal window:\n",
    "\n",
    "`conda env create -f environment.yml`\n",
    "\n",
    "`conda activate deepwfc3_env`\n",
    "\n",
    "We import the following libraries:\n",
    "- *numpy* for handling arrays\n",
    "- *matplotlib* for plotting\n",
    "- *tqdm* for keeping track of loop speed\n",
    "- *torchvision* for accessing MNIST images \n",
    "- *torch* as our machine learning framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YcR3do4SAr2_",
    "outputId": "c25bbf70-4814-463f-c934-480be9928728"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MNIST Dataset and Scaling<a id=\"mnist\"></a>\n",
    "\n",
    "The MNIST dataset is nicely packed in `torch` as `torch.Tensors`. We'll download the training and test sets, which is unpacked as `x_train` for training features, `y_train` for training labels, `x_test` for testing features, and `y_test` for testing labels. In addition, we'll convert the datasets to `np.arrays` for easier data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'mnist'\n",
    "train_dataset = torchvision.datasets.MNIST(root, train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "test_dataset  = torchvision.datasets.MNIST(root, train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "x_train = train_dataset.data.numpy()\n",
    "y_train = train_dataset.targets.numpy()\n",
    "x_test = test_dataset.data.numpy()\n",
    "y_test = test_dataset.targets.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also define some frequently used global variables. `x_train_size` is the number of images in the training set, `x_test_size` is the number of images in the test set, and `x_length` is the length/width of an image. In addition, we min-max scale our images to have a minimum value of 0 and a maximum value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_size = x_train.shape[0]\n",
    "x_test_size = x_test.shape[0]\n",
    "x_length = x_train.shape[1]\n",
    "\n",
    "norm = x_train.max()\n",
    "x_train_scale = x_train / norm\n",
    "x_test_scale = x_test / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a VAE <a id=\"build\"></a>\n",
    "\n",
    "PyTorch has its own unique data objects called `torch.utils.data.Dataset`. `Dataset` has methods to retrieve the data length and instances. The datasets built from the class are used as inputs for `torch.utils.data.Dataloader`, which prepares our data for training. Since an autoencoder isn't trained using labels, the \"labels\" are not defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, images):\n",
    "        self.images = images\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the functions and layers to build our encoder. The constructor has our model hyperparameters as inputs:\n",
    "\n",
    "- `filters`: the number of filters the convolutional layers will learn\n",
    "- `neurons`: the number of neurons in the fully connected layers\n",
    "- `sub_array_size`: the image length/width\n",
    "- `kernel_size`: the size of the filter being learned\n",
    "\n",
    "Using the constructor's parameters, we define the encoder's layers and functions. We use the [rectified linear unit (ReLU)](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) as our activation function to add nonlinearity to our model and [batch normalization](https://en.wikipedia.org/wiki/Batch_normalization) to rescale our features in each convolutional layer.\n",
    "\n",
    "The `forward` fucntion builds our encoder from the functions we defined in the constructor. The encoder is built as follows:\n",
    "- Encoder Layer 1\n",
    "    - convole 1 28x28 image into 8 24x24 feature maps\n",
    "    - perform batch normalization\n",
    "    - activate the feature maps using ReLU\n",
    "- Encoder Layer 2\n",
    "    - convole 8 24x24 feature maps into 16 20x20 feature maps\n",
    "    - perform batch normalization\n",
    "    - activate the feature maps using ReLU\n",
    "- Encoder Layer 3\n",
    "    - convolve 16 20x20 feature maps into 32 16x16 feature maps\n",
    "    - perform batch normalization\n",
    "    - activate the feature maps using ReLU\n",
    "- Flatten the 32 16x16 feature maps to a 1D 32 * 16 * 16 array\n",
    "- Hidden Layer\n",
    "    - use the flatten 1D array as inputs for a 128 neuron hidden layer\n",
    "    - activate the neurons using ReLU\n",
    "- Latent Space\n",
    "     - use the 128 neuron hidden layer as inputs for the 2 dimensional latent space\n",
    "     - perform the reparameterization trick on the latent space\n",
    "         - z = mu + sigma * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions and build encoder\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 params_dict):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Params\n",
    "        filters = params_dict['filters']\n",
    "        neurons = params_dict['neurons']\n",
    "        sub_array_size = params_dict['sub_array_size']\n",
    "        kernel_size = params_dict['kernel_size']\n",
    "        \n",
    "        # The Rectified Linear Unit (ReLU)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Flattens the feature map to a 1D array\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "        # ---- CONVOLUTION ----\n",
    "        self.conv1 = nn.Conv2d(in_channels=filters[0], out_channels=filters[1], kernel_size=kernel_size)\n",
    "        self.conv2 = nn.Conv2d(in_channels=filters[1], out_channels=filters[2], kernel_size=kernel_size)\n",
    "        self.conv3 = nn.Conv2d(in_channels=filters[2], out_channels=filters[3], kernel_size=kernel_size)\n",
    "        \n",
    "        # ---- BATCH NORMALIZATION ----\n",
    "        self.batch1 = nn.BatchNorm2d(filters[1])\n",
    "        self.batch2 = nn.BatchNorm2d(filters[2])\n",
    "        self.batch3 = nn.BatchNorm2d(filters[3])\n",
    "        \n",
    "        # ---- LATENT ----\n",
    "        last_feature_map_size = sub_array_size + (1 - kernel_size) * (len(filters) - 1)\n",
    "        size_before_latent = filters[-1] * last_feature_map_size ** 2\n",
    "        self.linear1a = nn.Linear(size_before_latent, neurons[0])\n",
    "        self.linear1b = nn.Linear(neurons[0], neurons[1])\n",
    "        self.linear2_mu = nn.Linear(neurons[1], neurons[2])\n",
    "        self.linear2_logvar = nn.Linear(neurons[1], neurons[2])\n",
    "        \n",
    "        # ---- REPARAM -----\n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Layer 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.batch3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Hidden Layer\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1a(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear1b(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Mu and LogVar\n",
    "        mu = self.linear2_mu(x)\n",
    "        logvar = self.linear2_logvar(x)\n",
    "        \n",
    "        # Reparamaterize\n",
    "        sigma = torch.exp(0.5 * logvar)\n",
    "        epsilon = self.N.sample(sigma.size())\n",
    "        z = mu + sigma * epsilon\n",
    "        \n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the functions and layers to build our decoder. The constructor uses the same parameters and functions as the encoder.\n",
    "\n",
    "The `forward` fucntion builds our decoder from the functions we defined in the constructor, and is built as follows:\n",
    " \n",
    "- Latent Space\n",
    "    - use the 2 dimensional latent space as inputs for the 128 neuron hidden layer\n",
    "    - activate using ReLU\n",
    "- Hidden Layer\n",
    "    - use the 128 neuron hidden layer as inputs for the flattened 1D array\n",
    "    - activate using ReLU\n",
    "- Unflatten a 1D 32 * 16 * 16 array to the 32 16x16 feature maps\n",
    "- Decoder Layer 1\n",
    "    - transpose convole 32 16x16 feature maps into 16 20x20 feature maps\n",
    "    - perform batch normalization\n",
    "    - activate using ReLU\n",
    "- Decoder Layer 2\n",
    "    - transpose convole 16 20x20 feature maps into 8 24x24 feature maps\n",
    "    - perform batch normalization\n",
    "    - activate using ReLU\n",
    "- Decoder Layer 3\n",
    "    - transpose convole 8 24x24 feature maps into 1 28x28 feature map\n",
    "    - activate using the [Sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) (all outputs are between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions and build decoder\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 params_dict):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Params\n",
    "        filters = params_dict['filters']\n",
    "        neurons = params_dict['neurons']\n",
    "        sub_array_size = params_dict['sub_array_size']\n",
    "        kernel_size = params_dict['kernel_size']\n",
    "        \n",
    "        last_feature_map_size = sub_array_size + (1 - kernel_size) * (len(filters) - 1)\n",
    "        unflattened_size = (filters[-1], last_feature_map_size, last_feature_map_size)\n",
    "        \n",
    "        # The Rectified Linear Unit (ReLU)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Unflattens the 1D array to feature maps\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=unflattened_size)\n",
    "\n",
    "        # ---- CONVTRANSPOSE ----\n",
    "        self.trans1 = nn.ConvTranspose2d(in_channels=filters[3], out_channels=filters[2], kernel_size=kernel_size)\n",
    "        self.trans2 = nn.ConvTranspose2d(in_channels=filters[2], out_channels=filters[1], kernel_size=kernel_size)\n",
    "        self.trans3 = nn.ConvTranspose2d(in_channels=filters[1], out_channels=filters[0], kernel_size=kernel_size)\n",
    "        \n",
    "        # ---- BATCH NORMALIZATION ----\n",
    "        self.batch1 = nn.BatchNorm2d(filters[2])\n",
    "        self.batch2 = nn.BatchNorm2d(filters[1])\n",
    "        \n",
    "        # ---- LATENT ----\n",
    "        size_before_latent = filters[-1] * (last_feature_map_size) ** 2\n",
    "        self.linear1a = nn.Linear(neurons[2], neurons[1])\n",
    "        self.linear1b = nn.Linear(neurons[1], neurons[0])\n",
    "        self.linear2 = nn.Linear(neurons[0], size_before_latent)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Out of Latent\n",
    "        x = self.linear1a(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear1b(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.unflatten(x)        \n",
    "        \n",
    "        # Layer 1\n",
    "        x = self.trans1(x)\n",
    "        x = self.batch1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.trans2(x)\n",
    "        x = self.batch2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Layer 3\n",
    "        x = self.trans3(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define our VAE using the encoder and decoder. **Note: one could have made the entire VAE under one class, but it's good practice to build more complex neural network architectures using separate classes and combining them together. In addition, it'll be easier to call the encoder or decoder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, params_dict):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.encoder = Encoder(params_dict)\n",
    "        self.decoder = Decoder(params_dict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encoder(x)\n",
    "        x = self.decoder(z)\n",
    "        return x, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip: a lot of CNNs use max pooling as a downsampling method for more robust feature extraction, but we do not use a downsampling method here. Instead, our convolutional layers will downsample the feature maps for us since we don't use zero padding. The author found it difficult to train a VAE to have the latent space normally distributed using max pooling. When trained using max pooling, all the samples were essentially collapsing on a mean of 0, which does not represent a normal distribution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Model Functionality <a id=\"test\"></a>\n",
    "\n",
    "Before training, we need to make sure our model is properly built, i.e. the expected input (2D 28x28 array) will return the expected output (2D 28x28 array). An error indicates the architecture is inconsistent in some way, such as unexpected input and output filters, unexpected input and output neurons, etc. Some ways to \"break\" the model are listed below:\n",
    "- comment out a method in the constructor or forward\n",
    "- manually change arguments in the methods to a different value\n",
    "\n",
    "To start off, we define some hyperparameters and build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {'filters': [1, 16, 32, 64],\n",
    "               'neurons': [256, 128, 4],\n",
    "               'sub_array_size': x_length,\n",
    "               'kernel_size': 5\n",
    "              }\n",
    "\n",
    "model = VariationalAutoencoder(params_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we change the shape of our image to be compatible with PyTorch. The input dimensions for images are (number of samples, number of input channels, y dimension, x dimension), which in our case is (1, 1, 28, 28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "test_image = x_train_scale[index].reshape(1,1,x_length,x_length) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dimensions are changed, we convert the image from a `np.array` to a `torch.Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_torch = torch.Tensor(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can \"reconstruct\" our input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testoutput_torch, test_mu, test_logvar = model(test_image_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no error, we know our model is working. We also move the output from our model using the `detach()` method and convert the `torch.Tensor` to a `np.array` by using the `numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testoutput = testoutput_torch.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shape of the output to make sure they are what we expect. If it's not, then we have to fix our parameters where we defined the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('The shape of the output is {}.'.format(testoutput.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the input and output. Since the model hasn't been trained, the ouput should look like random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=[10,5])\n",
    "axs[0].set_title('Training Scaled Image {}'.format(index))\n",
    "axs[0].imshow(test_image[0,0].reshape(x_length, x_length))\n",
    "axs[1].set_title('Reconstructed Output')\n",
    "axs[1].imshow(testoutput[0,0].reshape(x_length, x_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, it's good practice to know how many trainable parameters are in our model. The number of trainable parameters can be used as a proxy for estimating total training time. We define [a counting function](https://stackoverflow.com/questions/49201236/check-the-total-number-of-parameters-in-a-pytorch-model) for us and determine how many trainable parameters there are in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        print([name, param])\n",
    "        total_params+=param\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Training and Test Sets <a id=\"set\"></a>\n",
    "\n",
    "PyTorch uses iterables to create its data objects. Here we show two ways to format the data to be PyTorch compatible.\n",
    "\n",
    "1. **Use arrays:** Experienced Python users are more likely to be comfortable using and manipulating arrays. We will just reshape our images to have an input channel of 1, i.e. (1, 28, 28).\n",
    "\n",
    "2. **Use LoadDataset:** In [Section 2](#build), we defined the `LoadDataset` class to format the data to be PyTorch compatible. The `Dataset` class comes with additional functionality specifically for PyTorch, but is beyond the scope of this tutorial.\n",
    "\n",
    "We choose option 1 as default, but option 2 can be uncommented below. Using either does not affect training at all and is up to user comfortability/preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = x_train_scale.reshape(x_train_size, 1, x_length, x_length)\n",
    "val_set = x_test_scale.reshape(x_test_size, 1, x_length, x_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoadDataset class\n",
    "#train_set = LoadDataset(x_train_scale.reshape(x_train_size, 1, x_length, x_length))\n",
    "#val_set = LoadDataset(x_test_scale.reshape(x_test_size, 1, x_length, x_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define a baseline for our model to perform better than. The baseline helps us understand if our model is learning anything at all. We choose the mean pixel of the inputs to be our baseline, i.e. a poor model would learn the reconstructed image as an image of the mean pixel. By calculating the [Mean Squared Error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error) of our training set and mean pixels, we have an established baseline to outperform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find mean pixel values of each image\n",
    "mean = np.mean(x_test_scale, axis=(1,2)).reshape(x_test_size,1,1)\n",
    "\n",
    "# create mean pixel value images\n",
    "ones = np.ones((x_test_size, x_length, x_length))\n",
    "mean_ones = mean * ones\n",
    "\n",
    "# calculate baseline\n",
    "baseline = np.sum(np.square(x_test_scale - mean_ones)) / (x_test_size)\n",
    "\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameters and Loading <a id=\"hyper\"></a>\n",
    "\n",
    "We must set some other hyperparameters for the model to use for training. The hyperparamters we are using are batch size, shuffle, and number of workers. Batch size can be tuned as needed to improve results. Shuffle should almost always be True since the data shouldn't be ordered in any specific way when training. In addition, the number of workers has a default of 0, which uses the main processor on the machine you are using. We also choose the number of epochs we wish to train for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Prepping arguments we have to feed to `DataLoader`\n",
    "params = {\n",
    "        'batch_size': 128,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0\n",
    "    }\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful metric to calculate is how many updates our model will perform during training. We can calculate this by finding the number of batches in the training set (number of training samples / batch size) and multiplying it by the number of epochs. Knowing how many batches our model might need to be well trained can be a good place to start when tuning hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('The model will train using a total of {} batches'.format(num_epochs * \n",
    "                                                       int(x_train_size / params['batch_size'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our hyperparameters set, we can load our training and test set using `DataLoader`. \n",
    "\n",
    "**Note the variable and function names in the notebook are directed for validation sets, but we will use them for the test set instead.** That being said, we use the definitions for validation set and test set interchangeably here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING SET\n",
    "train_loader = DataLoader(train_set, **params)\n",
    "\n",
    "# VALIDATION SET\n",
    "valid_loader = DataLoader(val_set, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will initialize our model again to be sure we are starting from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VariationalAutoencoder(params_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our loss function, which in the sum of a reconstruction loss and a divergence.\n",
    "- The reconstruction loss ensures the decoder accurately reconstructs our input. We choose [Binary Cross Entropy (BCE)](https://en.wikipedia.org/wiki/Cross_entropy) to be the reconstruction loss. We can also choose MSE alternatively, but there are mathematical motivations beyond the scope of this tutorial as to why that isn't the best choice (see [Additional Resources](#add) for more information). In addition, since MNIST pixels approximate a multinomial distribution more than a multivariate gaussian distribution, BCE is more appropriate. In the cell block below, you can uncomment the reconstruction loss to be MSE instead of BCE. **Note: using a ReLU for a final activation instead of a sigmoid may better optimize MSE since the background will most likely become 0 instead of being close to 0 (i.e. the summation of millions of 1e-6 residuals becomes noticable).**\n",
    "- A divergence measures the statistical distance between two probability distributions. We use the [Kullback-Leibler divergence (also known as KL divergence or $D_{KL}$)](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) as our divergence. Since we want the latent space to approximate a normal distribution, the KL divergence enforces that constraint. In our encoder, the final outputs are the $\\mu$ and $log(\\sigma^2)$ vectors. We can use these vectors to find the divergence between our latent space and a normal distribution. Some notes about KL divergence:\n",
    "    - Here is a [derivation](https://leenashekhar.github.io/2019-01-30-KL-Divergence/) of the KL divergence between normal distributions\n",
    "    - KL divergence is an asymmetric function: $D_{KL}(p||q) \\neq D_{KL}(q||p)$\n",
    "    - KL divergence between a distribution and itself is 0: $D_{KL}(p||p)=0$\n",
    "    - KL divergence is non negative: $D_{KL}(p||q) \\geq 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss = nn.BCELoss(reduction='sum')\n",
    "#reconstruction_loss = nn.MSELoss(reduction='sum')\n",
    "def distance(output, data, mu, logvar):\n",
    "    recon_loss = reconstruction_loss(output, data)\n",
    "    KLD = -0.5 * (1 + logvar - mu ** 2 - torch.exp(logvar)).sum()\n",
    "    return recon_loss, KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we choose our optimizer to be [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam), since the learning rate updates automatically and trains relatvely fast compared to [Stochastic Gradient Descent (SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),  weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have GPUs available, then those will be used for training. If not, then the model will train on CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the device to make sure we know what's available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Model <a id=\"train\"></a>\n",
    "\n",
    "In order to train our model, we have to manually loop through our data for training. This is probably the biggest difference between PyTorch and [Tensorflow](https://www.tensorflow.org/), but this allows for more hands-on manipulation of how training is performed, which can be advantageous. We will train our model as follows:\n",
    "1. Change the model to trianing mode to activate backpropogation\n",
    "2. Initialize training loss to be 0\n",
    "3. Loop through each batch of features by:\n",
    "    - Putting the data onto your device\n",
    "    - Calculating the outputs and the loss\n",
    "    - Performing backgrpopogation and adding the batch training loss to total training loss\n",
    "4. Normalize the total training loss by number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train loop\n",
    "\n",
    "def train_model(train_loader):\n",
    "\n",
    "    # Change model to training mode (activates backpropogation)\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize training loss\n",
    "    train_loss_recon = 0\n",
    "    train_loss_kld = 0\n",
    "    \n",
    "    # Loop through batches of training data\n",
    "    for data in train_loader:\n",
    "        \n",
    "        # Put training batch on device\n",
    "        data = data.float().to(device)\n",
    "\n",
    "        # Calculate output and loss from training batch\n",
    "        output, mu, logvar = model(data)\n",
    "        recon_loss, kld = distance(output, data, mu, logvar)\n",
    "        loss = recon_loss + kld\n",
    "        \n",
    "        # Backpropogation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_recon += recon_loss.item()\n",
    "        train_loss_kld += kld.item()\n",
    "    \n",
    "    # Normalize training loss from one epoch\n",
    "    norm = train_loader.dataset.shape[0]\n",
    "    train_loss_recon_norm = train_loss_recon / norm\n",
    "    train_loss_kld_norm = train_loss_kld / norm\n",
    "    train_loss_norm = [train_loss_recon_norm, train_loss_kld_norm]\n",
    "    \n",
    "    return train_loss_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we define a similar loop for evaluating the test set at each epoch, which signals us if our model is generalizing. We will test our model as follows:\n",
    "1. Change model to evaluation mode to deactivate backpropogation\n",
    "2. Initialize test loss and variances to 0\n",
    "3. Loop through each batch of features by:\n",
    "    - Putting the data onto your device\n",
    "    - Calculating the outputs and the loss\n",
    "4. Calculate test set loss and normalize it by number of samples\n",
    "5. Calculate R2 score to determine how correlated the inputs are with the outputs (no correlation tends to 0, high correlation tends to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define validation loop\n",
    "\n",
    "def validate_model(valid_loader):\n",
    "\n",
    "    # Change model to evaluate mode (deactivates backpropogation)\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize validation loss and variances\n",
    "    val_loss_recon = 0\n",
    "    val_loss_kld = 0\n",
    "    data_variance = 0\n",
    "    res_variance = 0\n",
    "    \n",
    "    # Do not calculate gradients for the loop\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Loop through batches of validation data\n",
    "        for data in valid_loader:\n",
    "            \n",
    "            # Put validation batch on device\n",
    "            data = data.float().to(device)\n",
    "            \n",
    "            # Calculate output and loss from validation batch\n",
    "            output, mu, logvar = model(data)\n",
    "            recon_loss, kld = distance(output, data, mu, logvar)\n",
    "            \n",
    "            # Calculate variances\n",
    "            data = data.detach().numpy()\n",
    "            output = output.detach().numpy()\n",
    "            \n",
    "            data_mean = np.mean(data, axis=(1,2,3)).reshape(data.shape[0], 1, 1, 1)\n",
    "            data_var = np.nansum((data - data_mean)**2)\n",
    "            res_var = np.nansum((data - output)**2)\n",
    "            \n",
    "            data_variance += data_var\n",
    "            res_variance += res_var\n",
    "            \n",
    "            val_loss_recon += recon_loss.item()\n",
    "            val_loss_kld += kld.item()\n",
    "    \n",
    "    # Normalize validation loss from one epoch\n",
    "    norm = valid_loader.dataset.shape[0]\n",
    "    val_loss_recon_norm = val_loss_recon / norm\n",
    "    val_loss_kld_norm = val_loss_kld / norm\n",
    "    val_loss_norm = [val_loss_recon_norm, val_loss_kld_norm]\n",
    "    \n",
    "    # Calculate r2 score\n",
    "    r2_score = 1 - res_variance / data_variance\n",
    "    \n",
    "    return val_loss_norm, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train our model! We will print out the train and test loss/R2 score per epoch to keep track of performance. The loop below performs the training and validation loops defined above and records our metrics. **Warning: training may take close to an hour; we want our decoder to do a decent job reconstructing the inputs and our latent space to be as smooth as possible. If you have some time/computational constraints, feel free to decrease the number of filters and/or neurons for faster, but less accurate training. The author has found in practice the trade off between increased performance and increased training time to be worth it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keep track of metrics\n",
    "lst_train_loss = []\n",
    "lst_val_loss = []\n",
    "lst_r2_score = []\n",
    "\n",
    "# training loop\n",
    "for epoch in tqdm(range(num_epochs), total=num_epochs):\n",
    "\n",
    "    # Go through loops\n",
    "    train_loss = train_model(train_loader)\n",
    "    val_loss, r2_score = validate_model(valid_loader)\n",
    "\n",
    "    # Append metrics\n",
    "    lst_train_loss.append(train_loss)\n",
    "    lst_val_loss.append(val_loss)\n",
    "    lst_r2_score.append(r2_score)\n",
    "\n",
    "    # Log\n",
    "    print('Recon Loss Epoch {:.3f} - Train loss: {:.4f} - Val Loss: {:.4f}'.format(\n",
    "            epoch, train_loss[0], val_loss[0]))\n",
    "    print('KLD Epoch {:.3f} - Train loss: {:.4f} - Val Loss: {:.4f}'.format(\n",
    "            epoch, train_loss[1], val_loss[1]))\n",
    "    print('R2 Score: {:.4f}'.format(r2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plot Loss Function and R2 <a id=\"plot\"></a>\n",
    "\n",
    "We plot the train/test loss and R2 scores to determine how well converged our model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lst_train_loss = np.array(lst_train_loss)\n",
    "lst_val_loss = np.array(lst_val_loss)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=[15,5])\n",
    "\n",
    "axs[0].set_title('Reconstruction Loss')\n",
    "axs[0].plot(np.arange(num_epochs), lst_train_loss[:, 0], label='train')\n",
    "axs[0].plot(np.arange(num_epochs), lst_val_loss[:, 0], label='val')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_title('KLD')\n",
    "axs[1].plot(np.arange(num_epochs), lst_train_loss[:, 1], label='train')\n",
    "axs[1].plot(np.arange(num_epochs), lst_val_loss[:, 1], label='val')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].legend()\n",
    "\n",
    "axs[2].set_title('R2 Score')\n",
    "axs[2].plot(np.arange(num_epochs), lst_r2_score, color='C1')\n",
    "axs[2].set_xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry if the KL divergence is increasing; it acts as a regularizer. We can think of the reconstruction loss as fighting against KLD. \n",
    "\n",
    "Let's also compare our baseline to our model over the test set to see how well our reconstructions are on average. First, we predict the outputs of our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, mu, logvar = model(torch.Tensor(val_set).to(device))\n",
    "recon = output.detach().numpy()\n",
    "mse = np.sum((val_set - recon) ** 2, axis=(1,2,3))\n",
    "\n",
    "mu = mu.detach().numpy()\n",
    "logvar = logvar.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the baseline and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('The model is performing {:.4f} times better than the baseline'.format(baseline / (mse.sum() / x_test_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Samples <a id=\"analyze\">\n",
    "    \n",
    "Now that our model is trained, let's plot random samples, their reconstructions, and their squared residuals to see how well our decoder reconstructs inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose random image and corresponding output from test set\n",
    "rand_index = np.random.randint(x_test_size)\n",
    "rand_image = x_test_scale[rand_index]\n",
    "rand_recon = recon[rand_index][0]\n",
    "rand_sq_res = (rand_image - rand_recon) ** 2\n",
    "rand_mse = mse[rand_index]\n",
    "\n",
    "# plot input, output, and squared residuals\n",
    "fig, axs = plt.subplots(2,2,figsize=[10,10])\n",
    "axs[0,0].set_title('Testing Scaled Image {}'.format(rand_index))\n",
    "axs[0,0].imshow(rand_image)\n",
    "axs[0,1].set_title('Reconstructed Output'.format(rand_index))\n",
    "axs[0,1].imshow(rand_recon)\n",
    "axs[1,0].set_title('Squared Residual Image')\n",
    "axs[1,0].imshow(rand_sq_res)\n",
    "axs[1,1].set_title('Squared Residual Image (0-1 min-max)')\n",
    "axs[1,1].imshow(rand_sq_res, vmin=0, vmax=1)\n",
    "plt.tight_layout()\n",
    "\n",
    "print ('MSE: {:.4f}'.format(rand_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the distribution of MSEs to get a better understanding of how well our model reconstructs each test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,5])\n",
    "plt.title('Test Set MSE Distribution')\n",
    "plt.hist(mse, bins=50)\n",
    "plt.xlabel('mse')\n",
    "plt.ylabel('frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, let's see if we can distinguish the loss by class. If most of the distributions are within reason in relation to each other, then the model generalizes to all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,5])\n",
    "plt.title('Test Set MSE Distribution (by class)')\n",
    "for digit in range(10):\n",
    "    plt.hist(mse[y_test == digit], bins=50, label=digit, alpha=0.25)\n",
    "plt.xlabel('mse')\n",
    "plt.ylabel('frequency')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the model is performing a lot better than the baseline, there are still samples it struggles with. Let's see how many samples have a MSE more than 3 sigma above the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = mse.mean() + 3 * mse.std()\n",
    "mask = mse > threshold\n",
    "\n",
    "print ('There are {} MSEs above {:.4f}.'.format(mask.sum(), threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with our mask, we can look through \"poorly\" reconstructed samples. **Note: because the model performs the reparameterization trick on our inputs, the poor reconstructions could be due to high variance added to the sample in the latent space.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose random incorrect image and corresponding output from test set\n",
    "rand_index = np.random.randint(mask.sum())\n",
    "rand_image = x_test_scale[mask][rand_index]\n",
    "rand_recon = recon[mask][rand_index][0]\n",
    "rand_sq_res = (rand_image - rand_recon) ** 2\n",
    "rand_mse = mse[mask][rand_index]\n",
    "\n",
    "# plot input, output, and squared residuals\n",
    "fig, axs = plt.subplots(2,2,figsize=[10,10])\n",
    "axs[0,0].set_title('Testing Masked Scaled Image {}'.format(rand_index))\n",
    "axs[0,0].imshow(rand_image)\n",
    "axs[0,1].set_title('Reconstructed Output'.format(rand_index))\n",
    "axs[0,1].imshow(rand_recon)\n",
    "axs[1,0].set_title('Squared Residual Image')\n",
    "axs[1,0].imshow(rand_sq_res)\n",
    "axs[1,1].set_title('Squared Residual Image (0-1 min-max)')\n",
    "axs[1,1].imshow(rand_sq_res, vmin=0, vmax=1)\n",
    "plt.tight_layout()\n",
    "\n",
    "print ('MSE: {:.4f}'.format(rand_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize the Latent Space <a id=\"latent\"></a>\n",
    "\n",
    "As mentioned in the [Introduction](#intro), one of the use cases of an autoencoder is to reduce the dimensionality of the dataset. Since our decoder is able to reconstruct our inputs to a high degree, that means our data is efficiently stored in the latent space. By using the encoder as a feature extractor, we can visualize the data in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=[20,10])\n",
    "for digit in range(10):\n",
    "    axs[0].scatter(mu[:, 0][y_test==digit], mu[:, 2][y_test==digit], label=digit, alpha=0.25)\n",
    "    axs[1].scatter(mu[:, 1][y_test==digit], mu[:, 3][y_test==digit], label=digit, alpha=0.25)\n",
    "\n",
    "axs[0].set_xlabel('mu 0')\n",
    "axs[0].set_ylabel('mu 2')\n",
    "axs[1].set_xlabel('mu 1')\n",
    "axs[1].set_ylabel('mu 3')\n",
    "\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if the classes do not separate out distinctly, it's still impressive that the decoder is able to decipher what a digit will look like in a two dimensional space! There are a few things you can try to see if we can get better representation in the latent space.\n",
    "- Increase the number of dimensions of the latent space: currently we are using 2 dimensions because it's easy to plot and visualize. However, as the number of dimensions increases so does the amount of information the latent space can store. The learned manifold will drasically improve in these slightly higher dimensions (5-10), which is still far lower than the original input space (784).\n",
    "- Increase the depth/width of the VAE: currently we using 3 convolutional layers and 2 fully connected layers. By increasing the depth/width, we can extract even higher level features that could better represent the digits.\n",
    "\n",
    "The samples should approximate (subjective) a standard normal distribution: $N(\\mu\\approx0,\\sigma^2\\approx1)$. Let's check to make sure that's true. **Note: The $log(\\sigma^2)$ space, however, can form any distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Latent 0 Mean {:.3f} and Variance {:.3f}'.format(mu[:, 0].mean(), mu[:, 0].std()**2))\n",
    "print ('Latent 1 Mean {:.3f} and Variance {:.3f}'.format(mu[:, 1].mean(), mu[:, 1].std()**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can [plot the learned manifold](https://github.com/eugeniaring/Pytorch-tutorial/blob/main/VAE_mnist.ipynb) of our latent space to see how samples smoothly change across it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstructed(encoder, decoder, N=3, n=15):\n",
    "    r0 = (-N, N)\n",
    "    r1 = (-N, N)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    w = 28\n",
    "    img = np.zeros((n*w, n*w))\n",
    "    for i, y in enumerate(np.linspace(*r1, n)):\n",
    "        for j, x in enumerate(np.linspace(*r0, n)):\n",
    "            z = torch.Tensor([[x, 0, y, 0]]).to(device)\n",
    "            x_hat = decoder(z)\n",
    "            x_hat = x_hat.reshape(w, w).detach().numpy()\n",
    "            img[(n-1-i)*w:(n-1-i+1)*w, j*w:(j+1)*w] = x_hat\n",
    "    plt.imshow(img, extent=[*r0, *r1], cmap='gist_gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstructed(model.encoder, model.decoder)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Samples <a id=\"detect\"></a>\n",
    "\n",
    "As mentioned in the [Introduction](#intro), another use case of an autoencoder is to generate new samples. Since the latent space can be accurately decoded, we can randomly generate new digits by drawing from the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 6\n",
    "z = torch.Tensor(np.random.normal(0,1,(n**2, params_dict['neurons'][-1]))).to(device)\n",
    "generated_z = model.decoder(z).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot our generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(6, 6, figsize=[10,10])\n",
    "for i in range (n):\n",
    "    for j in range (n):\n",
    "        axs[i, j].imshow(generated_z[i*n+j][0])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can definetly tell some of them are fake, but not a bad start considering these 28x28 images are mapped from a 2D space! We can increase the purity of sample generation by training on a wider/deeper VAE, increasing the latent space to higher dimensions, and/or training until full convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusions <a id=\"con\"></a>\n",
    "\n",
    "The variational autoencoder is a popular and powerful unsupervised learning method for dimensionality reduction and sample generation. It adds on to the already impressive traditional autoencoder by approximating the latent space to a normal distribution. There's a reason it's become a staple in discovering the structure of complex datasets. \n",
    "\n",
    "Thank you for walking through this notebook. Now you should be more familiar with:\n",
    "- building a variational autoencoder (VAE)\n",
    "- training and evaluate a VAE\n",
    "- visualizing the latent space of a VAE\n",
    "- generating samples from the latent space\n",
    "\n",
    "**Congratulations, you have completed the notebook!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources <a id=\"add\"></a>\n",
    "\n",
    "Machine learning is a dense and rapidly evolving field of study. Becoming an expert takes years of practice and patience, but hopefully this notebook brought you closer in that direction. Here are some of the author's favorite resources for learning about machine learning and data science:\n",
    "\n",
    "- [Google Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/ml-intro)\n",
    "- [scikit-learn Python Library](https://scikit-learn.org/stable/index.html) (go-to for most ML algorithms besides neural networks)\n",
    "- [StatQuest YouTube Channel](https://www.youtube.com/c/joshstarmer)\n",
    "- [DeepLearningAI YouTube Channel](https://www.youtube.com/c/Deeplearningai/videos)\n",
    "- [Towards Data Science](https://towardsdatascience.com/) (articles about data science and machine learning, some involving example blocks of code)\n",
    "- Advance searching [arxiv](https://arxiv.org/search/advanced) (e.g. search term \"machine learning\" in Abstract for Subject astro-ph) to see what others are doing currently\n",
    "- Google, YouTube, and Wikipedia in general\n",
    "- [Variational Autoencoder Original Paper](https://arxiv.org/abs/1312.6114)\n",
    "- MSE vs BCE\n",
    "    - [A Tutorial on VAEs](https://arxiv.org/pdf/2006.10273.pdf) (see section 5.1 for arguments against MSE)\n",
    "    - [Maximizing Log Likelihood](https://www.expunctis.com/2019/01/27/Loss-functions.html)\n",
    "    - [StatsStackExchange](https://stats.stackexchange.com/questions/350211/loss-function-autoencoder-vs-variational-autoencoder-or-mse-loss-vs-binary-cross)\n",
    "- Supplementary GitHub Repos used for learning VAEs in PyTorch:\n",
    "    - [PyTorch Beginner - VAE](https://github.com/L1aoXingyu/pytorch-beginner/blob/master/08-AutoEncoder/Variational_autoencoder.py)\n",
    "    - [PyTorch Tutorials - VAE](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/variational_autoencoder/main.py)\n",
    "    - [Medium Articles - VAE](https://github.com/eugeniaring/Medium-Articles/blob/main/Pytorch/VAE_mnist.ipynb)\n",
    "    \n",
    "\n",
    "## About this Notebook <a id=\"about\"></a>\n",
    "\n",
    "**Author:** Fred Dauphin, DeepWFC3\n",
    "\n",
    "**Updated on:** 2021-12-14\n",
    "\n",
    "## Citations <a id=\"cite\"></a>\n",
    "\n",
    "If you use `numpy`, `matplotlib`, or `torch` for published research, please cite the authors. Follow these links for more information about citing `numpy`, `matplotlib`, and `torch`:\n",
    "\n",
    "* [Citing `numpy`](https://numpy.org/doc/stable/license.html)\n",
    "* [Citing `matplotlib`](https://matplotlib.org/stable/users/project/license.html#:~:text=Matplotlib%20only%20uses%20BSD%20compatible,are%20acceptable%20in%20matplotlib%20toolkits.)\n",
    "* [Citing `torch`](https://github.com/pytorch/pytorch/blob/master/LICENSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "[Top of Page](#title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "wfc3-ir-blob-finder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
